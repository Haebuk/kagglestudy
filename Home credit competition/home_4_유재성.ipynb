{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T14:01:39.018035Z",
     "start_time": "2021-02-07T14:01:37.056250Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import contextmanager\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from scipy.stats import kurtosis, iqr, skew\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T14:01:39.034050Z",
     "start_time": "2021-02-07T14:01:39.019036Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(debug=False):\n",
    "\tnum_rows = 30000 if debug else None\n",
    "\twith timer('application_train and application_test'):\n",
    "\t\tdf = get_train_test(DATA_DIRECTORY, num_rows=num_rows)\n",
    "\t\tprint('Application dataframe shape: ', df.shape)\n",
    "\twith timer('Bureau and bureau_balance data'):\n",
    "\t\tbureau_df = get_bureau(DATA_DIRECTORY, num_rows=num_rows)\n",
    "\t\tdf = pd.merge(df, bureau_df, on='SK_ID_CURR', how='left')\n",
    "\t\tprint('Bureau dataframe shape: ', bureau_df.shape)\n",
    "\t\tdel bureau_df; gc.collect()\n",
    "\twith timer('previous_application'):\n",
    "\t\tprev_df = get_previous_applications(DATA_DIRECTORY, num_rows)\n",
    "\t\tdf = pd.merge(df, prev_df, on='SK_ID_CURR', how='left')\n",
    "\t\tprint('Previous dataframe shape: ', prev_df.shape)\n",
    "\t\tdel prev_df; gc.collect()\n",
    "\twith timer('previous aapplications balances'):\n",
    "\t\tpos = get_pos_cash(DATA_DIRECTORY, num_rows)\n",
    "\t\tdf = pd.merge(df, pos, on='SK_ID_CURR', how='left')\n",
    "\t\tprint('Pos-cash dataframe shape: ', pos.shape)\n",
    "\t\tdel pos; gc.collect()\n",
    "\t\tins = get_installment_payments(DATA_DIRECTORY, num_rows)\n",
    "\t\tdf = pd.merge(df, ins, on='SK_ID_CURR', how='left')\n",
    "\t\tprint('Installments dataframe shape: ', ins.shape)\n",
    "\t\tdel ins; gc.collect()\n",
    "\t\tcc = get_credit_catd(DATA_DIRECTORY, num_rows)\n",
    "\t\tdf = pd.merge(df, cc, on='SK_ID_CURR', how='left')\n",
    "\t\tprint('Credit card dataframe shape: ', cc.shape)\n",
    "\t\tdel cc; gc.collect()\n",
    "\t# Add ratios and groupby between different tables\n",
    "\tdf = add_ratios_features(df)\n",
    "\tdf = reduce_memory(df)\n",
    "\tlgbm_categorical_feat = [\n",
    "\t\t'CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'NAME_TYPE_SUITE', 'WALLSMATERIAL_MODE'\n",
    "\t]\n",
    "\twith timer('Run LightGBM'):\n",
    "\t\tfeat_importance = kfold_lightgbm_sklearn(df, lgbm_categorical_feat)\n",
    "\t\tprint(feat_importance)\n",
    "\n",
    "def add_ratios_features(df):\n",
    "    # CREDIT TO INCOME RATIO\n",
    "    df['BUREAU_INCOME_CREDIT_RATIO'] = df['BUREAU_AMT_CREDIT_SUM_MEAN'] / df['AMT_INCOME_TOTAL']\n",
    "    df['BUREAU_ACTIVE_CREDIT_TO_INCOME_RATIO'] = df['BUREAU_ACTIVE_AMT_CREDIT_SUM_SUM'] / df['AMT_INCOME_TOTAL']\n",
    "    # PREVIOUS TO CURRENT CREDIT RATIO\n",
    "    df['CURRENT_TO_APPROVED_CREDIT_MIN_RATIO'] = df['APPROVED_AMT_CREDIT_MIN'] / df['AMT_CREDIT']\n",
    "    df['CURRENT_TO_APPROVED_CREDIT_MAX_RATIO'] = df['APPROVED_AMT_CREDIT_MAX'] / df['AMT_CREDIT']\n",
    "    df['CURRENT_TO_APPROVED_CREDIT_MEAN_RATIO'] = df['APPROVED_AMT_CREDIT_MEAN'] / df['AMT_CREDIT']\n",
    "    # PREVIOUS TO CURRENT ANNUITY RATIO\n",
    "    df['CURRENT_TO_APPROVED_ANNUITY_MAX_RATIO'] = df['APPROVED_AMT_ANNUITY_MAX'] / df['AMT_ANNUITY']\n",
    "    df['CURRENT_TO_APPROVED_ANNUITY_MEAN_RATIO'] = df['APPROVED_AMT_ANNUITY_MEAN'] / df['AMT_ANNUITY']\n",
    "    df['PAYMENT_MIN_TO_ANNUITY_RATIO'] = df['INS_AMT_PAYMENT_MIN'] / df['AMT_ANNUITY']\n",
    "    df['PAYMENT_MAX_TO_ANNUITY_RATIO'] = df['INS_AMT_PAYMENT_MAX'] / df['AMT_ANNUITY']\n",
    "    df['PAYMENT_MEAN_TO_ANNUITY_RATIO'] = df['INS_AMT_PAYMENT_MEAN'] / df['AMT_ANNUITY']\n",
    "    # PREVIOUS TO CURRENT CREDIT TO ANNUITY RATIO\n",
    "    df['CTA_CREDIT_TO_ANNUITY_MAX_RATIO'] = df['APPROVED_CREDIT_TO_ANNUITY_RATIO_MAX'] / df[\n",
    "        'CREDIT_TO_ANNUITY_RATIO']\n",
    "    df['CTA_CREDIT_TO_ANNUITY_MEAN_RATIO'] = df['APPROVED_CREDIT_TO_ANNUITY_RATIO_MEAN'] / df[\n",
    "        'CREDIT_TO_ANNUITY_RATIO']\n",
    "    # DAYS DIFFERENCES AND RATIOS\n",
    "    df['DAYS_DECISION_MEAN_TO_BIRTH'] = df['APPROVED_DAYS_DECISION_MEAN'] / df['DAYS_BIRTH']\n",
    "    df['DAYS_CREDIT_MEAN_TO_BIRTH'] = df['BUREAU_DAYS_CREDIT_MEAN'] / df['DAYS_BIRTH']\n",
    "    df['DAYS_DECISION_MEAN_TO_EMPLOYED'] = df['APPROVED_DAYS_DECISION_MEAN'] / df['DAYS_EMPLOYED']\n",
    "    df['DAYS_CREDIT_MEAN_TO_EMPLOYED'] = df['BUREAU_DAYS_CREDIT_MEAN'] / df['DAYS_EMPLOYED']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIGHTGBM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T14:01:39.050064Z",
     "start_time": "2021-02-07T14:01:39.035050Z"
    }
   },
   "outputs": [],
   "source": [
    "def kfold_lightgbm_sklearn(data, categorical_feature = None):\n",
    "    df = data[data['TARGET'].notnull()]\n",
    "    test = data[data['TARGET'].isnull()]\n",
    "    print('Train/valid shape: {}, test shape: {}'.format(df.shape, test.shape))\n",
    "    del_features = ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index', 'level_0']\n",
    "    predictors = list(filter(lambda v: v not in del_features, df.columns))\n",
    "    \n",
    "    if not STARTIFIED_KFOLD:\n",
    "        folds = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "    else:\n",
    "        folds = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "    \n",
    "    # Hold oof predictions, test predictions, feature importance and training/valid auc\n",
    "    oof_preds = np.zeros(df.shape[0])\n",
    "    sub_preds = np.zeros(test.shape[0])\n",
    "    importance_df = pd.DataFrame()\n",
    "    eval_results = dict()\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df[predictors], df['TARGET'])):\n",
    "        train_x, train_y = df[predictors].iloc[train_idx], df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = df[predictors].iloc[valid_idx], df['TARGET'].iloc[valid_idx]\n",
    "        \n",
    "        params = {'random_state': RANDOM_SEED, 'nthread': NUM_THREADS}\n",
    "        clf = LGBMClassifier(**{**params, **LIGHTGBM_PARAMS})\n",
    "        if not categorical_feature:\n",
    "            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric='auc', verbose=400, early_stopping_rounds=EARLY_STOPPING)\n",
    "        else:\n",
    "            clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric='auc', verbose=400, early_stopping_rounds=EARLY_STOPPING, feature_name= list(df[predictors].columns), categorical_feature=categorical_feature)\n",
    "        \n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:,1]\n",
    "        sub_preds += clf.predict_proba(test[predictors], num_iteration=clf.best_iteration_)[:,1] / folds.n_splits\n",
    "       \n",
    "        # Feature importance by GAIN and SPLIT\n",
    "        fold_importance = pd.DataFrame()\n",
    "        fold_importance['feature'] = predictors\n",
    "        fold_importance['gain'] = clf.booster_.feature_importance(importance_type='gain')\n",
    "        fold_importance['split'] = clf.booster_.feature_importance(importance_type='split')\n",
    "        importance_df = pd.concat([importance_df, fold_importance], axis=0)\n",
    "        eval_results['train_{}'.format(n_fold+1)] = clf.evals_result_['training']['auc']\n",
    "        eval_results['valid_{}'.format(n_fold+1)] = clf.evals_result_['valid_1']['auc']\n",
    "        \n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "    \n",
    "    print('Full AUC score %.6f' % roc_auc_score(df['TARGET'], oof_preds))\n",
    "    test['TARGET'] = sub_preds.copy()\n",
    "    \n",
    "    # Get the average feature importance between folds\n",
    "    mean_importance = importance_df.groupby('feature').mean().reset_index()\n",
    "    mean_importance.sort_values(by='gain', ascending=False, inplace=True)\n",
    "    # Save feature importance, test predictions and oof predictions as csv\n",
    "    if GENERATE_SUBMISSION_FILES:\n",
    "        \n",
    "        # Generate oof csv\n",
    "        oof = pd.DataFrame()\n",
    "        oof['SK_ID_CURR'] = df['SK_ID_CURR'].copy()\n",
    "        df['PREDICTIONS'] = oof_preds.copy()\n",
    "        df['TARGET'] = df['TARGET'].copy()\n",
    "        df.to_csv('oof{}.csv'.format(SUBMISSION_SUFIX), index=False)\n",
    "        # Save submission (test data) and feature importance\n",
    "        test[['SK_ID_CURR', 'TARGET']].to_csv('submission{}.csv'.format(SUBMISSION_SUFIX), index=False)\n",
    "        mean_importance.to_csv('feature_importance{}.csv'.format(SUBMISSION_SUFIX), index=False)\n",
    "    return mean_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPLICATION PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T14:01:39.066079Z",
     "start_time": "2021-02-07T14:01:39.051065Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_test(path, num_rows=None):\n",
    "    \"\"\" \n",
    "    Process application_train.csv and application_test.csv and \n",
    "    return a pandas dataframe.\n",
    "    \"\"\"\n",
    "    train = pd.read_csv(os.path.join(path, 'application_train.csv'), nrows=num_rows)\n",
    "    test = pd.read_csv(os.path.join(path, 'application_test.csv'), nrows=num_rows)\n",
    "    df = train.append(test)\n",
    "    del train, test; gc.collect()\n",
    "    # Data cleaning\n",
    "    df = df[df['CODE_GENDER'] != 'XNA'] # 4 people with XNA code gender\n",
    "    df = df[df['AMT_INCOME_TOTAL'] < 20000000] # Max income in test is 4M; train has a 117M value\n",
    "    df['DAYS_EMPLOYED'].replace(0, np.nan, inplace=True)\n",
    "    df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\n",
    "    \n",
    "    # Flag_document features - count and kurtosis\n",
    "    docs = [f for f in df.columns if 'FLAG_DOC' in f]\n",
    "    df['DOCUMENT_COUNT'] = df[docs].sum(axis=1)\n",
    "    df['NEW_DOC_KURT'] = df[docs].kurtosis(axis=1)\n",
    "    # Categorical age - based on target=1 plot\n",
    "    df['AGE_RANGE'] = df['DAYS_BIRTH'].apply(lambda x: get_age_label(x))\n",
    "    \n",
    "    # New features based on External sources\n",
    "    df['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3'] \n",
    "    df['EXT_SOURCES_WEIGHTED'] = df.EXT_SOURCE_1 * 2 + df.EXT_SOURCE_2 * 1 + df.EXT_SOURCE_3 * 3\n",
    "    np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
    "    for function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n",
    "        feature_name = 'EXT_SOURCES_{}'.format(function_name.upper())\n",
    "        df[feature_name] = eval('np.{}'.format(function_name))(df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n",
    "        \n",
    "    # Credit ratios\n",
    "    df['CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
    "    df['CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
    "    # Income ratios\n",
    "    df['ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
    "    df['INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
    "    df['INCOME_TO_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_BIRTH']\n",
    "    # Time ratios\n",
    "    df['EMPLOYED_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['ID_TO_BIRTH_RATIO'] = df['DAYS_ID_PUBLISH'] / df['DAYS_BIRTH']\n",
    "    df['CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n",
    "    df['CAR_TO_EMPLOYED_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n",
    "    df['PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n",
    "    \n",
    "    # Groupby: Statistics for applications in the same group\n",
    "    group = ['ORGANIZATION_TYPE', 'NAME_EDUVATION_TYPE', 'OCCUPATION_TYPE', 'AGE_RANGE', 'CODE_GENDER']\n",
    "    df = do_median(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_MEDIAN')\n",
    "    df = do_std(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_STD')\n",
    "    df = do_mean(df, group, 'AMT_INCOME_TOTAL', 'GROUP_INCOME_MEAN')\n",
    "    df = do_std(df, group, 'AMT_INCOME_TOTAL', 'GROUP_INCOME_STD')\n",
    "    df = do_mean(df, group, 'CREDIT_TO_ANNUITY_RATIO', 'GROUP_CREDIT_TO_ANNUITY_MEAN')\n",
    "    df = do_std(df, group, 'CREDIT_TO_ANNUITY_RATIO', 'GROUP_CREDIT_TO_ANNUITY_STD')\n",
    "    df = do_mean(df, group, 'AMT_CREDIT', 'GROUP_CREDIT_MEAN')\n",
    "    df = do_mean(df, group, 'AMT_ANNUITY', 'GROUP_ANNUITY_MEAN')\n",
    "    df = do_std(df, group, 'AMT_ANNUITY', 'GROUP_ANNUITY_STD')\n",
    "    \n",
    "    # Encode categorical features (LabelEncoder)\n",
    "    df, le_encoded_cols = label_encoder(df, None)\n",
    "    df = drop_application_columns(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T14:01:39.081092Z",
     "start_time": "2021-02-07T14:01:39.067080Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_application_columns(df):\n",
    "    \"\"\" Drop features based on permutation feature importance. \"\"\"\n",
    "    drop_list = [\n",
    "        'CNT_CHILDREN', 'CNT_FAM_MEMBERS', 'HOUR_APPR_PROCESS_START',\n",
    "        'FLAG_EMP_PHONE', 'FLAG_MOBIL', 'FLAG_CONT_MOBILE', 'FLAG_EMAIL', 'FLAG_PHONE',\n",
    "        'FLAG_OWN_REALTY', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',\n",
    "        'REG_CITY_NOT_WORK_CITY', 'OBS_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE',\n",
    "        'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_YEAR', \n",
    "        'COMMONAREA_MODE', 'NONLIVINGAREA_MODE', 'ELEVATORS_MODE', 'NONLIVINGAREA_AVG',\n",
    "        'FLOORSMIN_MEDI', 'LANDAREA_MODE', 'NONLIVINGAREA_MEDI', 'LIVINGAPARTMENTS_MODE',\n",
    "        'FLOORSMIN_AVG', 'LANDAREA_AVG', 'FLOORSMIN_MODE', 'LANDAREA_MEDI',\n",
    "        'COMMONAREA_MEDI', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'BASEMENTAREA_AVG',\n",
    "        'BASEMENTAREA_MODE', 'NONLIVINGAPARTMENTS_MEDI', 'BASEMENTAREA_MEDI', \n",
    "        'LIVINGAPARTMENTS_AVG', 'ELEVATORS_AVG', 'YEARS_BUILD_MEDI', 'ENTRANCES_MODE',\n",
    "        'NONLIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'LIVINGAPARTMENTS_MEDI',\n",
    "        'YEARS_BUILD_MODE', 'YEARS_BEGINEXPLUATATION_AVG', 'ELEVATORS_MEDI', 'LIVINGAREA_MEDI',\n",
    "        'YEARS_BEGINEXPLUATATION_MODE', 'NONLIVINGAPARTMENTS_AVG', 'HOUSETYPE_MODE',\n",
    "        'FONDKAPREMONT_MODE', 'EMERGENCYSTATE_MODE'\n",
    "    ]\n",
    "    # Drop most flag document columns\n",
    "    for doc_num in [2,4,5,6,7,9,10,11,12,13,14,15,16,17,19,20,21]:\n",
    "        drop_list.append('FLAG_DOCUMENT_{}'.format(doc_num))\n",
    "    df.drop(drop_list, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T14:01:39.097107Z",
     "start_time": "2021-02-07T14:01:39.082093Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_age_label(days_birth):\n",
    "    \"\"\"Return the age group label(int)\"\"\"\n",
    "    age_years = -days_birth/365\n",
    "    if age_years < 27: return 1\n",
    "    elif age_years < 40: return 2\n",
    "    elif age_years < 50: return 3\n",
    "    elif age_years < 65: return 4\n",
    "    elif age_years < 99: return 5\n",
    "    else: return 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUREAU PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T14:01:39.113121Z",
     "start_time": "2021-02-07T14:01:39.098108Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bureau(path, num_rows=None):\n",
    "    \"\"\"Process bureau.csv and bureau_balance.csv and return a pandas dataframe.\"\"\"\n",
    "    bureau = pd.read_csv(os.path.join(path, 'bureau.csv'), nrows=num_rows)\n",
    "    # Credit duration and credit/account end date difference\n",
    "    bureau['CREDIT_DURATION'] = -bureau['DAYS_CREDIT'] + bureau['DAYS_CREDIT_ENDDATE']\n",
    "    bureau['ENDDATE_DIF'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n",
    "    # Credit to debt ratio and difference\n",
    "    bureau['DEBT_PERCENTAGE'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    bureau['DEBT_CREDIT_DIFF'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    bureau['CREDIT_TO_ANNUITY_RATIO'] = bureau['AMT_CREDIT_SUM'] / bureau['AMT_ANNUITY']\n",
    "    \n",
    "    # One-hot encoder\n",
    "    bureau, categorical_cols = one_hot_encoder(bureau, nan_as_category=False)\n",
    "    # Join bureau balance features\n",
    "    bureau = bureau.merge(get_bureau_balance(path, num_rows), how='left', on='SK_ID_BUREAU')\n",
    "    # Flag months with late payments (days past due)\n",
    "    bureau['STATUS_12345'] = 0\n",
    "    for i in range(1, 6):\n",
    "        bureau['STATUS_12345'] += bureau['STATUS_{}'.format(i)]\n",
    "        \n",
    "    # Aggregate by the number of months in balance and merge with bureau (loan length agg)\n",
    "    features = ['AMT_CREDIT_MAX_OVERDUE', 'AMT_CREDIT_SUM_OVERDUE', 'AMT_CREDIT_SUM', 'AMT_CEDIT_SUM_DEBT', 'DEBT_PERCENTAGE', 'DEBT_CREDIT_DIFF', 'STATUS_0', 'STATUS_12345']\n",
    "    agg_length = bureau.groupby('MONTHS_BALANCE_SIZE')[features].mean().reset_index()\n",
    "    agg_length.rename({feat: 'LL_' + feat for feat in features}, axis=1, inplace=True)\n",
    "    bureau = bureau.merge(agg_length, how='left', one_hot_encoder='MONTHS_BALANCE_SIZE')\n",
    "    del agg_length; gc.collect()\n",
    "    \n",
    "    # General loans aggregations\n",
    "    agg_bureau = group(bureau, 'BUREAU_', BUREAU_AGG)\n",
    "    # Active and closed aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    agg_bureau = group_and_merge(active, agg_bureau, 'BUREAU_ACTIVE_', BUREAU_ACTIVE_AGG)\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    agg_bureau = group_and_merge(closed, agg_bureau, 'BUREAU_CLOSED_', BUREAU_CLOSED_AGG)\n",
    "    del active, closed; gc.collect()\n",
    "    # Aggregations for the main loan types\n",
    "    for credit_type in ['Consumer credit', 'Credit card', 'Mortgage', 'Car loan', 'Microloan']:\n",
    "        type_df =  bureau[bureau['CREDIT_TYPE_' + credit_type] == 1]\n",
    "        prefix = 'BUREAU_' + credit_type.split(' ')[0].upper() + '_'\n",
    "        agg_bureau = group_and_merge(type_df, agg_bureau, prefix, BUREAU_TIME_AGG)\n",
    "        del time_frame_df; gc.collect()\n",
    "        \n",
    "    # Last loan max overdue\n",
    "    sort_bureau = bureau.sort_values(by=['DAYS_CREDIT'])\n",
    "    gr = sort_bureau.groupby('SK_ID_CURR')['AMT_CREDIT_MAX_OVERDUE'].last().reset_index()\n",
    "    gr.rename({'AMT_CREDIT_MAX_OVERDUE': 'BUREAU_LAST_LOAN_MAX_OVERDUE'}, inplace=True)\n",
    "    agg_bureau = agg_bureau.merge(gr, on='SK_ID_CURR', how='left')\n",
    "    # Ratios: total debt/total credit and active loans debt/active loans credit\n",
    "    agg_bureau['BUREAU_DEBT_OVER_CREDIT'] = \\\n",
    "        agg_bureau['BUREAU_AMT_CREDIT_SUM_DEBT_SUM']/agg_bureau['BUREAU_AMT_CREDIT_SUM_SUM']\n",
    "    agg_bureau['BUREAU_ACTIVE_DEBT_OVER_CREDIT'] = \\\n",
    "        agg_bureau['BUREAU_ACTIVE_AMT_CREDIT_SUM_DEBT_SUM']/agg_bureau['BUREAU_ACTIVE_AMT_CREDIT_SUM_SUM']\n",
    "    return agg_bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T14:05:08.606687Z",
     "start_time": "2021-02-07T14:05:08.600682Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bureau_balance(path, num_rows=None):\n",
    "    bb = pd.read_csv(os.path.join(path, 'bureau_balance.csv'), nrows=num_rows)\n",
    "    bb, categorical_cols = one_hot_encoder(bb, nan_as_category=False)\n",
    "    # Calculate rate for each category with decay\n",
    "    bb_processed = bb.groupby('SK_ID_BUREAU')[categorical_cols].mean().reset_index()\n",
    "    # Min, Max, Count and mean duration of payments(months)\n",
    "    agg = {'MONTH_BALANCE': ['min', 'max', 'mean', 'size']}\n",
    "    bb_processed = group_and_merge(bb, bb_processed, '', agg, 'SK_ID_BUREAU')\n",
    "    del bb; gc.collect()\n",
    "    return bb_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREVIOUS PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T14:09:37.130952Z",
     "start_time": "2021-02-07T14:09:37.115938Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_previous_applications(path, num_rows=None):\n",
    "    \"\"\" Process previous_application.csv and return a pandas dataframe \"\"\"\n",
    "    prev = pd.read_csv(os.path.join(path, 'previous_application.csv'), nrows=num_rows)\n",
    "    pay = pd.read_csv(os.path.join(path, 'installments_payments.csv'), nrows=num_rows)\n",
    "    \n",
    "    # One-hot encode most important categorical features\n",
    "    ohe_columns = ['NAME_CONTRACT_STATUS', 'NAME_CONTRACT_TYPE', 'CHANNEL_TYPE',\n",
    "        'NAME_TYPE_SUITE', 'NAME_YIELD_GROUP', 'PRODUCT_COMBINATION',\n",
    "        'NAME_PRODUCT_TYPE', 'NAME_CLIENT_TYPE']\n",
    "    prev, categorical_cols = one_hot_encoder(prev, ohe_columns, nan_as_category=False)\n",
    "    \n",
    "    # Feature engineering: ratios and difference\n",
    "    prev['APPLICATION_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n",
    "    prev['APPLICATION_CREDIT_RATIO'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    prev['CREDIT_TO_ANNUITY_RATIO'] = prev['AMT_CREDIT'] / prev['AMT_ANNUITY']\n",
    "    prev['DOWN_PAYMENT_TO_CREDIT'] = prev['AMT_DOWN_PAYMENT'] / prev['AMT_CREDIT']\n",
    "    \n",
    "    # Interest ratio on previous application (simplified)\n",
    "    total_payment = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n",
    "    prev['SIMPLE_INTERESTS'] = (total_payment/prev['AMT_CREDIT'] - 1)/prev['CNT_PAYMENT']\n",
    "\n",
    "    # Active loans - approved and not complete yet (last_due 365243)\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    active_df = approved[approved['DAYS_LAST_DUE'] == 365243]\n",
    "    # Find how much was already payed in active loans (using installments csv)\n",
    "    active_pay = pay[pay['SK_ID_PREV'].isin(active_df['SK_ID_PREV'])]\n",
    "    active_pay_agg = active_pay.groupby('SK_ID_PREV')[['AMT_INSTALMENT', 'AMT_PAYMENT']].sum()\n",
    "    active_pay_agg.reset_index(inplace= True)\n",
    "    # Active loans: difference of what was payed and installments\n",
    "    active_pay_agg['INSTALMENT_PAYMENT_DIFF'] = active_pay_agg['AMT_INSTALMENT'] - active_pay_agg['AMT_PAYMENT']\n",
    "    # Merge with active_df\n",
    "    active_df = active_df.merge(active_pay_agg, on= 'SK_ID_PREV', how= 'left')\n",
    "    active_df['REMAINING_DEBT'] = active_df['AMT_CREDIT'] - active_df['AMT_PAYMENT']\n",
    "    active_df['REPAYMENT_RATIO'] = active_df['AMT_PAYMENT'] / active_df['AMT_CREDIT']\n",
    "    # Perform aggregations for active applications\n",
    "    active_agg_df = group(active_df, 'PREV_ACTIVE_', PREVIOUS_ACTIVE_AGG)\n",
    "    active_agg_df['TOTAL_REPAYMENT_RATIO'] = active_agg_df['PREV_ACTIVE_AMT_PAYMENT_SUM']/\\\n",
    "                                             active_agg_df['PREV_ACTIVE_AMT_CREDIT_SUM']\n",
    "    del active_pay, active_pay_agg, active_df; gc.collect()\n",
    "    # Change 365.243 values to nan (missing)\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    # Days last due difference (scheduled x done)\n",
    "    prev['DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n",
    "    approved['DAYS_LAST_DUE_DIFF'] = approved['DAYS_LAST_DUE_1ST_VERSION'] - approved['DAYS_LAST_DUE']\n",
    "\n",
    "    # Categorical features\n",
    "    categorical_agg = {key: ['mean'] for key in categorical_cols}\n",
    "    # Perform general aggregations\n",
    "    agg_prev = group(prev, 'PREV_', {**PREVIOUS_AGG, **categorical_agg})\n",
    "    # Merge active loans dataframe on agg_prev\n",
    "    agg_prev = agg_prev.merge(active_agg_df, how='left', on='SK_ID_CURR')\n",
    "    del active_agg_df; gc.collect()\n",
    "    # Aggregations for approved and refused loans\n",
    "    agg_prev = group_and_merge(approved, agg_prev, 'APPROVED_', PREVIOUS_APPROVED_AGG)\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    agg_prev = group_and_merge(refused, agg_prev, 'REFUSED_', PREVIOUS_REFUSED_AGG)\n",
    "    del approved, refused; gc.collect()\n",
    "    # Aggregations for Consumer loans and Cash loans\n",
    "    for loan_type in ['Consumer loans', 'Cash loans']:\n",
    "        type_df = prev[prev['NAME_CONTRACT_TYPE_{}'.format(loan_type)] == 1]\n",
    "        prefix = 'PREV_' + loan_type.split(\" \")[0] + '_'\n",
    "        agg_prev = group_and_merge(type_df, agg_prev, prefix, PREVIOUS_LOAN_TYPE_AGG)\n",
    "        del type_df; gc.collect()\n",
    "\n",
    "    # Get the SK_ID_PREV for loans with late payments (days past due)\n",
    "    pay['LATE_PAYMENT'] = pay['DAYS_ENTRY_PAYMENT'] - pay['DAYS_INSTALMENT']\n",
    "    pay['LATE_PAYMENT'] = pay['LATE_PAYMENT'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    dpd_id = pay[pay['LATE_PAYMENT'] > 0]['SK_ID_PREV'].unique()\n",
    "    # Aggregations for loans with late payments\n",
    "    agg_dpd = group_and_merge(prev[prev['SK_ID_PREV'].isin(dpd_id)], agg_prev,\n",
    "                                    'PREV_LATE_', PREVIOUS_LATE_PAYMENTS_AGG)\n",
    "    del agg_dpd, dpd_id; gc.collect()\n",
    "    # Aggregations for loans in the last x months\n",
    "    for time_frame in [12, 24]:\n",
    "        time_frame_df = prev[prev['DAYS_DECISION'] >= -30*time_frame]\n",
    "        prefix = 'PREV_LAST{}M_'.format(time_frame)\n",
    "        agg_prev = group_and_merge(time_frame_df, agg_prev, prefix, PREVIOUS_TIME_AGG)\n",
    "        del time_frame_df; gc.collect()\n",
    "    del prev; gc.collect()\n",
    "    return agg_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS_CASH PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T14:10:21.355180Z",
     "start_time": "2021-02-07T14:10:21.343169Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pos_cash(path, num_rows=None):\n",
    "    \"\"\" Process POS_CASH_balance.csv and return a pandas dataframe. \"\"\"\n",
    "    pos = pd.read_csv(os.path.join(path, 'POS_CASH_balance.csv'), nrows= num_rows)\n",
    "    pos, categorical_cols = one_hot_encoder(pos, nan_as_category= False)\n",
    "    # Flag months with late payment\n",
    "    pos['LATE_PAYMENT'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    # Aggregate by SK_ID_CURR\n",
    "    categorical_agg = {key: ['mean'] for key in categorical_cols}\n",
    "    pos_agg = group(pos, 'POS_', {**POS_CASH_AGG, **categorical_agg})\n",
    "    # Sort and group by SK_ID_PREV\n",
    "    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n",
    "    gp = sort_pos.groupby('SK_ID_PREV')\n",
    "    df = pd.DataFrame()\n",
    "    df['SK_ID_CURR'] = gp['SK_ID_CURR'].first()\n",
    "    df['MONTHS_BALANCE_MAX'] = gp['MONTHS_BALANCE'].max()\n",
    "    # Percentage of previous loans completed and completed before initial term\n",
    "    df['POS_LOAN_COMPLETED_MEAN'] = gp['NAME_CONTRACT_STATUS_Completed'].mean()\n",
    "    df['POS_COMPLETED_BEFORE_MEAN'] = gp['CNT_INSTALMENT'].first() - gp['CNT_INSTALMENT'].last()\n",
    "    df['POS_COMPLETED_BEFORE_MEAN'] = df.apply(lambda x: 1 if x['POS_COMPLETED_BEFORE_MEAN'] > 0\n",
    "                                                and x['POS_LOAN_COMPLETED_MEAN'] > 0 else 0, axis=1)\n",
    "    # Number of remaining installments (future installments) and percentage from total\n",
    "    df['POS_REMAINING_INSTALMENTS'] = gp['CNT_INSTALMENT_FUTURE'].last()\n",
    "    df['POS_REMAINING_INSTALMENTS_RATIO'] = gp['CNT_INSTALMENT_FUTURE'].last()/gp['CNT_INSTALMENT'].last()\n",
    "    # Group by SK_ID_CURR and merge\n",
    "    df_gp = df.groupby('SK_ID_CURR').sum().reset_index()\n",
    "    df_gp.drop(['MONTHS_BALANCE_MAX'], axis=1, inplace= True)\n",
    "    pos_agg = pd.merge(pos_agg, df_gp, on= 'SK_ID_CURR', how= 'left')\n",
    "    del df, gp, df_gp, sort_pos; gc.collect()\n",
    "\n",
    "    # Percentage of late payments for the 3 most recent applications\n",
    "    pos = do_sum(pos, ['SK_ID_PREV'], 'LATE_PAYMENT', 'LATE_PAYMENT_SUM')\n",
    "    # Last month of each application\n",
    "    last_month_df = pos.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n",
    "    # Most recent applications (last 3)\n",
    "    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n",
    "    gp = sort_pos.iloc[last_month_df].groupby('SK_ID_CURR').tail(3)\n",
    "    gp_mean = gp.groupby('SK_ID_CURR').mean().reset_index()\n",
    "    pos_agg = pd.merge(pos_agg, gp_mean[['SK_ID_CURR','LATE_PAYMENT_SUM']], on='SK_ID_CURR', how='left')\n",
    "\n",
    "    # Drop some useless categorical features\n",
    "    drop_features = [\n",
    "        'POS_NAME_CONTRACT_STATUS_Canceled_MEAN', 'POS_NAME_CONTRACT_STATUS_Amortized debt_MEAN',\n",
    "        'POS_NAME_CONTRACT_STATUS_XNA_MEAN']\n",
    "    pos_agg.drop(drop_features, axis=1, inplace=True)\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALLMENTS PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T14:10:36.595043Z",
     "start_time": "2021-02-07T14:10:36.579029Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_installment_payments(path, num_rows= None):\n",
    "    \"\"\" Process installments_payments.csv and return a pandas dataframe. \"\"\"\n",
    "    pay = pd.read_csv(os.path.join(path, 'installments_payments.csv'), nrows= num_rows)\n",
    "    # Group payments and get Payment difference\n",
    "    pay = do_sum(pay, ['SK_ID_PREV', 'NUM_INSTALMENT_NUMBER'], 'AMT_PAYMENT', 'AMT_PAYMENT_GROUPED')\n",
    "    pay['PAYMENT_DIFFERENCE'] = pay['AMT_INSTALMENT'] - pay['AMT_PAYMENT_GROUPED']\n",
    "    pay['PAYMENT_RATIO'] = pay['AMT_INSTALMENT'] / pay['AMT_PAYMENT_GROUPED']\n",
    "    pay['PAID_OVER_AMOUNT'] = pay['AMT_PAYMENT'] - pay['AMT_INSTALMENT']\n",
    "    pay['PAID_OVER'] = (pay['PAID_OVER_AMOUNT'] > 0).astype(int)\n",
    "    # Payment Entry: Days past due and Days before due\n",
    "    pay['DPD'] = pay['DAYS_ENTRY_PAYMENT'] - pay['DAYS_INSTALMENT']\n",
    "    pay['DPD'] = pay['DPD'].apply(lambda x: 0 if x <= 0 else x)\n",
    "    pay['DBD'] = pay['DAYS_INSTALMENT'] - pay['DAYS_ENTRY_PAYMENT']\n",
    "    pay['DBD'] = pay['DBD'].apply(lambda x: 0 if x <= 0 else x)\n",
    "    # Flag late payment\n",
    "    pay['LATE_PAYMENT'] = pay['DBD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    # Percentage of payments that were late\n",
    "    pay['INSTALMENT_PAYMENT_RATIO'] = pay['AMT_PAYMENT'] / pay['AMT_INSTALMENT']\n",
    "    pay['LATE_PAYMENT_RATIO'] = pay.apply(lambda x: x['INSTALMENT_PAYMENT_RATIO'] if x['LATE_PAYMENT'] == 1 else 0, axis=1)\n",
    "    # Flag late payments that have a significant amount\n",
    "    pay['SIGNIFICANT_LATE_PAYMENT'] = pay['LATE_PAYMENT_RATIO'].apply(lambda x: 1 if x > 0.05 else 0)\n",
    "    # Flag k threshold late payments\n",
    "    pay['DPD_7'] = pay['DPD'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "    pay['DPD_15'] = pay['DPD'].apply(lambda x: 1 if x >= 15 else 0)\n",
    "    # Aggregations by SK_ID_CURR\n",
    "    pay_agg = group(pay, 'INS_', INSTALLMENTS_AGG)\n",
    "\n",
    "    # Installments in the last x months\n",
    "    for months in [36, 60]:\n",
    "        recent_prev_id = pay[pay['DAYS_INSTALMENT'] >= -30*months]['SK_ID_PREV'].unique()\n",
    "        pay_recent = pay[pay['SK_ID_PREV'].isin(recent_prev_id)]\n",
    "        prefix = 'INS_{}M_'.format(months)\n",
    "        pay_agg = group_and_merge(pay_recent, pay_agg, prefix, INSTALLMENTS_TIME_AGG)\n",
    "\n",
    "    # Last x periods trend features\n",
    "    group_features = ['SK_ID_CURR', 'SK_ID_PREV', 'DPD', 'LATE_PAYMENT',\n",
    "                      'PAID_OVER_AMOUNT', 'PAID_OVER', 'DAYS_INSTALMENT']\n",
    "    gp = pay[group_features].groupby('SK_ID_CURR')\n",
    "    func = partial(trend_in_last_k_instalment_features, periods= INSTALLMENTS_LAST_K_TREND_PERIODS)\n",
    "    g = parallel_apply(gp, func, index_name='SK_ID_CURR', chunk_size=10000).reset_index()\n",
    "    pay_agg = pay_agg.merge(g, on='SK_ID_CURR', how='left')\n",
    "\n",
    "    # Last loan features\n",
    "    g = parallel_apply(gp, installments_last_loan_features, index_name='SK_ID_CURR', chunk_size=10000).reset_index()\n",
    "    pay_agg = pay_agg.merge(g, on='SK_ID_CURR', how='left')\n",
    "    return pay_agg\n",
    "\n",
    "\n",
    "def trend_in_last_k_instalment_features(gr, periods):\n",
    "    gr_ = gr.copy()\n",
    "    gr_.sort_values(['DAYS_INSTALMENT'], ascending=False, inplace=True)\n",
    "    features = {}\n",
    "\n",
    "    for period in periods:\n",
    "        gr_period = gr_.iloc[:period]\n",
    "        features = add_trend_feature(features, gr_period, 'DPD',\n",
    "                                           '{}_TREND_'.format(period))\n",
    "        features = add_trend_feature(features, gr_period, 'PAID_OVER_AMOUNT',\n",
    "                                           '{}_TREND_'.format(period))\n",
    "    return features\n",
    "\n",
    "\n",
    "def installments_last_loan_features(gr):\n",
    "    gr_ = gr.copy()\n",
    "    gr_.sort_values(['DAYS_INSTALMENT'], ascending=False, inplace=True)\n",
    "    last_installment_id = gr_['SK_ID_PREV'].iloc[0]\n",
    "    gr_ = gr_[gr_['SK_ID_PREV'] == last_installment_id]\n",
    "\n",
    "    features = {}\n",
    "    features = add_features_in_group(features, gr_, 'DPD',\n",
    "                                     ['sum', 'mean', 'max', 'std'],\n",
    "                                     'LAST_LOAN_')\n",
    "    features = add_features_in_group(features, gr_, 'LATE_PAYMENT',\n",
    "                                     ['count', 'mean'],\n",
    "                                     'LAST_LOAN_')\n",
    "    features = add_features_in_group(features, gr_, 'PAID_OVER_AMOUNT',\n",
    "                                     ['sum', 'mean', 'max', 'min', 'std'],\n",
    "                                     'LAST_LOAN_')\n",
    "    features = add_features_in_group(features, gr_, 'PAID_OVER',\n",
    "                                     ['count', 'mean'],\n",
    "                                     'LAST_LOAN_')\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREDIT CARD PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T14:10:49.694959Z",
     "start_time": "2021-02-07T14:10:49.678945Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_credit_card(path, num_rows= None):\n",
    "    \"\"\" Process credit_card_balance.csv and return a pandas dataframe. \"\"\"\n",
    "    cc = pd.read_csv(os.path.join(path, 'credit_card_balance.csv'), nrows= num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=False)\n",
    "    cc.rename(columns={'AMT_RECIVABLE': 'AMT_RECEIVABLE'}, inplace=True)\n",
    "    # Amount used from limit\n",
    "    cc['LIMIT_USE'] = cc['AMT_BALANCE'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "    # Current payment / Min payment\n",
    "    cc['PAYMENT_DIV_MIN'] = cc['AMT_PAYMENT_CURRENT'] / cc['AMT_INST_MIN_REGULARITY']\n",
    "    # Late payment\n",
    "    cc['LATE_PAYMENT'] = cc['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    # How much drawing of limit\n",
    "    cc['DRAWING_LIMIT_RATIO'] = cc['AMT_DRAWINGS_ATM_CURRENT'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "    # Aggregations by SK_ID_CURR\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(CREDIT_CARD_AGG)\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    cc_agg.reset_index(inplace= True)\n",
    "\n",
    "    # Last month balance of each credit card application\n",
    "    last_ids = cc.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n",
    "    last_months_df = cc[cc.index.isin(last_ids)]\n",
    "    cc_agg = group_and_merge(last_months_df,cc_agg,'CC_LAST_', {'AMT_BALANCE': ['mean', 'max']})\n",
    "\n",
    "    # Aggregations for last x months\n",
    "    for months in [12, 24, 48]:\n",
    "        cc_prev_id = cc[cc['MONTHS_BALANCE'] >= -months]['SK_ID_PREV'].unique()\n",
    "        cc_recent = cc[cc['SK_ID_PREV'].isin(cc_prev_id)]\n",
    "        prefix = 'INS_{}M_'.format(months)\n",
    "        cc_agg = group_and_merge(cc_recent, cc_agg, prefix, CREDIT_CARD_TIME_AGG)\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T14:11:01.464665Z",
     "start_time": "2021-02-07T14:11:01.439643Z"
    }
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(name, time.time() - t0))\n",
    "\n",
    "\n",
    "def group(df_to_agg, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
    "    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n",
    "    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper())\n",
    "                               for e in agg_df.columns.tolist()])\n",
    "    return agg_df.reset_index()\n",
    "\n",
    "\n",
    "def group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
    "    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by= aggregate_by)\n",
    "    return df_to_merge.merge(agg_df, how='left', on= aggregate_by)\n",
    "\n",
    "\n",
    "def do_mean(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].mean().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_median(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].median().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_std(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].std().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_sum(df, group_cols, counted, agg_name):\n",
    "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(\n",
    "        columns={counted: agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def one_hot_encoder(df, categorical_columns=None, nan_as_category=True):\n",
    "    \"\"\"Create a new column for each categorical value in categorical columns. \"\"\"\n",
    "    original_columns = list(df.columns)\n",
    "    if not categorical_columns:\n",
    "        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    categorical_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, categorical_columns\n",
    "\n",
    "\n",
    "def label_encoder(df, categorical_columns=None):\n",
    "    \"\"\"Encode categorical values as integers (0,1,2,3...) with pandas.factorize. \"\"\"\n",
    "    if not categorical_columns:\n",
    "        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    for col in categorical_columns:\n",
    "        df[col], uniques = pd.factorize(df[col])\n",
    "    return df, categorical_columns\n",
    "\n",
    "\n",
    "def add_features(feature_name, aggs, features, feature_names, groupby):\n",
    "    feature_names.extend(['{}_{}'.format(feature_name, agg) for agg in aggs])\n",
    "\n",
    "    for agg in aggs:\n",
    "        if agg == 'kurt':\n",
    "            agg_func = kurtosis\n",
    "        elif agg == 'iqr':\n",
    "            agg_func = iqr\n",
    "        else:\n",
    "            agg_func = agg\n",
    "\n",
    "        g = groupby[feature_name].agg(agg_func).reset_index().rename(index=str,\n",
    "                                                                     columns={feature_name: '{}_{}'.format(feature_name,agg)})\n",
    "        features = features.merge(g, on='SK_ID_CURR', how='left')\n",
    "    return features, feature_names\n",
    "\n",
    "\n",
    "def add_features_in_group(features, gr_, feature_name, aggs, prefix):\n",
    "    for agg in aggs:\n",
    "        if agg == 'sum':\n",
    "            features['{}{}_sum'.format(prefix, feature_name)] = gr_[feature_name].sum()\n",
    "        elif agg == 'mean':\n",
    "            features['{}{}_mean'.format(prefix, feature_name)] = gr_[feature_name].mean()\n",
    "        elif agg == 'max':\n",
    "            features['{}{}_max'.format(prefix, feature_name)] = gr_[feature_name].max()\n",
    "        elif agg == 'min':\n",
    "            features['{}{}_min'.format(prefix, feature_name)] = gr_[feature_name].min()\n",
    "        elif agg == 'std':\n",
    "            features['{}{}_std'.format(prefix, feature_name)] = gr_[feature_name].std()\n",
    "        elif agg == 'count':\n",
    "            features['{}{}_count'.format(prefix, feature_name)] = gr_[feature_name].count()\n",
    "        elif agg == 'skew':\n",
    "            features['{}{}_skew'.format(prefix, feature_name)] = skew(gr_[feature_name])\n",
    "        elif agg == 'kurt':\n",
    "            features['{}{}_kurt'.format(prefix, feature_name)] = kurtosis(gr_[feature_name])\n",
    "        elif agg == 'iqr':\n",
    "            features['{}{}_iqr'.format(prefix, feature_name)] = iqr(gr_[feature_name])\n",
    "        elif agg == 'median':\n",
    "            features['{}{}_median'.format(prefix, feature_name)] = gr_[feature_name].median()\n",
    "    return features\n",
    "\n",
    "\n",
    "def add_trend_feature(features, gr, feature_name, prefix):\n",
    "    y = gr[feature_name].values\n",
    "    try:\n",
    "        x = np.arange(0, len(y)).reshape(-1, 1)\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(x, y)\n",
    "        trend = lr.coef_[0]\n",
    "    except:\n",
    "        trend = np.nan\n",
    "    features['{}{}'.format(prefix, feature_name)] = trend\n",
    "    return features\n",
    "\n",
    "\n",
    "def parallel_apply(groups, func, index_name='Index', num_workers=0, chunk_size=100000):\n",
    "    if num_workers <= 0: num_workers = NUM_THREADS\n",
    "    #n_chunks = np.ceil(1.0 * groups.ngroups / chunk_size)\n",
    "    indeces, features = [], []\n",
    "    for index_chunk, groups_chunk in chunk_groups(groups, chunk_size):\n",
    "        with mp.pool.Pool(num_workers) as executor:\n",
    "            features_chunk = executor.map(func, groups_chunk)\n",
    "        features.extend(features_chunk)\n",
    "        indeces.extend(index_chunk)\n",
    "\n",
    "    features = pd.DataFrame(features)\n",
    "    features.index = indeces\n",
    "    features.index.name = index_name\n",
    "    return features\n",
    "\n",
    "\n",
    "def chunk_groups(groupby_object, chunk_size):\n",
    "    n_groups = groupby_object.ngroups\n",
    "    group_chunk, index_chunk = [], []\n",
    "    for i, (index, df) in enumerate(groupby_object):\n",
    "        group_chunk.append(df)\n",
    "        index_chunk.append(index)\n",
    "        if (i + 1) % chunk_size == 0 or i + 1 == n_groups:\n",
    "            group_chunk_, index_chunk_ = group_chunk.copy(), index_chunk.copy()\n",
    "            group_chunk, index_chunk = [], []\n",
    "            yield index_chunk_, group_chunk_\n",
    "\n",
    "\n",
    "def reduce_memory(df):\n",
    "    \"\"\"Reduce memory usage of a dataframe by setting data types. \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Initial df memory usage is {:.2f} MB for {} columns'\n",
    "          .format(start_mem, len(df.columns)))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            cmin = df[col].min()\n",
    "            cmax = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                # Can use unsigned int here too\n",
    "                if cmin > np.iinfo(np.int8).min and cmax < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif cmin > np.iinfo(np.int16).min and cmax < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if cmin > np.finfo(np.float16).min and cmax < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    memory_reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "    print('Final memory usage is: {:.2f} MB - decreased by {:.1f}%'.format(end_mem, memory_reduction))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIGURATIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-07T14:11:14.468495Z",
     "start_time": "2021-02-07T14:11:14.109168Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/application_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-cf21578ba20e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'display.max_columns'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline total time\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-09670df19ea4>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(debug)\u001b[0m\n\u001b[0;32m      2\u001b[0m         \u001b[0mnum_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30000\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdebug\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'application_train and application_test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                 \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_train_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_DIRECTORY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Application dataframe shape: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Bureau and bureau_balance data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-2efe070088c9>\u001b[0m in \u001b[0;36mget_train_test\u001b[1;34m(path, num_rows)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'application_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'application_test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/application_train.csv'"
     ]
    }
   ],
   "source": [
    "# GENERAL CONFIGURATIONS\n",
    "NUM_THREADS = 4\n",
    "DATA_DIRECTORY = \"../input/\"\n",
    "SUBMISSION_SUFIX = \"_model2_04\"\n",
    "\n",
    "# INSTALLMENTS TREND PERIODS\n",
    "INSTALLMENTS_LAST_K_TREND_PERIODS =  [12, 24, 60, 120]\n",
    "\n",
    "# LIGHTGBM CONFIGURATION AND HYPER-PARAMETERS\n",
    "GENERATE_SUBMISSION_FILES = True\n",
    "STRATIFIED_KFOLD = False\n",
    "RANDOM_SEED = 737851\n",
    "NUM_FOLDS = 10\n",
    "EARLY_STOPPING = 100\n",
    "\n",
    "LIGHTGBM_PARAMS = {\n",
    "    'boosting_type': 'goss',\n",
    "    'n_estimators': 10000,\n",
    "    'learning_rate': 0.005134,\n",
    "    'num_leaves': 54,\n",
    "    'max_depth': 10,\n",
    "    'subsample_for_bin': 240000,\n",
    "    'reg_alpha': 0.436193,\n",
    "    'reg_lambda': 0.479169,\n",
    "    'colsample_bytree': 0.508716,\n",
    "    'min_split_gain': 0.024766,\n",
    "    'subsample': 1,\n",
    "    'is_unbalance': False,\n",
    "    'silent':-1,\n",
    "    'verbose':-1\n",
    "}\n",
    "\n",
    "# AGGREGATIONS\n",
    "BUREAU_AGG = {\n",
    "    'SK_ID_BUREAU': ['nunique'],\n",
    "    'DAYS_CREDIT': ['min', 'max', 'mean'],\n",
    "    'DAYS_CREDIT_ENDDATE': ['min', 'max'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n",
    "    'AMT_ANNUITY': ['mean'],\n",
    "    'DEBT_CREDIT_DIFF': ['mean', 'sum'],\n",
    "    'MONTHS_BALANCE_MEAN': ['mean', 'var'],\n",
    "    'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n",
    "    # Categorical\n",
    "    'STATUS_0': ['mean'],\n",
    "    'STATUS_1': ['mean'],\n",
    "    'STATUS_12345': ['mean'],\n",
    "    'STATUS_C': ['mean'],\n",
    "    'STATUS_X': ['mean'],\n",
    "    'CREDIT_ACTIVE_Active': ['mean'],\n",
    "    'CREDIT_ACTIVE_Closed': ['mean'],\n",
    "    'CREDIT_ACTIVE_Sold': ['mean'],\n",
    "    'CREDIT_TYPE_Consumer credit': ['mean'],\n",
    "    'CREDIT_TYPE_Credit card': ['mean'],\n",
    "    'CREDIT_TYPE_Car loan': ['mean'],\n",
    "    'CREDIT_TYPE_Mortgage': ['mean'],\n",
    "    'CREDIT_TYPE_Microloan': ['mean'],\n",
    "    # Group by loan duration features (months)\n",
    "    'LL_AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "    'LL_DEBT_CREDIT_DIFF': ['mean'],\n",
    "    'LL_STATUS_12345': ['mean'],\n",
    "}\n",
    "\n",
    "BUREAU_ACTIVE_AGG = {\n",
    "    'DAYS_CREDIT': ['max', 'mean'],\n",
    "    'DAYS_CREDIT_ENDDATE': ['min', 'max'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean'],\n",
    "    'DAYS_CREDIT_UPDATE': ['min', 'mean'],\n",
    "    'DEBT_PERCENTAGE': ['mean'],\n",
    "    'DEBT_CREDIT_DIFF': ['mean'],\n",
    "    'CREDIT_TO_ANNUITY_RATIO': ['mean'],\n",
    "    'MONTHS_BALANCE_MEAN': ['mean', 'var'],\n",
    "    'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n",
    "}\n",
    "\n",
    "BUREAU_CLOSED_AGG = {\n",
    "    'DAYS_CREDIT': ['max', 'var'],\n",
    "    'DAYS_CREDIT_ENDDATE': ['max'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['max', 'sum'],\n",
    "    'DAYS_CREDIT_UPDATE': ['max'],\n",
    "    'ENDDATE_DIF': ['mean'],\n",
    "    'STATUS_12345': ['mean'],\n",
    "}\n",
    "\n",
    "BUREAU_LOAN_TYPE_AGG = {\n",
    "    'DAYS_CREDIT': ['mean', 'max'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['mean', 'max'],\n",
    "    'AMT_CREDIT_SUM': ['mean', 'max'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['mean', 'max'],\n",
    "    'DEBT_PERCENTAGE': ['mean'],\n",
    "    'DEBT_CREDIT_DIFF': ['mean'],\n",
    "    'DAYS_CREDIT_ENDDATE': ['max'],\n",
    "}\n",
    "\n",
    "BUREAU_TIME_AGG = {\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "    'AMT_CREDIT_SUM': ['max', 'sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['mean', 'sum'],\n",
    "    'DEBT_PERCENTAGE': ['mean'],\n",
    "    'DEBT_CREDIT_DIFF': ['mean'],\n",
    "    'STATUS_0': ['mean'],\n",
    "    'STATUS_12345': ['mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_AGG = {\n",
    "    'SK_ID_PREV': ['nunique'],\n",
    "    'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "    'AMT_DOWN_PAYMENT': ['max', 'mean'],\n",
    "    'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "    'RATE_DOWN_PAYMENT': ['max', 'mean'],\n",
    "    'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "    'CNT_PAYMENT': ['max', 'mean'],\n",
    "    'DAYS_TERMINATION': ['max'],\n",
    "    # Engineered features\n",
    "    'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n",
    "    'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean'],\n",
    "    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean', 'var'],\n",
    "    'DOWN_PAYMENT_TO_CREDIT': ['mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_ACTIVE_AGG = {\n",
    "    'SK_ID_PREV': ['nunique'],\n",
    "    'SIMPLE_INTERESTS': ['mean'],\n",
    "    'AMT_ANNUITY': ['max', 'sum'],\n",
    "    'AMT_APPLICATION': ['max', 'mean'],\n",
    "    'AMT_CREDIT': ['sum'],\n",
    "    'AMT_DOWN_PAYMENT': ['max', 'mean'],\n",
    "    'DAYS_DECISION': ['min', 'mean'],\n",
    "    'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
    "    # Engineered features\n",
    "    'AMT_PAYMENT': ['sum'],\n",
    "    'INSTALMENT_PAYMENT_DIFF': ['mean', 'max'],\n",
    "    'REMAINING_DEBT': ['max', 'mean', 'sum'],\n",
    "    'REPAYMENT_RATIO': ['mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_APPROVED_AGG = {\n",
    "    'SK_ID_PREV': ['nunique'],\n",
    "    'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "    'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "    'AMT_DOWN_PAYMENT': ['max'],\n",
    "    'AMT_GOODS_PRICE': ['max'],\n",
    "    'HOUR_APPR_PROCESS_START': ['min', 'max'],\n",
    "    'DAYS_DECISION': ['min', 'mean'],\n",
    "    'CNT_PAYMENT': ['max', 'mean'],\n",
    "    'DAYS_TERMINATION': ['mean'],\n",
    "    # Engineered features\n",
    "    'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n",
    "    'APPLICATION_CREDIT_DIFF': ['max'],\n",
    "    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n",
    "    # The following features are only for approved applications\n",
    "    'DAYS_FIRST_DRAWING': ['max', 'mean'],\n",
    "    'DAYS_FIRST_DUE': ['min', 'mean'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
    "    'DAYS_LAST_DUE': ['max', 'mean'],\n",
    "    'DAYS_LAST_DUE_DIFF': ['min', 'max', 'mean'],\n",
    "    'SIMPLE_INTERESTS': ['min', 'max', 'mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_REFUSED_AGG = {\n",
    "    'AMT_APPLICATION': ['max', 'mean'],\n",
    "    'AMT_CREDIT': ['min', 'max'],\n",
    "    'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "    'CNT_PAYMENT': ['max', 'mean'],\n",
    "    # Engineered features\n",
    "    'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean', 'var'],\n",
    "    'APPLICATION_CREDIT_RATIO': ['min', 'mean'],\n",
    "    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_LATE_PAYMENTS_AGG = {\n",
    "    'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
    "    # Engineered features\n",
    "    'APPLICATION_CREDIT_DIFF': ['min'],\n",
    "    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_LOAN_TYPE_AGG = {\n",
    "    'AMT_CREDIT': ['sum'],\n",
    "    'AMT_ANNUITY': ['mean', 'max'],\n",
    "    'SIMPLE_INTERESTS': ['min', 'mean', 'max', 'var'],\n",
    "    'APPLICATION_CREDIT_DIFF': ['min', 'var'],\n",
    "    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n",
    "    'DAYS_DECISION': ['max'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['max', 'mean'],\n",
    "    'CNT_PAYMENT': ['mean'],\n",
    "}\n",
    "\n",
    "PREVIOUS_TIME_AGG = {\n",
    "    'AMT_CREDIT': ['sum'],\n",
    "    'AMT_ANNUITY': ['mean', 'max'],\n",
    "    'SIMPLE_INTERESTS': ['mean', 'max'],\n",
    "    'DAYS_DECISION': ['min', 'mean'],\n",
    "    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n",
    "    # Engineered features\n",
    "    'APPLICATION_CREDIT_DIFF': ['min'],\n",
    "    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n",
    "    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n",
    "    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n",
    "}\n",
    "\n",
    "POS_CASH_AGG = {\n",
    "    'SK_ID_PREV': ['nunique'],\n",
    "    'MONTHS_BALANCE': ['min', 'max', 'size'],\n",
    "    'SK_DPD': ['max', 'mean', 'sum', 'var'],\n",
    "    'SK_DPD_DEF': ['max', 'mean', 'sum'],\n",
    "    'LATE_PAYMENT': ['mean']\n",
    "}\n",
    "\n",
    "INSTALLMENTS_AGG = {\n",
    "    'SK_ID_PREV': ['size', 'nunique'],\n",
    "    'DAYS_ENTRY_PAYMENT': ['min', 'max', 'mean'],\n",
    "    'AMT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n",
    "    'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "    'DPD': ['max', 'mean', 'var'],\n",
    "    'DBD': ['max', 'mean', 'var'],\n",
    "    'PAYMENT_DIFFERENCE': ['mean'],\n",
    "    'PAYMENT_RATIO': ['mean'],\n",
    "    'LATE_PAYMENT': ['mean', 'sum'],\n",
    "    'SIGNIFICANT_LATE_PAYMENT': ['mean', 'sum'],\n",
    "    'LATE_PAYMENT_RATIO': ['mean'],\n",
    "    'DPD_7': ['mean'],\n",
    "    'DPD_15': ['mean'],\n",
    "    'PAID_OVER': ['mean']\n",
    "}\n",
    "\n",
    "INSTALLMENTS_TIME_AGG = {\n",
    "    'SK_ID_PREV': ['size'],\n",
    "    'DAYS_ENTRY_PAYMENT': ['min', 'max', 'mean'],\n",
    "    'AMT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n",
    "    'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "    'DPD': ['max', 'mean', 'var'],\n",
    "    'DBD': ['max', 'mean', 'var'],\n",
    "    'PAYMENT_DIFFERENCE': ['mean'],\n",
    "    'PAYMENT_RATIO': ['mean'],\n",
    "    'LATE_PAYMENT': ['mean'],\n",
    "    'SIGNIFICANT_LATE_PAYMENT': ['mean'],\n",
    "    'LATE_PAYMENT_RATIO': ['mean'],\n",
    "    'DPD_7': ['mean'],\n",
    "    'DPD_15': ['mean'],\n",
    "}\n",
    "\n",
    "CREDIT_CARD_AGG = {\n",
    "    'MONTHS_BALANCE': ['min'],\n",
    "    'AMT_BALANCE': ['max'],\n",
    "    'AMT_CREDIT_LIMIT_ACTUAL': ['max'],\n",
    "    'AMT_DRAWINGS_ATM_CURRENT': ['max', 'sum'],\n",
    "    'AMT_DRAWINGS_CURRENT': ['max', 'sum'],\n",
    "    'AMT_DRAWINGS_POS_CURRENT': ['max', 'sum'],\n",
    "    'AMT_INST_MIN_REGULARITY': ['max', 'mean'],\n",
    "    'AMT_PAYMENT_TOTAL_CURRENT': ['max', 'mean', 'sum', 'var'],\n",
    "    'AMT_TOTAL_RECEIVABLE': ['max', 'mean'],\n",
    "    'CNT_DRAWINGS_ATM_CURRENT': ['max', 'mean', 'sum'],\n",
    "    'CNT_DRAWINGS_CURRENT': ['max', 'mean', 'sum'],\n",
    "    'CNT_DRAWINGS_POS_CURRENT': ['mean'],\n",
    "    'SK_DPD': ['mean', 'max', 'sum'],\n",
    "    'SK_DPD_DEF': ['max', 'sum'],\n",
    "    'LIMIT_USE': ['max', 'mean'],\n",
    "    'PAYMENT_DIV_MIN': ['min', 'mean'],\n",
    "    'LATE_PAYMENT': ['max', 'sum'],\n",
    "}\n",
    "\n",
    "CREDIT_CARD_TIME_AGG = {\n",
    "    'CNT_DRAWINGS_ATM_CURRENT': ['mean'],\n",
    "    'SK_DPD': ['max', 'sum'],\n",
    "    'AMT_BALANCE': ['mean', 'max'],\n",
    "    'LIMIT_USE': ['max', 'mean']\n",
    "}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pd.set_option('display.max_rows', 60)\n",
    "    pd.set_option('display.max_columns', 100)\n",
    "    with timer(\"Pipeline total time\"):\n",
    "        main(debug= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
