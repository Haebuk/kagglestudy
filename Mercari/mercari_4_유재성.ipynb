{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"mercari_4_유재성.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9fLYZxaIUlBD"},"source":["# LGB and FM\n","https://www.kaggle.com/peterhurford/lgb-and-fm-18th-place-0-40604"]},{"cell_type":"code","metadata":{"id":"qpt_khh6UlBG","executionInfo":{"status":"ok","timestamp":1616411094689,"user_tz":-540,"elapsed":1121,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}}},"source":["import time\n","start_time = time.time()\n","\n","SUBMIT_MODE = True\n","\n","import pandas as pd\n","import numpy as np\n","import gc\n","import string\n","import re\n","\n","from nltk.corpus import stopwords\n","\n","from scipy.sparse import csr_matrix, hstack\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_selection.univariate_selection import SelectKBest, f_regression\n","from sklearn.preprocessing import LabelBinarizer\n","#!pip install wordbatch\n","from wordbatch.pipelines import WordBatch\n","from wordbatch.batcher import Batcher\n","from wordbatch.extractors import WordBag\n","from wordbatch.models import FM_FTRL\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import Ridge\n","from sklearn.naive_bayes import MultinomialNB\n","import lightgbm as lgb\n"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"4CyDGfHcUlBI","executionInfo":{"status":"ok","timestamp":1616411095052,"user_tz":-540,"elapsed":659,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}}},"source":["#from google.colab import drive\n","\n","#drive.mount('/content/gdrive/')"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"CrlcTj6sUlBI","executionInfo":{"status":"ok","timestamp":1616411096259,"user_tz":-540,"elapsed":578,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}}},"source":["def rmse(predicted, actual):\n","  return np.sqrt(((predicted - actual) ** 2).mean())"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"6n6QtOvOWTIo","executionInfo":{"status":"ok","timestamp":1616411097981,"user_tz":-540,"elapsed":851,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}}},"source":["def split_cat(text):\n","  try:\n","    return text.split('/')\n","  except:\n","    return ('No Label', 'No Label', 'No Label')"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"EzRlYK5kUlBJ","executionInfo":{"status":"ok","timestamp":1616412525995,"user_tz":-540,"elapsed":968,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}}},"source":["class TargetEncoder:\n","    # Adapted from https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\n","    def __repr__(self):\n","        return 'TargetEncoder'\n","\n","    def __init__(self, cols, smoothing=1, min_samples_leaf=1, noise_level=0, keep_original=False):\n","        self.cols = cols\n","        self.smoothing = smoothing\n","        self.min_samples_leaf = min_samples_leaf\n","        self.noise_level = noise_level\n","        self.keep_original = keep_original\n","\n","    @staticmethod\n","    def add_noise(series, noise_level):\n","        return series * (1 + noise_level * np.random.randn(len(series)))\n","\n","    def encode(self, train, test, target):\n","        for col in self.cols:\n","            if self.keep_original:\n","                train[col + '_te'], test[col + '_te'] = self.encode_column(train[col], test[col], target)\n","            else:\n","                train[col], test[col] = self.encode_column(train[col], test[col], target)\n","        return train, test\n","\n","    def encode_column(self, trn_series, tst_series, target):\n","        temp = pd.concat([trn_series, target], axis=1)\n","        # Compute target mean\n","        averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n","        # Compute smoothing\n","        smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - self.min_samples_leaf) / self.smoothing))\n","        # Apply average function to all target data\n","        prior = target.mean()\n","        # The bigger the count the less full_avg is taken into account\n","        averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n","        averages.drop(['mean', 'count'], axis=1, inplace=True)\n","        # Apply averages to trn and tst series\n","        ft_trn_series = pd.merge(\n","            trn_series.to_frame(trn_series.name),\n","            averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n","            on=trn_series.name,\n","            how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n","        # pd.merge does not keep the index so restore it\n","        ft_trn_series.index = trn_series.index\n","        ft_tst_series = pd.merge(\n","            tst_series.to_frame(tst_series.name),\n","            averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n","            on=tst_series.name,\n","            how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n","        # pd.merge does not keep the index so restore it\n","        ft_tst_series.index = tst_series.index\n","        return self.add_noise(ft_trn_series, self.noise_level), self.add_noise(ft_tst_series, self.noise_level)   \n"],"execution_count":83,"outputs":[]},{"cell_type":"code","metadata":{"id":"eqQELH7XUlBJ","executionInfo":{"status":"ok","timestamp":1616412542077,"user_tz":-540,"elapsed":811,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}}},"source":["def to_number(x):\n","    try:\n","        if not x.isdigit():\n","            return 0\n","        x = int(x)\n","        if x > 100:\n","            return 100\n","        else:\n","            return x\n","    except:\n","        return 0"],"execution_count":84,"outputs":[]},{"cell_type":"code","metadata":{"id":"WfD2GYT0ayNL","executionInfo":{"status":"ok","timestamp":1616412552928,"user_tz":-540,"elapsed":948,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}}},"source":["def sum_numbers(desc):\n","    if not isinstance(desc, str):\n","        return 0\n","    try:\n","        return sum([to_number(s) for s in desc.split()])\n","    except:\n","        return 0"],"execution_count":85,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":274},"id":"smr9mXF2a8Mm","executionInfo":{"status":"error","timestamp":1616412764106,"user_tz":-540,"elapsed":664,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"898147c5-cf0b-4afd-9109-3ccd38e0b620"},"source":["# Define helpers for text normalization\n","nltk.download('stopwords')\n","stopwords = {x: 1 for x in stopwords.words('english')}\n","non_alphanums = re.compile(u'[^A-Az-z0-9]+')\n","non_alphanumpunct = re.compile(u'[^A-Za-z0-9\\.?!,; \\(\\)\\[\\]\\'\\\"\\$]+')\n","RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])"],"execution_count":89,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-89-a0fe5679af97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define helpers for text normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnon_alphanums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'[^A-Az-z0-9]+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnon_alphanumpunct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'[^A-Za-z0-9\\.?!,; \\(\\)\\[\\]\\'\\\"\\$]+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'words'"]}]},{"cell_type":"code","metadata":{"id":"Wf4LCiihbd7d","executionInfo":{"status":"ok","timestamp":1616412377417,"user_tz":-540,"elapsed":794,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}}},"source":["def normalize_text(text):\n","    return u\" \".join(\n","        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")] \\\n","         if len(x) > 1 and x not in stopwords])"],"execution_count":75,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4RI8lafrcTLy","executionInfo":{"status":"ok","timestamp":1616411102246,"user_tz":-540,"elapsed":3345,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"030850c7-4205-4460-f1de-c60d18700029"},"source":["def clean_name(x):\n","    if len(x):\n","        x = non_alphanums.sub(' ', x).split()\n","        if len(x):\n","            return x[0].lower()\n","    return ''\n","\n","print('[{}] Finished defining stuff'.format(time.time() - start_time))"],"execution_count":54,"outputs":[{"output_type":"stream","text":["[6.0510618686676025] Finished defining stuff\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W1tU5pGKcgx-","executionInfo":{"status":"ok","timestamp":1616411110476,"user_tz":-540,"elapsed":11260,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"6462d231-dcaa-450e-bd21-919a8162bf7c"},"source":["train = pd.read_table('/content/gdrive/MyDrive/input (1)/train.tsv', engine='c',\n","                       dtype={'item_condition_id': 'category',\n","                              'shipping': 'category'},\n","                       converters = {'category_name': split_cat})\n","test = pd.read_table('/content/gdrive/MyDrive/input (1)/test.tsv', engine='c',\n","                       dtype={'item_condition_id': 'category',\n","                              'shipping': 'category'},\n","                       converters = {'category_name': split_cat})\n","print('[{}] Finished load data'.format(time.time() - start_time))"],"execution_count":55,"outputs":[{"output_type":"stream","text":["[15.893715620040894] Finished load data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KwwCl_EUlBK","executionInfo":{"status":"ok","timestamp":1616411110479,"user_tz":-540,"elapsed":10974,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"53b55f56-cb9c-4d7c-f8f7-ee0797dc3d70"},"source":["train['is_train'] = 1\n","test['is_train'] = 0\n","print('[{}] Compiled train / test'.format(time.time() - start_time))\n","print('Train shape: ', train.shape)\n","print('Test shape:', test.shape)"],"execution_count":56,"outputs":[{"output_type":"stream","text":["[15.907379388809204] Compiled train / test\n","Train shape:  (1482535, 9)\n","Test shape: (693359, 8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DX08gYUmdvga","executionInfo":{"status":"ok","timestamp":1616411110480,"user_tz":-540,"elapsed":10678,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"396fde20-d1be-4556-ce7c-7afe9afb5586"},"source":["train = train[train.price != 0].reset_index(drop=True)\n","print('[{}] Removed nonzero price'.format(time.time() - start_time))\n","print('Train shape:', train.shape)\n","print('Test shape:', test.shape)"],"execution_count":57,"outputs":[{"output_type":"stream","text":["[16.218364238739014] Removed nonzero price\n","Train shape: (1481661, 9)\n","Test shape: (693359, 8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TjjfKtCpeATf","executionInfo":{"status":"ok","timestamp":1616411110481,"user_tz":-540,"elapsed":10443,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}}},"source":["y = np.log1p(train['price'])\n","nrow_train = train.shape[0]"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZCRI1InxeFLe","executionInfo":{"status":"ok","timestamp":1616411110857,"user_tz":-540,"elapsed":10314,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"7300afae-c385-4e5e-fdb0-3f448c70591d"},"source":["merge = pd.concat([train, test])\n","submission = test[['test_id']]\n","print('[{}] Compiled merge'.format(time.time() - start_time))\n","print('Merge shape:', merge.shape)"],"execution_count":59,"outputs":[{"output_type":"stream","text":["[16.60564088821411] Compiled merge\n","Merge shape: (2175020, 10)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jqLqmN_5eZq_","executionInfo":{"status":"ok","timestamp":1616411112292,"user_tz":-540,"elapsed":11667,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"5273a326-4f76-4d2e-b761-04e39bc8f11f"},"source":["del train\n","del test\n","merge.drop(['train_id', 'test_id', 'price'], axis=1, inplace=True)\n","gc.collect()\n","print('[{}] Garbage collection'.format(time.time() - start_time))"],"execution_count":60,"outputs":[{"output_type":"stream","text":["[17.580373764038086] Garbage collection\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CzdKIHwGel-6","executionInfo":{"status":"ok","timestamp":1616411117006,"user_tz":-540,"elapsed":16036,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"884fe3d7-60f2-48d7-cbec-0193e2fba1be"},"source":["merge['gencat_name'] = merge['category_name'].str.get(0).replace('', 'missing').astype('category')\n","merge['subcat1_name'] = merge['category_name'].str.get(1).fillna('missing').astype('category')\n","merge['subcat2_name'] = merge['category_name'].str.get(2).fillna('missing').astype('category')\n","merge.drop('category_name', axis=1, inplace=True)\n","print('[{}] Split categories completed.'.format(time.time() - start_time))"],"execution_count":61,"outputs":[{"output_type":"stream","text":["[22.475751399993896] Split categories completed.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NOdL--1dfEXZ","executionInfo":{"status":"ok","timestamp":1616411117416,"user_tz":-540,"elapsed":16102,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"09757e99-ae40-4ec2-ce06-03d0a252c161"},"source":["merge['item_condition_id'] = merge['item_condition_id'].cat.add_categories(['missing']).fillna('missing')\n","merge['shipping'] = merge['shipping'].cat.add_categories(['missing']).fillna('missing')\n","merge['item_description'].fillna('missing', inplace=True)\n","merge['brand_name'] = merge['brand_name'].fillna('missing').astype('category')\n","print('[{}] Handle missing completed.'.format(time.time() - start_time))"],"execution_count":62,"outputs":[{"output_type":"stream","text":["[23.015498399734497] Handle missing completed.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wp51ZbzLfqRN","executionInfo":{"status":"ok","timestamp":1616411204009,"user_tz":-540,"elapsed":102622,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"f49c00df-7c61-4fcc-c727-b56bca8f3eb6"},"source":["merge['name_first'] = merge['name'].apply(clean_name)\n","print('[{}] FE 1/37'.format(time.time() - start_time))\n","merge['name_first_count'] = merge.groupby('name_first')['name_first'].transform('count')\n","print('[{}] FE 2/37'.format(time.time() - start_time))\n","merge['gencat_name_count'] = merge.groupby('gencat_name')['gencat_name'].transform('count')\n","print('[{}] FE 3/37'.format(time.time() - start_time))\n","merge['subcat1_name_count'] = merge.groupby('subcat1_name')['subcat1_name'].transform('count')\n","print('[{}] FE 4/37'.format(time.time() - start_time))\n","merge['subcat2_name_count'] = merge.groupby('subcat2_name')['subcat2_name'].transform('count')\n","print('[{}] FE 5/37'.format(time.time() - start_time))\n","merge['brand_name_count'] = merge.groupby('brand_name')['brand_name'].transform('count')\n","print('[{}] FE 6/37'.format(time.time() - start_time))\n","merge['NameLower'] = merge.name.str.count('[a-z]')\n","print('[{}] FE 7/37'.format(time.time() - start_time))\n","merge['DescriptionLower'] = merge.item_description.str.count('[a-z]')\n","print('[{}] FE 8/37'.format(time.time() - start_time))\n","merge['NameUpper'] = merge.name.str.count('[A-Z]')\n","print('[{}] FE 9/37'.format(time.time() - start_time))\n","merge['DescriptionUpper'] = merge.item_description.str.count('[A-Z]')\n","print('[{}] FE 10/37'.format(time.time() - start_time))\n","merge['name_len'] = merge['name'].apply(lambda x: len(x))\n","print('[{}] FE 11/37'.format(time.time() - start_time))\n","merge['des_len'] = merge['item_description'].apply(lambda x: len(x))\n","print('[{}] FE 12/37'.format(time.time() - start_time))\n","merge['name_desc_len_ratio'] = merge['name_len']/merge['des_len']\n","print('[{}] FE 13/37'.format(time.time() - start_time))\n","merge['desc_word_count'] = merge['item_description'].apply(lambda x: len(x.split()))\n","print('[{}] FE 14/37'.format(time.time() - start_time))\n","merge['mean_des'] = merge['item_description'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) / len(x)) * 10\n","print('[{}] FE 15/37'.format(time.time() - start_time))\n","merge['name_word_count'] = merge['name'].apply(lambda x: len(x.split()))\n","print('[{}] FE 16/37'.format(time.time() - start_time))\n","merge['mean_name'] = merge['name'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) / len(x))  * 10\n","print('[{}] FE 17/37'.format(time.time() - start_time))\n","merge['desc_letters_per_word'] = merge['des_len'] / merge['desc_word_count']\n","print('[{}] FE 18/37'.format(time.time() - start_time))\n","merge['name_letters_per_word'] = merge['name_len'] / merge['name_word_count']\n","print('[{}] FE 19/37'.format(time.time() - start_time))\n","merge['NameLowerRatio'] = merge['NameLower'] / merge['name_len']\n","print('[{}] FE 20/37'.format(time.time() - start_time))\n","merge['DescriptionLowerRatio'] = merge['DescriptionLower'] / merge['des_len']\n","print('[{}] FE 21/37'.format(time.time() - start_time))\n","merge['NameUpperRatio'] = merge['NameUpper'] / merge['name_len']\n","print('[{}] FE 22/37'.format(time.time() - start_time))\n","merge['DescriptionUpperRatio'] = merge['DescriptionUpper'] / merge['des_len']\n","print('[{}] FE 23/37'.format(time.time() - start_time))\n","merge['NamePunctCount'] = merge.name.str.count(RE_PUNCTUATION)\n","print('[{}] FE 24/37'.format(time.time() - start_time))\n","merge['DescriptionPunctCount'] = merge.item_description.str.count(RE_PUNCTUATION)\n","print('[{}] FE 25/37'.format(time.time() - start_time))\n","merge['NamePunctCountRatio'] = merge['NamePunctCount'] / merge['name_word_count']\n","print('[{}] FE 26/37'.format(time.time() - start_time))\n","merge['DescriptionPunctCountRatio'] = merge['DescriptionPunctCount'] / merge['desc_word_count']\n","print('[{}] FE 27/37'.format(time.time() - start_time))\n","merge['NameDigitCount'] = merge.name.str.count('[0-9]')\n","print('[{}] FE 28/37'.format(time.time() - start_time))\n","merge['DescriptionDigitCount'] = merge.item_description.str.count('[0-9]')\n","print('[{}] FE 29/37'.format(time.time() - start_time))\n","merge['NameDigitCountRatio'] = merge['NameDigitCount'] / merge['name_word_count']\n","print('[{}] FE 30/37'.format(time.time() - start_time))\n","merge['DescriptionDigitCountRatio'] = merge['DescriptionDigitCount']/merge['desc_word_count']\n","print('[{}] FE 31/37'.format(time.time() - start_time))\n","merge['stopword_ratio_desc'] = merge['item_description'].apply(lambda x: len([w for w in x.split() if w in stopwords])) / merge['desc_word_count']\n","print('[{}] FE 32/37'.format(time.time() - start_time))\n","merge['num_sum'] = merge['item_description'].apply(sum_numbers) \n","print('[{}] FE 33/37'.format(time.time() - start_time))\n","merge['weird_characters_desc'] = merge['item_description'].str.count(non_alphanumpunct)\n","print('[{}] FE 34/37'.format(time.time() - start_time))\n","merge['weird_characters_name'] = merge['name'].str.count(non_alphanumpunct)\n","print('[{}] FE 35/37'.format(time.time() - start_time))\n","merge['prices_count'] = merge['item_description'].str.count('[rm]')\n","print('[{}] FE 36/37'.format(time.time() - start_time))\n","merge['price_in_name'] = merge['item_description'].str.contains('[rm]', regex=False).astype('int')\n","print('[{}] FE 37/37'.format(time.time() - start_time))\n"],"execution_count":63,"outputs":[{"output_type":"stream","text":["[25.39472985267639] FE 1/37\n","[25.609914302825928] FE 2/37\n","[25.636143445968628] FE 3/37\n","[25.6603901386261] FE 4/37\n","[25.68544340133667] FE 5/37\n","[25.71013379096985] FE 6/37\n","[29.406676769256592] FE 7/37\n","[45.34774470329285] FE 8/37\n","[47.34826946258545] FE 9/37\n","[52.30910682678223] FE 10/37\n","[52.98735165596008] FE 11/37\n","[53.86513018608093] FE 12/37\n","[53.87625050544739] FE 13/37\n","[57.37128233909607] FE 14/37\n","[61.200392961502075] FE 15/37\n","[62.43785810470581] FE 16/37\n","[63.975412130355835] FE 17/37\n","[63.98659873008728] FE 18/37\n","[63.997546911239624] FE 19/37\n","[64.00802731513977] FE 20/37\n","[64.01850771903992] FE 21/37\n","[64.02924752235413] FE 22/37\n","[64.03958630561829] FE 23/37\n","[65.3788993358612] FE 24/37\n","[68.89011120796204] FE 25/37\n","[68.9012017250061] FE 26/37\n","[68.91201877593994] FE 27/37\n","[70.33301949501038] FE 28/37\n","[73.82167100906372] FE 29/37\n","[73.83567690849304] FE 30/37\n","[73.84670925140381] FE 31/37\n","[82.98139834403992] FE 32/37\n","[95.46020412445068] FE 33/37\n","[102.46889972686768] FE 34/37\n","[104.57946729660034] FE 35/37\n","[108.82040858268738] FE 36/37\n","[109.47570824623108] FE 37/37\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CqZQ3ErJgYT1","executionInfo":{"status":"ok","timestamp":1616411204013,"user_tz":-540,"elapsed":102272,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}}},"source":["cols = set(merge.columns.values)\n","basic_cols = {'name', 'item_condition_id', 'brand_name', 'shipping',\n","              'item_description', 'gencat_name', 'subcat1_name',\n","              'subcat2_name', 'name_first', 'is_train'}"],"execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"id":"NNqJF0BXg-v1","executionInfo":{"status":"ok","timestamp":1616411204015,"user_tz":-540,"elapsed":101566,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}}},"source":["cols_to_normalize = cols - basic_cols - {'price_in_name'}\n","other_cols = basic_cols | {'price_in_name'}"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ohSBpERhJdv","executionInfo":{"status":"ok","timestamp":1616411205274,"user_tz":-540,"elapsed":102537,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"c0d017e6-7e5c-4a30-e27b-0744f28389df"},"source":["merge_to_normalize = merge[list(cols_to_normalize)]\n","merge_to_normalize = (merge_to_normalize - merge_to_normalize.mean()) / (merge_to_normalize.max() - merge_to_normalize.min())\n","print('[{}] FE Normalized'.format(time.time() - start_time))"],"execution_count":66,"outputs":[{"output_type":"stream","text":["[110.87131142616272] FE Normalized\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VSdz3fzQhc5F","executionInfo":{"status":"ok","timestamp":1616411206169,"user_tz":-540,"elapsed":103130,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"a5ad30af-fe89-4792-c52d-aeaa00f564d2"},"source":["merge = merge[list(other_cols)]\n","merge = pd.concat([merge, merge_to_normalize], axis=1)\n","print('[{}] FE Merged.'.format(time.time() - start_time))"],"execution_count":67,"outputs":[{"output_type":"stream","text":["[111.61076593399048] FE Merged.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pyWgvrTMhtTH","executionInfo":{"status":"ok","timestamp":1616411206174,"user_tz":-540,"elapsed":102873,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"4e320526-fc62-4114-ac9d-6e1cda4ab39f"},"source":["del (merge_to_normalize)\n","gc.collect()\n","print('[{}] Garbage collection'.format(time.time() - start_time))"],"execution_count":68,"outputs":[{"output_type":"stream","text":["[111.73305416107178] Garbage collection\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"utvOOFsEh40g","executionInfo":{"status":"ok","timestamp":1616411207407,"user_tz":-540,"elapsed":103806,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}}},"source":["df_test = merge.loc[merge['is_train'] == 0]\n","df_train = merge.loc[merge['is_train'] == 1]\n","del merge\n","gc.collect()\n","df_test = df_test.drop(['is_train'], axis=1)\n","df_train = df_train.drop(['is_train'], axis=1)"],"execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e36WR3f1iGvN","executionInfo":{"status":"ok","timestamp":1616411207410,"user_tz":-540,"elapsed":103504,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"3060ce5e-64ff-4e45-b8fe-d8d885b58ea8"},"source":["if SUBMIT_MODE:\n","    y_train = y\n","    del y\n","    gc.collect()\n","else:\n","    df_train, df_test, y_train, y_test = train_test_split(df_train, y, test_size=0.2, random_state=144)\n","    \n","print('[{}] Splitting completed.'.format(time.time() - start_time))"],"execution_count":70,"outputs":[{"output_type":"stream","text":["[112.85792851448059] Splitting completed.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CtXYJWK6iVKH","executionInfo":{"status":"ok","timestamp":1616412459347,"user_tz":-540,"elapsed":1005,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}}},"source":["wb = WordBatch(normalize_text,\n","               extractor=(WordBag, {'hash_ngrams': 2,\n","                                    'hash_ngrams_weights': [1.5, 1.0],\n","                                    'hash_size': 2 ** 29,\n","                                    'norm': None,\n","                                    'tf': 'binary',\n","                                    'idf': None}))"],"execution_count":81,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":668},"id":"hpfOhR7EjGSS","executionInfo":{"status":"error","timestamp":1616412467015,"user_tz":-540,"elapsed":5849,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"10f924f1-85c9-49e8-ce44-293fa7f88e24"},"source":["#wb.dictionary_freeze = True\n","X_name_train = wb.fit_transform(df_train['name'])\n","X_name_test = wb.transform(df_test['name'])\n","del (wb)\n","mask = np.where(X_name_train.getnnz(axis=0) > 3)[0]\n","X_name_train = X_name_train[:, mask]\n","X_name_test = X_name_test[:, mask]\n","print(\"[{}] Vectorize 'name' completed.\".format(time.time() - start_time))"],"execution_count":82,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)","\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"/usr/local/lib/python3.7/dist-packages/wordbatch/pipelines/batch_transformer.py\", line 9, in batch_transform\n    return args[1].transform(args[0])\nAttributeError: 'tuple' object has no attribute 'transform'\n\"\"\"","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-82-e3aec7fe64a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#wb.dictionary_freeze = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_name_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_name_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_name_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wordbatch/pipelines/wordbatch.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, data, y, cache_features, input_split, reset, minibatch_size, batcher)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mbatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \t\treturn self.transform(data, y, cache_features, input_split, reset, update=True, minibatch_size= minibatch_size,\n\u001b[0;32m--> 104\u001b[0;31m \t\t                      batcher=batcher)\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \tdef partial_fit_transform(self, data, y=None, cache_features=None, input_split=False, minibatch_size=None,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wordbatch/pipelines/wordbatch.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, data, y, cache_features, input_split, reset, update, minibatch_size, batcher)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extract features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcache_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"load_features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wordbatch/pipelines/batch_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, data, input_split, merge_output, minibatch_size, batcher)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_fit\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wordbatch/pipelines/batch_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, data, input_split, merge_output, minibatch_size, batcher)\u001b[0m\n\u001b[1;32m     31\u001b[0m \t\treturn batcher.process_batches(batch_transform, data, [self.transformer],\n\u001b[1;32m     32\u001b[0m                                                \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \t\t                               minibatch_size= minibatch_size)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wordbatch/batcher.py\u001b[0m in \u001b[0;36mprocess_batches\u001b[0;34m(self, task, data, args, backend, backend_handle, input_split, merge_output, minibatch_size, procs, verbose)\u001b[0m\n\u001b[1;32m    211\u001b[0m                                         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                                         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                         \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                         \u001b[0;32melif\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"threading\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                                 \u001b[0;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'transform'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mcget2oQdny9","executionInfo":{"status":"ok","timestamp":1616412134347,"user_tz":-540,"elapsed":914,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"42a50266-b91c-4b30-9d18-f56724367a07"},"source":["df_train['name']"],"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0            MLB Cincinnati Reds T Shirt Size XL\n","1               Razer BlackWidow Chroma Keyboard\n","2                                 AVA-VIV Blouse\n","3                          Leather Horse Statues\n","4                           24K GOLD plated rose\n","                           ...                  \n","1481656               Free People Inspired Dress\n","1481657            Little mermaid handmade dress\n","1481658    21 day fix containers and eating plan\n","1481659                   World markets lanterns\n","1481660            Brand new lux de ville wallet\n","Name: name, Length: 1481661, dtype: object"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"code","metadata":{"id":"Wke9AuKuUlBL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7D5DaiMbarPw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sgg7aG5VarLS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gr1tjC24arJu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_BpYWzSuarFP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QNL9DUawarEB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"xLOokOAoaqzn","executionInfo":{"status":"error","timestamp":1616413394763,"user_tz":-540,"elapsed":121814,"user":{"displayName":"해벅","photoUrl":"","userId":"02404938411472623204"}},"outputId":"9e3d8499-aba6-4163-a95d-0afb5c2f58d8"},"source":["import time\n","start_time = time.time()\n","\n","SUBMIT_MODE = True\n","\n","\n","import pandas as pd\n","import numpy as np\n","import time\n","import gc\n","import string\n","import re\n","\n","from nltk.corpus import stopwords\n","\n","from scipy.sparse import csr_matrix, hstack\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_selection.univariate_selection import SelectKBest, f_regression\n","from sklearn.preprocessing import LabelBinarizer\n","\n","import wordbatch\n","from wordbatch.extractors import WordBag\n","from wordbatch.models import FM_FTRL\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import Ridge\n","from sklearn.naive_bayes import MultinomialNB\n","import lightgbm as lgb\n","\n","\n","def rmse(predicted, actual):\n","    return np.sqrt(((predicted - actual) ** 2).mean())\n","\n","\n","def split_cat(text):\n","    try:\n","        return text.split(\"/\")\n","    except:\n","        return (\"No Label\", \"No Label\", \"No Label\")\n","\n","\n","class TargetEncoder:\n","    # Adapted from https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\n","    def __repr__(self):\n","        return 'TargetEncoder'\n","\n","    def __init__(self, cols, smoothing=1, min_samples_leaf=1, noise_level=0, keep_original=False):\n","        self.cols = cols\n","        self.smoothing = smoothing\n","        self.min_samples_leaf = min_samples_leaf\n","        self.noise_level = noise_level\n","        self.keep_original = keep_original\n","\n","    @staticmethod\n","    def add_noise(series, noise_level):\n","        return series * (1 + noise_level * np.random.randn(len(series)))\n","\n","    def encode(self, train, test, target):\n","        for col in self.cols:\n","            if self.keep_original:\n","                train[col + '_te'], test[col + '_te'] = self.encode_column(train[col], test[col], target)\n","            else:\n","                train[col], test[col] = self.encode_column(train[col], test[col], target)\n","        return train, test\n","\n","    def encode_column(self, trn_series, tst_series, target):\n","        temp = pd.concat([trn_series, target], axis=1)\n","        # Compute target mean\n","        averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n","        # Compute smoothing\n","        smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - self.min_samples_leaf) / self.smoothing))\n","        # Apply average function to all target data\n","        prior = target.mean()\n","        # The bigger the count the less full_avg is taken into account\n","        averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n","        averages.drop(['mean', 'count'], axis=1, inplace=True)\n","        # Apply averages to trn and tst series\n","        ft_trn_series = pd.merge(\n","            trn_series.to_frame(trn_series.name),\n","            averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n","            on=trn_series.name,\n","            how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n","        # pd.merge does not keep the index so restore it\n","        ft_trn_series.index = trn_series.index\n","        ft_tst_series = pd.merge(\n","            tst_series.to_frame(tst_series.name),\n","            averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n","            on=tst_series.name,\n","            how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n","        # pd.merge does not keep the index so restore it\n","        ft_tst_series.index = tst_series.index\n","        return self.add_noise(ft_trn_series, self.noise_level), self.add_noise(ft_tst_series, self.noise_level)   \n","\n","\n","def to_number(x):\n","    try:\n","        if not x.isdigit():\n","            return 0\n","        x = int(x)\n","        if x > 100:\n","            return 100\n","        else:\n","            return x\n","    except:\n","        return 0\n","\n","def sum_numbers(desc):\n","    if not isinstance(desc, str):\n","        return 0\n","    try:\n","        return sum([to_number(s) for s in desc.split()])\n","    except:\n","        return 0\n","\n","\n","# Define helpers for text normalization\n","stopwords = {x: 1 for x in stopwords.words('english')}\n","non_alphanums = re.compile(u'[^A-Za-z0-9]+')\n","non_alphanumpunct = re.compile(u'[^A-Za-z0-9\\.?!,; \\(\\)\\[\\]\\'\\\"\\$]+')\n","RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])\n","\n","def normalize_text(text):\n","    return u\" \".join(\n","        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")] \\\n","         if len(x) > 1 and x not in stopwords])\n","\n","def clean_name(x):\n","    if len(x):\n","        x = non_alphanums.sub(' ', x).split()\n","        if len(x):\n","            return x[0].lower()\n","    return ''\n","\n","    \n","print('[{}] Finished defining stuff'.format(time.time() - start_time))\n","\n","\n","train = pd.read_table('/content/gdrive/MyDrive/input (1)/train.tsv', engine='c', \n","                      dtype={'item_condition_id': 'category',\n","                             'shipping': 'category',\n","                            }, \n","                     converters={'category_name': split_cat})\n","test = pd.read_table('/content/gdrive/MyDrive/input (1)/test.tsv', engine='c', \n","                      dtype={'item_condition_id': 'category',\n","                             'shipping': 'category',\n","                            },\n","                    converters={'category_name': split_cat})\n","print('[{}] Finished load data'.format(time.time() - start_time))\n","\n","train['is_train'] = 1\n","test['is_train'] = 0\n","print('[{}] Compiled train / test'.format(time.time() - start_time))\n","print('Train shape: ', train.shape)\n","print('Test shape: ', test.shape)\n","\n","train = train[train.price != 0].reset_index(drop=True)\n","print('[{}] Removed nonzero price'.format(time.time() - start_time))\n","print('Train shape: ', train.shape)\n","print('Test shape: ', test.shape)\n","\n","y = np.log1p(train['price'])\n","nrow_train = train.shape[0]\n","\n","merge = pd.concat([train, test])\n","submission = test[['test_id']]\n","print('[{}] Compiled merge'.format(time.time() - start_time))\n","print('Merge shape: ', merge.shape)\n","\n","\n","del train\n","del test\n","merge.drop(['train_id', 'test_id', 'price'], axis=1, inplace=True)\n","gc.collect()\n","print('[{}] Garbage collection'.format(time.time() - start_time))\n","\n","\n","merge['gencat_name'] = merge['category_name'].str.get(0).replace('', 'missing').astype('category')\n","merge['subcat1_name'] = merge['category_name'].str.get(1).fillna('missing').astype('category')\n","merge['subcat2_name'] = merge['category_name'].str.get(2).fillna('missing').astype('category')\n","merge.drop('category_name', axis=1, inplace=True)\n","print('[{}] Split categories completed.'.format(time.time() - start_time))\n","\n","merge['item_condition_id'] = merge['item_condition_id'].cat.add_categories(['missing']).fillna('missing')\n","merge['shipping'] = merge['shipping'].cat.add_categories(['missing']).fillna('missing')\n","merge['item_description'].fillna('missing', inplace=True)\n","merge['brand_name'] = merge['brand_name'].fillna('missing').astype('category')\n","print('[{}] Handle missing completed.'.format(time.time() - start_time))\n","\n","\n","merge['name_first'] = merge['name'].apply(clean_name)\n","print('[{}] FE 1/37'.format(time.time() - start_time))\n","merge['name_first_count'] = merge.groupby('name_first')['name_first'].transform('count')\n","print('[{}] FE 2/37'.format(time.time() - start_time))\n","merge['gencat_name_count'] = merge.groupby('gencat_name')['gencat_name'].transform('count')\n","print('[{}] FE 3/37'.format(time.time() - start_time))\n","merge['subcat1_name_count'] = merge.groupby('subcat1_name')['subcat1_name'].transform('count')\n","print('[{}] FE 4/37'.format(time.time() - start_time))\n","merge['subcat2_name_count'] = merge.groupby('subcat2_name')['subcat2_name'].transform('count')\n","print('[{}] FE 5/37'.format(time.time() - start_time))\n","merge['brand_name_count'] = merge.groupby('brand_name')['brand_name'].transform('count')\n","print('[{}] FE 6/37'.format(time.time() - start_time))\n","merge['NameLower'] = merge.name.str.count('[a-z]')\n","print('[{}] FE 7/37'.format(time.time() - start_time))\n","merge['DescriptionLower'] = merge.item_description.str.count('[a-z]')\n","print('[{}] FE 8/37'.format(time.time() - start_time))\n","merge['NameUpper'] = merge.name.str.count('[A-Z]')\n","print('[{}] FE 9/37'.format(time.time() - start_time))\n","merge['DescriptionUpper'] = merge.item_description.str.count('[A-Z]')\n","print('[{}] FE 10/37'.format(time.time() - start_time))\n","merge['name_len'] = merge['name'].apply(lambda x: len(x))\n","print('[{}] FE 11/37'.format(time.time() - start_time))\n","merge['des_len'] = merge['item_description'].apply(lambda x: len(x))\n","print('[{}] FE 12/37'.format(time.time() - start_time))\n","merge['name_desc_len_ratio'] = merge['name_len']/merge['des_len']\n","print('[{}] FE 13/37'.format(time.time() - start_time))\n","merge['desc_word_count'] = merge['item_description'].apply(lambda x: len(x.split()))\n","print('[{}] FE 14/37'.format(time.time() - start_time))\n","merge['mean_des'] = merge['item_description'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) / len(x)) * 10\n","print('[{}] FE 15/37'.format(time.time() - start_time))\n","merge['name_word_count'] = merge['name'].apply(lambda x: len(x.split()))\n","print('[{}] FE 16/37'.format(time.time() - start_time))\n","merge['mean_name'] = merge['name'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) / len(x))  * 10\n","print('[{}] FE 17/37'.format(time.time() - start_time))\n","merge['desc_letters_per_word'] = merge['des_len'] / merge['desc_word_count']\n","print('[{}] FE 18/37'.format(time.time() - start_time))\n","merge['name_letters_per_word'] = merge['name_len'] / merge['name_word_count']\n","print('[{}] FE 19/37'.format(time.time() - start_time))\n","merge['NameLowerRatio'] = merge['NameLower'] / merge['name_len']\n","print('[{}] FE 20/37'.format(time.time() - start_time))\n","merge['DescriptionLowerRatio'] = merge['DescriptionLower'] / merge['des_len']\n","print('[{}] FE 21/37'.format(time.time() - start_time))\n","merge['NameUpperRatio'] = merge['NameUpper'] / merge['name_len']\n","print('[{}] FE 22/37'.format(time.time() - start_time))\n","merge['DescriptionUpperRatio'] = merge['DescriptionUpper'] / merge['des_len']\n","print('[{}] FE 23/37'.format(time.time() - start_time))\n","merge['NamePunctCount'] = merge.name.str.count(RE_PUNCTUATION)\n","print('[{}] FE 24/37'.format(time.time() - start_time))\n","merge['DescriptionPunctCount'] = merge.item_description.str.count(RE_PUNCTUATION)\n","print('[{}] FE 25/37'.format(time.time() - start_time))\n","merge['NamePunctCountRatio'] = merge['NamePunctCount'] / merge['name_word_count']\n","print('[{}] FE 26/37'.format(time.time() - start_time))\n","merge['DescriptionPunctCountRatio'] = merge['DescriptionPunctCount'] / merge['desc_word_count']\n","print('[{}] FE 27/37'.format(time.time() - start_time))\n","merge['NameDigitCount'] = merge.name.str.count('[0-9]')\n","print('[{}] FE 28/37'.format(time.time() - start_time))\n","merge['DescriptionDigitCount'] = merge.item_description.str.count('[0-9]')\n","print('[{}] FE 29/37'.format(time.time() - start_time))\n","merge['NameDigitCountRatio'] = merge['NameDigitCount'] / merge['name_word_count']\n","print('[{}] FE 30/37'.format(time.time() - start_time))\n","merge['DescriptionDigitCountRatio'] = merge['DescriptionDigitCount']/merge['desc_word_count']\n","print('[{}] FE 31/37'.format(time.time() - start_time))\n","merge['stopword_ratio_desc'] = merge['item_description'].apply(lambda x: len([w for w in x.split() if w in stopwords])) / merge['desc_word_count']\n","print('[{}] FE 32/37'.format(time.time() - start_time))\n","merge['num_sum'] = merge['item_description'].apply(sum_numbers) \n","print('[{}] FE 33/37'.format(time.time() - start_time))\n","merge['weird_characters_desc'] = merge['item_description'].str.count(non_alphanumpunct)\n","print('[{}] FE 34/37'.format(time.time() - start_time))\n","merge['weird_characters_name'] = merge['name'].str.count(non_alphanumpunct)\n","print('[{}] FE 35/37'.format(time.time() - start_time))\n","merge['prices_count'] = merge['item_description'].str.count('[rm]')\n","print('[{}] FE 36/37'.format(time.time() - start_time))\n","merge['price_in_name'] = merge['item_description'].str.contains('[rm]', regex=False).astype('int')\n","print('[{}] FE 37/37'.format(time.time() - start_time))\n","\n","cols = set(merge.columns.values)\n","basic_cols = {'name', 'item_condition_id', 'brand_name',\n","  'shipping', 'item_description', 'gencat_name',\n","  'subcat1_name', 'subcat2_name', 'name_first', 'is_train'}\n","\n","cols_to_normalize = cols - basic_cols - {'price_in_name'}\n","other_cols = basic_cols | {'price_in_name'}\n","\n","merge_to_normalize = merge[list(cols_to_normalize)]\n","merge_to_normalize = (merge_to_normalize - merge_to_normalize.mean()) / (merge_to_normalize.max() - merge_to_normalize.min())\n","print('[{}] FE Normalized'.format(time.time() - start_time))\n","\n","merge = merge[list(other_cols)]\n","merge = pd.concat([merge, merge_to_normalize],axis=1)\n","print('[{}] FE Merged'.format(time.time() - start_time))\n","\n","del(merge_to_normalize)\n","gc.collect()\n","print('[{}] Garbage collection'.format(time.time() - start_time))\n","\n","\n","df_test = merge.loc[merge['is_train'] == 0]\n","df_train = merge.loc[merge['is_train'] == 1]\n","del merge\n","gc.collect()\n","df_test = df_test.drop(['is_train'], axis=1)\n","df_train = df_train.drop(['is_train'], axis=1)\n","\n","if SUBMIT_MODE:\n","    y_train = y\n","    del y\n","    gc.collect()\n","else:\n","    df_train, df_test, y_train, y_test = train_test_split(df_train, y, test_size=0.2, random_state=144)\n","\n","print('[{}] Splitting completed.'.format(time.time() - start_time))\n","\n","\n","wb = WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2,\n","                                                              \"hash_ngrams_weights\": [1.5, 1.0],\n","                                                              \"hash_size\": 2 ** 29,\n","                                                              \"norm\": None,\n","                                                              \"tf\": 'binary',\n","                                                              \"idf\": None,\n","                                                              }))\n","wb.dictionary_freeze = True\n","X_name_train = wb.fit_transform(df_train['name'])\n","X_name_test = wb.transform(df_test['name'])\n","del(wb)\n","mask = np.where(X_name_train.getnnz(axis=0) > 3)[0]\n","X_name_train = X_name_train[:, mask]\n","X_name_test = X_name_test[:, mask]\n","print('[{}] Vectorize `name` completed.'.format(time.time() - start_time))\n"],"execution_count":92,"outputs":[{"output_type":"stream","text":["[0.001630544662475586] Finished defining stuff\n","[9.882857322692871] Finished load data\n","[9.886682987213135] Compiled train / test\n","Train shape:  (1482535, 9)\n","Test shape:  (693359, 8)\n","[10.185098648071289] Removed nonzero price\n","Train shape:  (1481661, 9)\n","Test shape:  (693359, 8)\n","[10.5514976978302] Compiled merge\n","Merge shape:  (2175020, 10)\n","[11.521096467971802] Garbage collection\n","[16.436152458190918] Split categories completed.\n","[16.98466157913208] Handle missing completed.\n","[21.4402334690094] FE 1/37\n","[21.76729941368103] FE 2/37\n","[21.789425134658813] FE 3/37\n","[21.81127381324768] FE 4/37\n","[21.8343026638031] FE 5/37\n","[21.85753321647644] FE 6/37\n","[25.56296968460083] FE 7/37\n","[41.5116868019104] FE 8/37\n","[43.52388858795166] FE 9/37\n","[48.483853340148926] FE 10/37\n","[49.151068925857544] FE 11/37\n","[50.037089586257935] FE 12/37\n","[50.04812216758728] FE 13/37\n","[53.618956089019775] FE 14/37\n","[57.623435497283936] FE 15/37\n","[58.8881721496582] FE 16/37\n","[60.43563175201416] FE 17/37\n","[60.44714021682739] FE 18/37\n","[60.457953453063965] FE 19/37\n","[60.46926307678223] FE 20/37\n","[60.48261046409607] FE 21/37\n","[60.49313521385193] FE 22/37\n","[60.503185749053955] FE 23/37\n","[61.825310707092285] FE 24/37\n","[65.29030537605286] FE 25/37\n","[65.30120968818665] FE 26/37\n","[65.31163239479065] FE 27/37\n","[66.74565815925598] FE 28/37\n","[70.18318724632263] FE 29/37\n","[70.1941978931427] FE 30/37\n","[70.20497846603394] FE 31/37\n","[79.06839346885681] FE 32/37\n","[91.29087567329407] FE 33/37\n","[98.2568428516388] FE 34/37\n","[100.40541625022888] FE 35/37\n","[104.68311071395874] FE 36/37\n","[105.33122181892395] FE 37/37\n","[106.88505935668945] FE Normalized\n","[107.63702154159546] FE Merged\n","[107.71509051322937] Garbage collection\n","[108.96138620376587] Splitting completed.\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)","\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"/usr/local/lib/python3.7/dist-packages/wordbatch/pipelines/batch_transformer.py\", line 9, in batch_transform\n    return args[1].transform(args[0])\nAttributeError: 'tuple' object has no attribute 'transform'\n\"\"\"","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-92-9ae2f1721396>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m                                                               }))\n\u001b[1;32m    310\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary_freeze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m \u001b[0mX_name_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0mX_name_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wordbatch/pipelines/wordbatch.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, data, y, cache_features, input_split, reset, minibatch_size, batcher)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mbatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \t\treturn self.transform(data, y, cache_features, input_split, reset, update=True, minibatch_size= minibatch_size,\n\u001b[0;32m--> 104\u001b[0;31m \t\t                      batcher=batcher)\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \tdef partial_fit_transform(self, data, y=None, cache_features=None, input_split=False, minibatch_size=None,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wordbatch/pipelines/wordbatch.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, data, y, cache_features, input_split, reset, update, minibatch_size, batcher)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extract features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcache_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"load_features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wordbatch/pipelines/batch_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, data, input_split, merge_output, minibatch_size, batcher)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_fit\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wordbatch/pipelines/batch_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, data, input_split, merge_output, minibatch_size, batcher)\u001b[0m\n\u001b[1;32m     31\u001b[0m \t\treturn batcher.process_batches(batch_transform, data, [self.transformer],\n\u001b[1;32m     32\u001b[0m                                                \u001b[0minput_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \t\t                               minibatch_size= minibatch_size)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wordbatch/batcher.py\u001b[0m in \u001b[0;36mprocess_batches\u001b[0;34m(self, task, data, args, backend, backend_handle, input_split, merge_output, minibatch_size, procs, verbose)\u001b[0m\n\u001b[1;32m    211\u001b[0m                                         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                                         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                         \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                         \u001b[0;32melif\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"threading\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                                 \u001b[0;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'transform'"]}]},{"cell_type":"code","metadata":{"id":"2xa_asE5UlBM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VnjZ1BVyUlBN"},"source":[""],"execution_count":null,"outputs":[]}]}