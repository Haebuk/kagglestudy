{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- competition/dataset : [https://www.kaggle.com/c/mercari-price-suggestion-challenge](https://www.kaggle.com/c/mercari-price-suggestion-challenge)\n",
    "- date : 2021/03/19\n",
    "- original : [https://www.kaggle.com/rumbok/ridge-lb-0-41944](https://www.kaggle.com/rumbok/ridge-lb-0-41944)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge (LB 0.41943)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-19T23:23:23.804833Z",
     "iopub.status.busy": "2021-01-19T23:23:23.803834Z",
     "iopub.status.idle": "2021-01-19T23:23:23.811814Z",
     "shell.execute_reply": "2021-01-19T23:23:23.810817Z",
     "shell.execute_reply.started": "2021-01-19T23:23:23.804833Z"
    }
   },
   "source": [
    "**✏ 필사 1회** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T04:26:57.604399Z",
     "iopub.status.busy": "2021-03-19T04:26:57.604399Z",
     "iopub.status.idle": "2021-03-19T04:26:57.617364Z",
     "shell.execute_reply": "2021-03-19T04:26:57.616366Z",
     "shell.execute_reply.started": "2021-03-19T04:26:57.604399Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**os.environ**  \n",
    "문자열 환경을 나타내는 매핑 객체  \n",
    "환경을 조회하는 것뿐만 아니라 환경을 수정하는 데도 사용될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T04:26:57.619359Z",
     "iopub.status.busy": "2021-03-19T04:26:57.618361Z",
     "iopub.status.idle": "2021-03-19T04:26:57.632325Z",
     "shell.execute_reply": "2021-03-19T04:26:57.631355Z",
     "shell.execute_reply.started": "2021-03-19T04:26:57.619359Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['MKL_NUM_THREDS'] = '4'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "os.environ['JOBLIB_START_METHOD'] = 'forkserver'\n",
    "\n",
    "INPUT_PATH = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T04:26:57.634319Z",
     "iopub.status.busy": "2021-03-19T04:26:57.633321Z",
     "iopub.status.idle": "2021-03-19T04:26:57.712110Z",
     "shell.execute_reply": "2021-03-19T04:26:57.711146Z",
     "shell.execute_reply.started": "2021-03-19T04:26:57.634319Z"
    }
   },
   "outputs": [],
   "source": [
    "def Damerau_Levenshtein(seq1, seq2):\n",
    "    '''\n",
    "    두 시퀀스 사이의 Damerau-Levenshtein 거리를 계산하는 함수\n",
    "    Source: http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\n",
    "    \n",
    "    이 거리는 첫 번째 시퀀스를 두 번째 시퀀스로 변환하는데 필요한 추가, 삭제, 대체,\n",
    "    전환 횟수를 의미함.\n",
    "    일반적으로 문자열과 함께 사용되긴 하나, 유사한 객체들의 시퀀스에도 작동됨.\n",
    "    \n",
    "    전환은 연속적인 문자열의 교환이며, 다른 것들은 자체적으로 수행됨.\n",
    "    \n",
    "    이 함수는 실행하는데 O(N*M)의 시간복잡도와 O(M)의 공간복잡도를 가짐.\n",
    "    (N과 M은 두 시퀀스의 길이)\n",
    "    \n",
    "    >>> Demerau_Levenshtein('ba', 'abc')\n",
    "    2\n",
    "    >>> Demerau_Levenshtein('fee', 'deed')\n",
    "    2\n",
    "    \n",
    "    * 임의의 순서에도 작동함\n",
    "    >>> Demerau_Levenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
    "    2\n",
    "    '''\n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # len(seq1) + len(seq2) + 1 matrix에 기초함\n",
    "    # 현재 행과 이전 두 행이 동시에 필요함\n",
    "    oneago = None\n",
    "    thisrow = list(range(1, len(seq2)+1)) + [0]\n",
    "    for x in range(len(seq1)):\n",
    "        twoago, oneago, thisrow = (oneago, thisrow, [0] * len(seq2) + [x+1])\n",
    "        for y in range(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y-1] + 1\n",
    "            subcost = oneago[y-1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # transposition(전환)\n",
    "            if(x > 0 and y > 0 and seq1[x] == seq2[y-1]\n",
    "               and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y-2] + 1)\n",
    "    \n",
    "    return thisrow[len(seq2) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T04:26:57.716100Z",
     "iopub.status.busy": "2021-03-19T04:26:57.714105Z",
     "iopub.status.idle": "2021-03-19T04:26:57.757988Z",
     "shell.execute_reply": "2021-03-19T04:26:57.756990Z",
     "shell.execute_reply.started": "2021-03-19T04:26:57.716100Z"
    }
   },
   "outputs": [],
   "source": [
    "class SymSpell:\n",
    "    def __init__(self, max_edit_distance=3, verbose=0):\n",
    "        self.max_edit_distance = max_edit_distance\n",
    "        self.verbose = verbose\n",
    "        # 0: top suggestion\n",
    "        # 1: all sugestion of smallest edit distance\n",
    "        # 2: all suggestions <= max_edit_distance (slower, no early termination)\n",
    "        \n",
    "        self.dictionary = {}\n",
    "        self.longest_word_length = 0\n",
    "    \n",
    "    def get_deletes_list(self, w):\n",
    "        '''\n",
    "        단어를 입력하면 max_edit_distance 문자가 삭제된 문자열 리턴\n",
    "        '''\n",
    "        deletes = []\n",
    "        queue = [w]\n",
    "        for d in range(self.max_edit_distance):\n",
    "            temp_queue = []\n",
    "            for word in queue:\n",
    "                if len(word) > 1:\n",
    "                    for c in range(len(word)):\n",
    "                        word_minus_c = word[:c] + word[c+1:]\n",
    "                        if word_minus_c not in deletes:\n",
    "                            deletes.append(word_minus_c)\n",
    "                        if word_minus_c not in temp_queue:\n",
    "                            temp.queue.append(word_minus_c)\n",
    "            queue = temp_queue\n",
    "        \n",
    "        return deletes\n",
    "    \n",
    "    def create_dictionary_entry(self, w):\n",
    "        '''\n",
    "        단어 편집 및 딕셔너리에 삭제 실행\n",
    "        '''\n",
    "        # 단어가 딕셔너리에 있는지 확인\n",
    "        # 딕셔너리 형태: (suggested correction 목록, 말뭉치 내 단어 빈도수)\n",
    "        new_real_word_added = False\n",
    "        if w in self.dictionary:\n",
    "            # 말뭉치에서 단어의 증가 수\n",
    "            self.dictionary[w] = (self.dictionary[w][0], self.dictionary[w][1] + 1)\n",
    "        else:\n",
    "            self.dictionary[w] = ([], 1)\n",
    "            self.longest_word_length = max(self.longest_word_length, len(w))\n",
    "        \n",
    "        if self.dictionary[w][1] == 1:\n",
    "            # 말뭉치에서 처음 등장하는 단어\n",
    "            # 이미 파생된 단어로 딕셔너리에 들어있을 수 있음\n",
    "            # (실제 단어에서 문자 삭제)\n",
    "            # 말뭉치에서 단어의 빈도 수는 증가하지 않음\n",
    "            new_real_word_added = True\n",
    "            deletes = self.get_deletes_list(w)\n",
    "            for item in deletes:\n",
    "                if item in self.dictionary:\n",
    "                    # delete의 suggested correction 목록에 추가\n",
    "                    self.dictionary[item][0].append(w)\n",
    "                else:\n",
    "                    self.dictionary[item] = ([w], 0)\n",
    "        \n",
    "        return new_real_word_added\n",
    "    \n",
    "    def create_dictionary_from_arr(self, arr, token_pattern=r'[a-z]+'):\n",
    "        total_word_count = 0\n",
    "        unique_word_count = 0\n",
    "        \n",
    "        for line in arr:\n",
    "            # 알파벳이 아닌 문자와 분리\n",
    "            words = re.findall(token_pattern, line.lower())\n",
    "            for word in words:\n",
    "                total_word_count += 1\n",
    "                if self.create_dictionary_entry(word):\n",
    "                    unique_word_count += 1\n",
    "        \n",
    "        print('total words processed: %i'%total_word_count)\n",
    "        print('total unique words in corpus: %i'%unique_word_count)\n",
    "        print('total items in dictionary (corpus words and deletions): %i'%len(self.dictionary))\n",
    "        print('  edit distance for deletions: %i'%self.max_edit_distance)\n",
    "        print('  length of longest word in corpus: %i'%self.longest_word_length)\n",
    "        \n",
    "        return self.dictionary\n",
    "    \n",
    "    def create_dictionary(self, fname):\n",
    "        total_word_count = 0\n",
    "        unique_word_count = 0\n",
    "        \n",
    "        with open(fname) as file:\n",
    "            for line in file:\n",
    "                # 알파벳이 아닌 문자와 분리\n",
    "                words = re.findall('[a-z]+', line.lower())\n",
    "                for word in words:\n",
    "                    total_word_count += 1\n",
    "                    if self.create_dictionary_entry(word):\n",
    "                        unique_word_count += 1\n",
    "        \n",
    "        print('total words processed: %i'%total_word_count)\n",
    "        print('total unique words in corpus: %i'%unique_word_count)\n",
    "        print('total items in dictinary (corpus words and deletions): %i'%len(self.dictionary))\n",
    "        print('  edit distance for deletions: %i'%self.max_edit_distance)\n",
    "        print('  length of longest word in corpus: %i'%self.longest_word_length)\n",
    "        \n",
    "        return self.dictionary\n",
    "    \n",
    "    def get_suggestions(self, string, silent=False):\n",
    "        '''\n",
    "        철자가 잘못되었을 수 있는 단어에 대한 suggested correction 리스트 리턴\n",
    "        '''\n",
    "        if (len(string) - self.longest_word_length) > self.max_edit_distance:\n",
    "            if not silent:\n",
    "                print('no items in dictionary within maximum edit distance')\n",
    "            return []\n",
    "        \n",
    "        suggest_dict = {}\n",
    "        min_suggest_len = float('inf')\n",
    "        \n",
    "        queue = [string]\n",
    "        q_dictionary = {}  # 체크한 문자열 이외의 항목\n",
    "        \n",
    "        while len(queue) > 0:\n",
    "            q_item = queue[0]\n",
    "            queue = queue[1:]\n",
    "            \n",
    "            # early exit\n",
    "            if ((self.verbose < 2) and (len(suggest_dict) > 0)) and\\\n",
    "                ((len(string) - len(q_item) > min_suggest_len)):\n",
    "                    break\n",
    "            \n",
    "            # process queue item\n",
    "            if (q_item in self.dictionary) and (q_item not in suggest_dict):\n",
    "                if self.dictionary[q_item][1] > 0:\n",
    "                    # 말뭉치에서 온 단어가 딕셔너리에는 있고 제안 리스트에는 없으면\n",
    "                    # 단어를 value (말뭉치에서의 빈도수, 편집거리)와 함께 인덱싱\n",
    "                    # 입력 문자열이 아닌 q_item은 삭제가 추가되었으므로\n",
    "                    # 입력 문자열보다 길이가 짧아야 함\n",
    "                    assert len(string) >= len(q_item)\n",
    "                    suggest_dict[q_item] = (self.dictionary[q_item][1], len(string) - len(q_item))\n",
    "                    \n",
    "                    # early exit\n",
    "                    if (self.verbose < 2) and (len(string) == len(q_item)):\n",
    "                        break\n",
    "                    elif (len(string) - len(q_item)) < min_suggest_len:\n",
    "                        min_suggest_len = len(string) - len(q_item)\n",
    "                \n",
    "                # 딕셔너리에 저장된 q_item의 suggested correction은\n",
    "                # valid correction으로 사용 가능\n",
    "                for sc_item in self.dictionary[q_item][0]:\n",
    "                    if sc_item not in suggest_dict:\n",
    "                        \n",
    "                        # 편집 거리 계산\n",
    "                        # suggested item은 항상 더 길어야함\n",
    "                        assert len(sc_item) > len(q_item)\n",
    "                        \n",
    "                        # 입력이 아닌 q_item은 원래 문자열보다 짧아야 함\n",
    "                        assert len(q_item) <= len(string)\n",
    "                        \n",
    "                        if len(q_item) == len(string):\n",
    "                            assert q_item == string\n",
    "                            item_dist = len(sc_item) - len(q_item)\n",
    "                        \n",
    "                        # suggestion 리스트에 있는 아이템은 자신의 문자열과\n",
    "                        # 길이가 같아서는 안됨\n",
    "                        assert sc_item != string\n",
    "                        \n",
    "                        # Damerau-Levenshtine 거리를 이용하여 편집거리 계산\n",
    "                        item_dist = Damerau-Levenshtein(sc_item, string)\n",
    "                        \n",
    "                        # verbose가 설정되지 않은 경우 편집거리가 더 큰 문자는 추가x\n",
    "                        if (self.verbose < 2) and (item_dist > min_suggest_len):\n",
    "                            pass\n",
    "                        elif item_dist <= self.max_edit_distance:\n",
    "                            # suggestion 리스트에 있으면 딕셔너리에도 있어야 함\n",
    "                            assert sc_item in self.dicitionary\n",
    "                            suggest_dict[sc_item] = (self(dictionary[sc_item][1], item_dist))\n",
    "                            if item_dist < min_suggest_len:\n",
    "                                min_suggest_len = item_dist\n",
    "                            \n",
    "                        # 단어가 처리된 순서에 따라 편집거리가 다른 일부 단어들은\n",
    "                        # suggestion에 들어감\n",
    "                        # verbose가 설정되지 않은 경우 suggestion 딕셔너리 절삭\n",
    "                        if self.verbose < 2:\n",
    "                            suggest_dict = {k: v for k, v in suggest_dict.items()\n",
    "                                            if v[1] <= min_suggest_len}\n",
    "                        \n",
    "            # queue item으로부터 체크를 위한 추가적인 item으로 delete 생성\n",
    "            assert len(string) >= len(q_item)\n",
    "            \n",
    "            # verbose가 설정되지 않은 경우 더 큰 편집거리를 가진 단어 추가x\n",
    "            if (self.verbose < 2) and ((len(string) - len(q_item)) > min_suggest_len):\n",
    "                pass\n",
    "            elif (len(string) - len(q_item)) < self.max_edit_distance and len(q_item) > 1:\n",
    "                for c in range(len(q_item)):\n",
    "                    word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                    if word_minus_c not in q_dictionary:\n",
    "                        queue.append(word_minus_c)\n",
    "                        q_dictionary[word_minus_c] = None\n",
    "        # queue는 이제 빈 상태: suggestion 딕셔너리를 출력할 리스트로 변환\n",
    "        if not silent and self.verbose != 0:\n",
    "            print('number of possible corrections: %i'%len(suggest_dict))\n",
    "            print('  edit distance for deletions: %i'%self.max_edit_distance)\n",
    "        \n",
    "        # output option 1\n",
    "        # 편집거리에 대해 오름차순 정렬, 빈도수에 대해 내림차순 정렬\n",
    "        # suggested word correction만 리턴\n",
    "        # return sorted(suggest_dict, key=lambda x: (suggest_dict[x][1], -suggest_dict[x][0]))\n",
    "        \n",
    "        # output option 2\n",
    "        # (correction, 말뭉치 빈도수, 편집거리) 형태의 suggestion 리스트 리턴\n",
    "        as_list = suggest_dict.items()\n",
    "        # outlist = sorted(as_list, key=lambda (term, (freq, dist)): (dist, -freq))\n",
    "        outlist = sorted(as_list, key=lambda x: (x[1][1], -x[1][0]))\n",
    "        \n",
    "        if self.verbose == 0:\n",
    "            return outlist[0]\n",
    "        else:\n",
    "            return outlist\n",
    "        \n",
    "        '''\n",
    "        option 1:\n",
    "        ['file', 'five', 'fire', 'fine', ...]\n",
    "        \n",
    "        option 2:\n",
    "        [('file', (5, 0)),\n",
    "         ('five', (67, 1)),\n",
    "         ('fire', (54, 1)),\n",
    "         ('fine', (17, 1)), ...]\n",
    "        '''\n",
    "    \n",
    "    def best_word(self, s, silent=False):\n",
    "        try:\n",
    "            return self.get_suggestions(s, silent)[0]\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T04:26:57.759982Z",
     "iopub.status.busy": "2021-03-19T04:26:57.758985Z",
     "iopub.status.idle": "2021-03-19T04:26:57.773944Z",
     "shell.execute_reply": "2021-03-19T04:26:57.772946Z",
     "shell.execute_reply.started": "2021-03-19T04:26:57.758985Z"
    }
   },
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field, start_time=time()):\n",
    "        self.field = field\n",
    "        self.start_time = start_time\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataframe):\n",
    "        print(f'[{time() - self.start_time}] select {self.field}')\n",
    "        dt = dataframe[self.field].dtype\n",
    "        if is_categorical_dtype(dt):\n",
    "            return dataframe[self.field].cat.codes[:, None]\n",
    "        elif is_numeric_dtype(dt):\n",
    "            return dataframe[self.field][:, None]\n",
    "        else:\n",
    "            return dataframe[self.field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T04:26:57.774943Z",
     "iopub.status.busy": "2021-03-19T04:26:57.774943Z",
     "iopub.status.idle": "2021-03-19T04:26:57.789902Z",
     "shell.execute_reply": "2021-03-19T04:26:57.788904Z",
     "shell.execute_reply.started": "2021-03-19T04:26:57.774943Z"
    }
   },
   "outputs": [],
   "source": [
    "class DropColumnsByDf(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_df=1, max_df=1.0):\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        m = x.tocsc()\n",
    "        self.nnz_cols = ((m != 0).sum(axis=0) >= self.min_df).A1\n",
    "        if self.max_df < 1.0:\n",
    "            max_df = m.shape[0] * self.max_df\n",
    "            self.nnz_cols = self.nnz_cols & ((m != 0).sum(axis=0) <= max_df).A1\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        m = x.tocsc()\n",
    "        return m[:, self.nnz_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T04:26:57.790899Z",
     "iopub.status.busy": "2021-03-19T04:26:57.790899Z",
     "iopub.status.idle": "2021-03-19T04:26:57.805860Z",
     "shell.execute_reply": "2021-03-19T04:26:57.804861Z",
     "shell.execute_reply.started": "2021-03-19T04:26:57.790899Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T04:26:57.808852Z",
     "iopub.status.busy": "2021-03-19T04:26:57.807854Z",
     "iopub.status.idle": "2021-03-19T04:26:57.820819Z",
     "shell.execute_reply": "2021-03-19T04:26:57.819823Z",
     "shell.execute_reply.started": "2021-03-19T04:26:57.808852Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_cat(text):\n",
    "    try:\n",
    "        cats = text.split('/')\n",
    "        return cats[0], cats[1], cats[2], cats[0] + '/' + cats[1]\n",
    "    except:\n",
    "        print('no category')\n",
    "        return 'other', 'other', 'other', 'other/other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T04:26:57.822815Z",
     "iopub.status.busy": "2021-03-19T04:26:57.821817Z",
     "iopub.status.idle": "2021-03-19T04:26:57.836777Z",
     "shell.execute_reply": "2021-03-19T04:26:57.835779Z",
     "shell.execute_reply.started": "2021-03-19T04:26:57.822815Z"
    }
   },
   "outputs": [],
   "source": [
    "def brands_filling(dataset):\n",
    "    vc = dataset['brand_name'].value_counts()\n",
    "    brands = vc[vc > 0].index\n",
    "    brand_word = r\"[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\"\n",
    "    \n",
    "    many_w_brands = brands[brands.str.contains(' ')]\n",
    "    one_w_brands = brands[~brands.str.contains(' ')]\n",
    "    \n",
    "    ss2 = SymSpell(max_edit_distance=0)\n",
    "    ss2.create_dictionary_from_arr(many_w_brands, token_pattern=r'.+')\n",
    "    \n",
    "    ss1 = SymSpell(max_edit_distance=0)\n",
    "    ss1.create_dictionary_from_arr(one_w_brands, token_pattern=r'.+')\n",
    "    \n",
    "    two_words_re = re.compile(r\"(?=(\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+))\")\n",
    "    \n",
    "    def find_in_str_ss2(row):\n",
    "        for doc_word in two_words_re.finditer(row):\n",
    "            print(doc_word)\n",
    "            suggestion = ss2.best_word(doc_word.group(1), silent=True)\n",
    "            if suggestion is not None:\n",
    "                return doc_word.group(1)\n",
    "        return ''\n",
    "    \n",
    "    def find_in_list_ss1(list):\n",
    "        for doc_word in list:\n",
    "            suggestion = ss1.best_word(doc_word, silent=True)\n",
    "            if suggestion is not None:\n",
    "                return doc_word\n",
    "        return ''\n",
    "    \n",
    "    def find_in_list_ss2(list):\n",
    "        for doc_word in list:\n",
    "            suggestion = ss2.best_word(doc_word, silent=True)\n",
    "            if suggestion is not None:\n",
    "                return doc_word\n",
    "        return ''\n",
    "    \n",
    "    print(f\"Before empty brand_name: {len(dataset[dataset['brand_name'] == ''].index)}\")\n",
    "    \n",
    "    n_name = dataset[dataset['brand_name'] == '']['name'].str.findall(\n",
    "        pat=r\"^[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\"\n",
    "    )\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss2(row) for row in n_name]\n",
    "    \n",
    "    n_desc = dataset[dataset['brand_name'] == '']['item_description'].str.findall(\n",
    "        pat=r\"^[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\"\n",
    "    )\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss2(row) for row in n_desc]\n",
    "    \n",
    "    n_name = dataset[dataset['brand_name'] == '']['name'].str.findall(pat=brand_word)\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss1(row) for row in n_name]\n",
    "\n",
    "    desc_lower = dataset[dataset['brand_name'] == '']['item_description'].str.findall(pat=brand_word)\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss1(row) for row in desc_lower]\n",
    "    \n",
    "    print(f\"After empty brand_name: {len(dataset[dataset['brand_name'] == ''].index)}\")\n",
    "    \n",
    "    del ss1, ss2\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T04:26:57.837774Z",
     "iopub.status.busy": "2021-03-19T04:26:57.837774Z",
     "iopub.status.idle": "2021-03-19T04:26:57.859715Z",
     "shell.execute_reply": "2021-03-19T04:26:57.850740Z",
     "shell.execute_reply.started": "2021-03-19T04:26:57.837774Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_regex(dataset, start_time=time()):\n",
    "    karats_regex = r'(\\d)([\\s-]?)(karat|karats|carat|carats|kt)([^\\w])'\n",
    "    karats_repl = r'\\1k\\4'\n",
    "    \n",
    "    unit_regex = r'(\\d+)[\\s-]([a-z]{2})(\\s)'\n",
    "    unit_repl = r'\\1\\2\\3'\n",
    "    \n",
    "    dataset['name'] = dataset['name'].str.replace(karats_regex, karats_repl)\n",
    "    dataset['item_description'] = dataset['item_description'].str.replace(karats_regex, karats_repl)\n",
    "    print(f'[{time() - start_time}] Karats normalized.')\n",
    "    \n",
    "    dataset['name'] = dataset['name'].str.replace(unit_regex, unit_repl)\n",
    "    dataset['item_description'] = dataset['item_description'].str.replace(unit_regex, unit_repl)\n",
    "    print(f'[{time() - start_time}] Units glued.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T04:26:57.863705Z",
     "iopub.status.busy": "2021-03-19T04:26:57.861710Z",
     "iopub.status.idle": "2021-03-19T04:26:57.883652Z",
     "shell.execute_reply": "2021-03-19T04:26:57.882653Z",
     "shell.execute_reply.started": "2021-03-19T04:26:57.862707Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_pandas(train, test, start_time=time()):\n",
    "    train = train[train['price'] > 0.0].reset_index(drop=True)\n",
    "    print('Train shape without zero price:', train.shape)\n",
    "    \n",
    "    nrow_train = train.shape[0]\n",
    "    y_train = np.log1p(train['price'])\n",
    "    merge: pd.DataFrame = pd.concat([train, test])\n",
    "    \n",
    "    del train, test\n",
    "    gc.collect()\n",
    "    \n",
    "    merge['has_category'] = (merge['category_name'].notnull()).astype('category')\n",
    "    print(f'[{time() - start_time}] Has_category filled.')\n",
    "    \n",
    "    merge['category_name'] = merge['category_name'].fillna('other/other/other').str.lower()\n",
    "    merge['general_cat'], merge['subcat_1'], merge['subcat_2'], merge['gen_subcat1'] =\\\n",
    "    zip(*merge['category_name'].apply(lambda x: split_cat(x)))\n",
    "    print(f'[{time() - start_time}] Split categories completed.')\n",
    "    \n",
    "    merge['has_brand'] = (merge['brand_name'].notnull()).astype('category')\n",
    "    print(f'[{time() - start_time}] Has_brand filled.')\n",
    "    \n",
    "    merge['gencat_cond'] = merge['general_cat'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
    "    merge['subcat_1_cond'] = merge['subcat_1'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
    "    merge['subcat_2_cond'] = merge['subcat_2'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
    "    print(f'[{time() - start_time}] Categories and item_condition_id concatenated.')\n",
    "    \n",
    "    merge['name'] = merge['name'].fillna('').str.lower()\n",
    "    merge['brand_name'] = merge['brand_name'].fillna('').str.lower()\n",
    "    merge['item_description'] = merge['item_description'].fillna('').str.lower().replace(to_replace='No description yet', value='')\n",
    "    print(f'[{time() - start_time}] Missing filled.')\n",
    "    \n",
    "    preprocess_regex(merge, start_time)\n",
    "    \n",
    "    brands_filling(merge)\n",
    "    print(f'[{time() - start_time}] Brand name filled.')\n",
    "    \n",
    "    merge['name'] = merge['name'] + ' ' + merge['brand_name']\n",
    "    print(f'[{time() - start_time}] Name concatenated')\n",
    "    \n",
    "    merge['item_description'] = merge['item_description']\\\n",
    "                                + ' ' + merge['name']\\\n",
    "                                + ' ' + merge['subcat_1']\\\n",
    "                                + ' ' + merge['subcat_2']\\\n",
    "                                + ' ' + merge['general_cat']\\\n",
    "                                + ' ' + merge['brand_name']\n",
    "    print(f'[{time() - start_time}] Item description concatenated.')\n",
    "    \n",
    "    merge.drop(['price', 'test_id', 'train_id'], axis=1, inplace=True)\n",
    "    \n",
    "    return merge, y_train, nrow_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T04:26:57.884648Z",
     "iopub.status.busy": "2021-03-19T04:26:57.884648Z",
     "iopub.status.idle": "2021-03-19T04:26:57.899609Z",
     "shell.execute_reply": "2021-03-19T04:26:57.898610Z",
     "shell.execute_reply.started": "2021-03-19T04:26:57.884648Z"
    }
   },
   "outputs": [],
   "source": [
    "def intersect_drop_columns(train: csr_matrix, valid: csr_matrix, min_df=0):\n",
    "    t = train.tocsc()\n",
    "    v = valid.tocsc()\n",
    "    nnz_train = ((t != 0).sum(axis=0) >= min_df).A1\n",
    "    nnz_valid = ((v != 0).sum(axis=0) >= min_df).A1\n",
    "    nnz_cols = nnz_train & nnz_valid\n",
    "    res = t[:, nnz_cols], v[:, nnz_cols]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-19T04:26:57.900606Z",
     "iopub.status.busy": "2021-03-19T04:26:57.900606Z",
     "iopub.status.idle": "2021-03-19T04:53:47.572986Z",
     "shell.execute_reply": "2021-03-19T04:53:47.570023Z",
     "shell.execute_reply.started": "2021-03-19T04:26:57.900606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.101563930511475] Finished to load data\n",
      "Train shape: (1482535, 8)\n",
      "Test shape: (693359, 7)\n",
      "Train shape without zero price: (1481661, 8)\n",
      "[11.673523664474487] Has_category filled.\n",
      "[18.598693132400513] Split categories completed.\n",
      "[18.722391605377197] Has_brand filled.\n",
      "[21.615187883377075] Categories and item_condition_id concatenated.\n",
      "[24.93429160118103] Missing filled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n",
      "C:\\Users\\PC\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41.0000057220459] Karats normalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\PC\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62.87273335456848] Units glued.\n",
      "total words processed: 2671\n",
      "total unique words in corpus: 2671\n",
      "total items in dictionary (corpus words and deletions): 2671\n",
      "  edit distance for deletions: 0\n",
      "  length of longest word in corpus: 39\n",
      "total words processed: 2616\n",
      "total unique words in corpus: 2616\n",
      "total items in dictionary (corpus words and deletions): 2616\n",
      "  edit distance for deletions: 0\n",
      "  length of longest word in corpus: 15\n",
      "Before empty brand_name: 927861\n",
      "After empty brand_name: 252719\n",
      "[125.69549107551575] Brand name filled.\n",
      "[126.28608417510986] Name concatenated\n",
      "[133.23039627075195] Item description concatenated.\n",
      "[133.92292404174805] select name\n",
      "[167.14917182922363] select category_name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\envs\\kaggle\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[178.5758183002472] select brand_name\n",
      "[184.75419211387634] select gencat_cond\n",
      "[190.9366328716278] select subcat_1_cond\n",
      "[197.15698981285095] select subcat_2_cond\n",
      "[203.48507714271545] select has_brand\n",
      "[203.61310505867004] select shipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\PC\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[203.74375653266907] select item_condition_id\n",
      "[203.89841198921204] select item_description\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[488.6556258201599] Merge vectorized\n",
      "(2175020, 8961796)\n",
      "[578.784182548523] TF/IDF completed\n",
      "(1481661, 8961796)\n",
      "[627.6966650485992] Drop only in train or test cols: 5976503\n",
      "[1606.3656115531921] Train Ridge completed. Iterations: None\n",
      "[1606.8883068561554] Predict Ridge completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\envs\\kaggle\\lib\\site-packages\\pandas\\core\\indexing.py:1597: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "C:\\Users\\PC\\anaconda3\\envs\\kaggle\\lib\\site-packages\\pandas\\core\\indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "C:\\Users\\PC\\anaconda3\\envs\\kaggle\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    start_time = time()\n",
    "    \n",
    "    train = pd.read_table(os.path.join(INPUT_PATH, 'train.tsv'),\n",
    "                          engine='c',\n",
    "                          dtype={'item_condition_id':'category',\n",
    "                                 'shipping':'category'})\n",
    "    test = pd.read_table(os.path.join(INPUT_PATH, 'test.tsv'),\n",
    "                          engine='c',\n",
    "                          dtype={'item_condition_id':'category',\n",
    "                                 'shipping':'category'})\n",
    "    print(f'[{time() - start_time}] Finished to load data')\n",
    "    print('Train shape:', train.shape)\n",
    "    print('Test shape:', test.shape)\n",
    "    \n",
    "    submission: pd.DataFrame = test[['test_id']]\n",
    "    \n",
    "    merge, y_train, nrow_train = preprocess_pandas(train, test, start_time)\n",
    "    \n",
    "    meta_params = {\n",
    "        'name_ngram':(1, 2),\n",
    "        'name_max_f':75000,\n",
    "        'name_min_df':10,\n",
    "        \n",
    "        'category_ngram':(2, 3),\n",
    "        'category_token':'.+',\n",
    "        'category_min_df':10,\n",
    "        \n",
    "        'brand_min_df':10,\n",
    "        \n",
    "        'desc_ngram':(1, 3),\n",
    "        'desc_max_f':150000,\n",
    "        'desc_max_df':0.5,\n",
    "        'desc_min_df':10\n",
    "    }\n",
    "    \n",
    "    stopwords = frozenset(['the', 'a', 'an', 'is', 'it', 'this'])\n",
    "    # 'i', 'so', 'its', 'am', 'are'\n",
    "    \n",
    "    vectorizer = FeatureUnion([\n",
    "        ('name',\n",
    "         Pipeline([\n",
    "             ('select', ItemSelector('name', start_time=start_time)),\n",
    "             ('transform', HashingVectorizer(\n",
    "                 ngram_range=(1, 2),\n",
    "                 n_features=2**27,\n",
    "                 norm='l2',\n",
    "                 lowercase=False,\n",
    "                 stop_words=stopwords\n",
    "             )),\n",
    "             ('drop_cols', DropColumnsByDf(min_df=2))\n",
    "         ])),\n",
    "        ('category_name',\n",
    "         Pipeline([\n",
    "             ('select', ItemSelector('category_name', start_time=start_time)),\n",
    "             ('transform', HashingVectorizer(\n",
    "                 ngram_range=(1, 1),\n",
    "                 token_pattern='.+',\n",
    "                 tokenizer=split_cat,\n",
    "                 n_features=2**27,\n",
    "                 norm='l2',\n",
    "                 lowercase=False\n",
    "             )),\n",
    "             ('drop_cols', DropColumnsByDf(min_df=2))\n",
    "         ])),\n",
    "        ('brand_name',\n",
    "         Pipeline([\n",
    "             ('select', ItemSelector('brand_name', start_time=start_time)),\n",
    "             ('transform', CountVectorizer(\n",
    "                 token_pattern='.+',\n",
    "                 min_df=2,\n",
    "                 lowercase=False\n",
    "             )),\n",
    "         ])),\n",
    "        ('gencat_cond',\n",
    "          Pipeline([\n",
    "            ('select', ItemSelector('gencat_cond', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('subcat_1_cond',\n",
    "          Pipeline([\n",
    "            ('select', ItemSelector('subcat_1_cond', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('subcat_2_cond',\n",
    "          Pipeline([\n",
    "            ('select', ItemSelector('subcat_2_cond', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('has_brand',\n",
    "          Pipeline([\n",
    "            ('select', ItemSelector('has_brand', start_time=start_time)),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('shipping',\n",
    "          Pipeline([\n",
    "            ('select', ItemSelector('shipping', start_time=start_time)),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('item_condition_id',\n",
    "          Pipeline([\n",
    "            ('select', ItemSelector('item_condition_id', start_time=start_time)),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('item_description',\n",
    "          Pipeline([\n",
    "            ('select', ItemSelector('item_description', start_time=start_time)),\n",
    "            ('hash', HashingVectorizer(\n",
    "                ngram_range=(1, 3),\n",
    "                n_features=2 ** 27,\n",
    "                dtype=np.float32,\n",
    "                norm='l2',\n",
    "                lowercase=False,\n",
    "                stop_words=stopwords\n",
    "            )),\n",
    "            ('drop_cols', DropColumnsByDf(min_df=2)),\n",
    "        ]))\n",
    "    ], n_jobs=1)\n",
    "    \n",
    "    sparse_merge = vectorizer.fit_transform(merge)\n",
    "    print(f'[{time() - start_time}] Merge vectorized')\n",
    "    print(sparse_merge.shape)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    x = tfidf_transformer.fit_transform(sparse_merge)\n",
    "    print(f'[{time() - start_time}] TF/IDF completed')\n",
    "    \n",
    "    x_train = x[:nrow_train]\n",
    "    print(x_train.shape)\n",
    "    \n",
    "    x_test = x[nrow_train:]\n",
    "    \n",
    "    del merge, sparse_merge, vectorizer, tfidf_transformer\n",
    "    gc.collect()\n",
    "    \n",
    "    x_train, x_test = intersect_drop_columns(x_train, x_test, min_df=1)\n",
    "    print(f'[{time() - start_time}] Drop only in train or test cols: {x_train.shape[1]}')\n",
    "    gc.collect()\n",
    "    \n",
    "    ridge = Ridge(solver='auto', fit_intercept=True, alpha=0.4, max_iter=200,\n",
    "                  normalize=False, tol=0.01)\n",
    "    ridge.fit(x_train, y_train)\n",
    "    print(f'[{time() - start_time}] Train Ridge completed. Iterations: {ridge.n_iter_}')\n",
    "    \n",
    "    predsR = ridge.predict(x_test)\n",
    "    print(f'[{time() - start_time}] Predict Ridge completed.')\n",
    "    \n",
    "    submission.loc[:, 'price'] = np.expm1(predsR)\n",
    "    submission.loc[submission['price'] < 0.0, 'price'] = 0.0\n",
    "    submission.to_csv('data/submission_3_submission_ridge.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
