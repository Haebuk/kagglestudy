{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hundred-spyware",
   "metadata": {},
   "source": [
    "# Ridge (LB 0.41943)\n",
    "https://www.kaggle.com/rumbok/ridge-lb-0-41944"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "operational-fetish",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T06:32:52.446439Z",
     "start_time": "2021-03-19T06:32:51.479560Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "automotive-reform",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T06:32:52.462455Z",
     "start_time": "2021-03-19T06:32:52.447440Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['MKL_NUM_THREADS'] = '4'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "os.environ['JOBLIN_START_METHOD'] = 'forkserver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "maritime-scout",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T06:32:52.478468Z",
     "start_time": "2021-03-19T06:32:52.464456Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_PATH = r'./input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "saved-italic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T06:32:52.494483Z",
     "start_time": "2021-03-19T06:32:52.479470Z"
    }
   },
   "outputs": [],
   "source": [
    "def dameraulevenshtein(seq1, seq2):\n",
    "    \"\"\"\n",
    "    시퀀스 간 Damerau-Levenshtein 거리 계산\n",
    "    \n",
    "    이 거리는 추가, 삭제, 대체 횟수입니다.\n",
    "    첫 번째 시퀀스를 두 번째로 변환하는 데 필요한 전환 작업입니다.\n",
    "    일반적으로 문자열과 함께 사용되지만 비교할 수 있는 개체의 시퀀스는 모두 작동합니다.\n",
    "    \n",
    "    대체는 '연속'글자를 교환하는 것이며, 다른 모든 작업은 자체 설명이 가능합니다.\n",
    "    \n",
    "    이 구현은 O(N*M) 시간과 O(M) 공간입니다. N과 M의 경우 두 시퀀스의 길이입니다.\n",
    "    \n",
    "    >>> dameraulevenshtein('ba', 'abc')\n",
    "    2\n",
    "    >>> dameraulevenshtein('fee', 'deed')\n",
    "    2\n",
    "    \n",
    "    임의 시퀀스에서도 작동합니다.\n",
    "    >>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
    "    2\n",
    "    \"\"\"\n",
    "    # 개념적으로 len(seq1) + len(seq2) + 1 matrix을 기초로 합니다.\n",
    "    # 그러나 현재와, 두 개의 이전 열만 한 번에 필요합니다.\n",
    "    # 따라서 이들을 저장합니다.\n",
    "    oneago = None\n",
    "    thisrow = list(range(1, len(seq2) + 1)) + [0]\n",
    "    for x in range(len(seq1)):\n",
    "        # 파이썬 리스트는 음수 인덱스를 감싸므로, 목록의 '끝'에 맨 왼쪽 열을 배치합니다.\n",
    "        # 이렇게 하면 인덱싱되지 않은 문자열과 일치하고 추가 계산이 저장됩니다.\n",
    "        twoago, oneago, thisrow = (oneago, thisrow, [0] * len(seq2) + [x+1])\n",
    "        for y in range(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneage[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # 이 블록은 대체를 처리합니다.\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x - 1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "    return thisrow[len(seq2) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "convenient-removal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T06:32:52.542527Z",
     "start_time": "2021-03-19T06:32:52.495484Z"
    }
   },
   "outputs": [],
   "source": [
    "class SymSpell:\n",
    "    def __init__(self, max_edit_distance=3, verbose=0):\n",
    "        self.max_edit_distance = max_edit_distance\n",
    "        self.verbose = verbose\n",
    "        # 0: 최고의 제안\n",
    "        # 1: 가장 작은 거리의 모든 제안\n",
    "        # 2: 모든 제안 <= max_edit_distance (느림, 조기 대체 없음)\n",
    "        self.dictionary = {}\n",
    "        self.longest_word_length = 0\n",
    "        \n",
    "    def get_deletes_list(self, w):\n",
    "        \"\"\"단어가 주어지면 max_edit_distance 문자가 삭제된 문자열을 파생합니다.\"\"\"\n",
    "        \n",
    "        deletes = []\n",
    "        queue = [w]\n",
    "        for d in range(self.max_edit_distance):\n",
    "            temp_queue = []\n",
    "            for word in queue:\n",
    "                if len(word) > 1:\n",
    "                    for c in range(len(word)): # 문자 인덱스\n",
    "                        word_minus_c = word[:c] + word[c + 1:]\n",
    "                        if word_minus_c not in deletes:\n",
    "                            deletes.append(word_minus_c)\n",
    "                        if word_minus_c not in temp_queue:\n",
    "                            temp_queue.append(word_minus_c)\n",
    "            queue = temp_queue\n",
    "        return deletes\n",
    "    \n",
    "    def create_dictionary_entry(self, w):\n",
    "        \"\"\"단어와 단어에서 파생된 삭제 내용을 딕셔너리에 추가합니다.\"\"\"\n",
    "        # 단어가 딕셔너리에 이미 존재하는지 체크합니다.\n",
    "        # 딕셔너리 항목은 다음과 같은 형식으로 이루어져 있습니다.\n",
    "        # (제안된 수정 목록, 말뭉치의 단어 빈도)\n",
    "        new_real_word_added = False\n",
    "        if w in self.dictionary:\n",
    "            # 단어가 말뭉치 내에 있으면 카운트 증가\n",
    "            self.dictionary[w] = (self.dictionary[w][0], self.dictionary[w][1] + 1)\n",
    "        else:\n",
    "            self.dictionary[w] = ([], 1)\n",
    "            self.longest_word_length = max(self.longest_word_length, len(w))\n",
    "\n",
    "        if self.dictionary[w][1] == 1:\n",
    "            # 말뭉치에서 단어의 첫 등장입니다.\n",
    "            # n.b 단어는 파생된 단어로 이미 딕셔너리에 있을 수 있습니다.\n",
    "            # (실제 단어에서 문자를 추출합니다.)\n",
    "            # 그러나 말뭉치에서 단어의 빈도수는 증가하지 않았습니다(이 경우)\n",
    "            new_real_word_added = True\n",
    "            deletes = self.get_deletes_list(w)\n",
    "            for item in deletes:\n",
    "                if item in self.dictionary:\n",
    "                    # 삭제가 제안된 수정 목록에 단어를 추가합니다.\n",
    "                    self.dictionary[item][0].append(w)\n",
    "                else:\n",
    "                    # 말뭉치의 단어 빈도는 증가하지 않습니다.\n",
    "                    self.dictionary[item] = ([w], 0)\n",
    "        return new_real_word_added\n",
    "\n",
    "    def create_dictionary_from_arr(self, arr, token_pattern=r'[a-z]+'):\n",
    "        total_word_count = 0\n",
    "        unique_word_count = 0\n",
    "\n",
    "        for line in arr:\n",
    "            # 단어로, 비문자로 구분합니다.\n",
    "            words = re.findall(token_pattern, line.lower())\n",
    "            for word in words:\n",
    "                total_word_count += 1\n",
    "                if self.create_dictionary_entry(word):\n",
    "                    unique_word_count += 1\n",
    "        print('total words processed: %i' % total_word_count)\n",
    "        print('total unique words in corpus: %i' % unique_word_count)\n",
    "        print('total items in dictionary (corpus words and deletions): %i' \n",
    "              % len(self.dictionary))\n",
    "        print('    edit distance for deletions: %i' % self.max_edit_distance)\n",
    "        print('    length of longest word in corpus: %i' % self.longest_word_length)\n",
    "        return self.dictionary\n",
    "\n",
    "    def create_dictionary(self, fname):\n",
    "        total_word_count = 0\n",
    "        unique_word_count = 0\n",
    "        \n",
    "        with open(fname) as file:\n",
    "            for line in file:\n",
    "                # 비문자, 단어로 구분\n",
    "                words = re.findall('[a-z]+', line.lower())\n",
    "                for word in words:\n",
    "                    total_word_count += 1\n",
    "                    if self.create_dictionary_entry(word):\n",
    "                        unique_word_count += 1\n",
    "                        \n",
    "        print('total words processed: %i' % total_word_count)\n",
    "        print('total unique words in corpus: %i' % unique_word_count)\n",
    "        print('total items in dictionary (corpus words and deletions): %i' \n",
    "              % len(self.dictionary))\n",
    "        print('    edit distance for deletions: %i' % self.max_edit_distance)\n",
    "        print('    length of longest word in corpus: %i' % self.longest_word_length)\n",
    "        return self.dictionary\n",
    "    \n",
    "    def get_suggestions(self, string, silent=False):\n",
    "        \"\"\"철자가 잘못되었을 가능성이 있는 단어에 대해 제안된 수정 목록을 반환합니다.\"\"\"\n",
    "        if (len(string) - self.longest_word_length) > self.max_edit_distance:\n",
    "            if not silent:\n",
    "                print('no items in dictionary within maximum edit distance')\n",
    "            return []\n",
    "        \n",
    "        suggest_dict = {}\n",
    "        min_suggest_len = float('inf')\n",
    "        \n",
    "        queue = [string]\n",
    "        q_dictionary = {} # 체크한 문자열 이외의 항목입니다.\n",
    "        \n",
    "        while len(queue) > 0:\n",
    "            q_item = queue[0] # pop\n",
    "            queue = queue[1:]\n",
    "            \n",
    "            # early exit\n",
    "            if ((self.verbose < 2) and (len(suggest_dict) > 0) and\n",
    "                ((len(string) - len(q_item)) > min_suggest_len)):\n",
    "                break\n",
    "                \n",
    "            # process queue item\n",
    "            if (q_item in self.dictionary) and (q_item not in suggest_dict):\n",
    "                if self.dictionary[q_item][1] > 0:\n",
    "                    # 단어는 딕셔너리에 있고, 말뭉치에서 온 단어이고, 아직 제안 목록에\n",
    "                    # 없는 단어이므로 제안 딕셔너리에 값을 가진 단어에 의해\n",
    "                    # 색인화(말뭉치 빈도, 편집 거리)됩니다.\n",
    "                    # 삭제만 추가되므로(수동 딕셔너리 수정이 추가되지 않는 한) 입력 문자열이 아닌\n",
    "                    #  q_items는 입력 문자열보다 짧습니다.\n",
    "                    assert len(string) >= len(q_item)\n",
    "                    suggest_dict[q_item] = (self.dictionary[q_item][1],\n",
    "                                            len(string) - len(q_item))\n",
    "                    \n",
    "                    # early exit\n",
    "                    if (self.verbose < 2) and (len(string) == len(q_item)):\n",
    "                        break\n",
    "                    elif (len(string) - len(q_item)) < min_suggest_len:\n",
    "                        min_suggest_len = len(string) - len(q_item)\n",
    "                        \n",
    "                # 딕셔너리에 저장된 q_item에 대해 제안된 수정 사항(q_item 자체가\n",
    "                # 유효한 단어인지 아니면 단순히 삭제인지 여부)은 유효한\n",
    "                # 수정 사항이 될 수 있습니다.\n",
    "                for sc_item in self.dictionary[q_item][0]:\n",
    "                    if sc_item not in suggest_dict:\n",
    "                        \n",
    "                        # 계산된 편집 거리 제안 항목은 항상 더 길어야 합니다.\n",
    "                        # (수동 수정이 추가되지 않는 한)\n",
    "                        assert len(sc_item) > len(q_item)\n",
    "                        \n",
    "                        # 입력하지 않은 q_item은 원래 문자열보다 짧아야 합니다.\n",
    "                        # (수동 수정이 추가되지 않는 경우)\n",
    "                        assert len(q_item) <= len(string)\n",
    "                        \n",
    "                        if len(q_item) <= len(string):\n",
    "                            assert q_item == string\n",
    "                            item_dist = len(sc_item) - len(q_item)\n",
    "                            \n",
    "                        # 제안 목록의 항목은 문자열 자체와 동일하면 안됩니다.\n",
    "                        assert sc_item != string\n",
    "                        \n",
    "                        # 예를 들어, Damerau-Levenshtein 거리를 사용하여 편집 거리를 계산합니다.\n",
    "                        item_dist = dameraulevenshtein(sc_item, string)\n",
    "                        \n",
    "                        # verbose가 켜져 있지 않으면 편집 거리가 더 큰 단어를 추가하지 마십시오\n",
    "                        if (self.verbose < 2) and (item_dist > min_suggest_len):\n",
    "                            pass\n",
    "                        elif item_dist <= self.max_edit_distance:\n",
    "                            # 제안 목록에 있는 경우 이미 사전에 있어야 합니다.\n",
    "                            assert sc_item in self.dictionary \n",
    "                            suggest_dict[sc_item] = (self.dictionary[sc_item][1], item_dist)\n",
    "                            if item_dist < min_suggest_len:\n",
    "                                min_suggest_len = item_dist\n",
    "                        # 단어가 처리되는 순서에 따라 다른 편집 거리의 단어들이 제안으로\n",
    "                        # 입력될 수 있습니다. verbose가 설정되지 않은 경우\n",
    "                        # 제안 딕셔너리를 트리밍 합니다.\n",
    "                        if self.verbose < 2:\n",
    "                            suggest_dict = {k: v for k, v in suggest_dict.items()\n",
    "                                            if v[1] <= min_suggest_len}\n",
    "                            \n",
    "            # 이제 대기열 항목에서 삭제(예: 문자열의 하위 문자열 또는 삭제)를 확인할 추가 항목\n",
    "            # 으로 생성합니다. - 대기열 끝에 추가\n",
    "            assert len(string) >= len(q_item)\n",
    "\n",
    "            # verbose가 켜져 있지 않은 경우 더 큰 편집 거리 단어를 추가하지 마십시오\n",
    "            if (self.verbose < 2) and ((len(string) - len(q_item)) > min_suggest_len):\n",
    "                pass\n",
    "            elif (len(string) - len(q_item)) < self.max_edit_distance and len(q_item) > 1:\n",
    "                for c in range(len(q_item)): # 문자 인덱스\n",
    "                    if word_minus_c not in q_dictionary:\n",
    "                        queue.append(word_minus_c)\n",
    "                        q_dictionary[word_minus_c] = None # 임의의 값,단지 확인했다는 것을 증명\n",
    "        \n",
    "        # 이제 대기열이 비어 있습니다. 딕셔너리의 제안을 출력할 목록으로 변환합니다.\n",
    "        if not silent and self.verbose != 0:\n",
    "            print('num. of possible corrections: %i' % len(suggest_dict))\n",
    "            print('  edit distance for deletions: %i' % self.max_edit_distance)\n",
    "            \n",
    "        # output option 1\n",
    "        # 결과를 편집거리의 오름차순으과 빈도의 내림차 순으로 정렬\n",
    "        # 수정된 단어의 제안 목록 반환:\n",
    "        # return sorted(suggest_dict, key = lambda x:\n",
    "        #               (suggest_dict[x][1], -suggest_dict[x][0]))\n",
    "\n",
    "        # output option 2\n",
    "        # return list of suggestions with (correction,\n",
    "        #                                  (frequency in corpus, edit distance)):\n",
    "        as_list = suggest_dict.items()\n",
    "        outlist = sorted(as_list, key=lambda x: (x[1][1], -x[1][0]))\n",
    "        \n",
    "        if self.verbose == 0:\n",
    "            return outlist[0]\n",
    "        else:\n",
    "            return outlist\n",
    "        \n",
    "        '''\n",
    "        Option 1:\n",
    "        ['file', 'five', 'fire', 'fine', ...]\n",
    "\n",
    "        Option 2:\n",
    "        [('file', (5, 0)),\n",
    "         ('five', (67, 1)),\n",
    "         ('fire', (54, 1)),\n",
    "         ('fine', (17, 1))...]  \n",
    "        '''\n",
    "    \n",
    "    def best_word(self, s, silent=False):\n",
    "        try:\n",
    "            return self.get_suggestions(s, silent)[0]\n",
    "        except:\n",
    "            return None\n",
    "                    \n",
    "                    \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "worse-personality",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T06:32:52.558541Z",
     "start_time": "2021-03-19T06:32:52.543528Z"
    }
   },
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field, start_time=time()):\n",
    "        self.field = field\n",
    "        self.start_time = start_time\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataframe):\n",
    "        print(f'[{time()-self.start_time}] select {self.field}')\n",
    "        dt = dataframe[self.field].dtype\n",
    "        if is_categorical_dtype(dt):\n",
    "            return dataframe[self.field].cat.codes[:,None]\n",
    "        elif is_numeric_dtype(dt):\n",
    "            return dataframe[self.field][:,None]\n",
    "        else:\n",
    "            return dataframe[self.field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "original-reality",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T06:38:21.606254Z",
     "start_time": "2021-03-19T06:38:21.598247Z"
    }
   },
   "outputs": [],
   "source": [
    "class DropColumnsByDf(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_df=1, max_df=1.0):\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        m = X.tocsc()\n",
    "        self.nnz_cols = ((m != 0).sum(axis=0) >= self.min_df).A1\n",
    "        if self.max_df < 1.0:\n",
    "            max_df = m.shape[0] * self.max_df\n",
    "            self.nnz_cols = self.nnz_cols & ((m != 0).sum(axis=0) <= max_df).A1\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        m = X.tocsc()\n",
    "        return m[:, self.nnz_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "partial-warehouse",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T06:32:52.590570Z",
     "start_time": "2021-03-19T06:32:52.575557Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "medium-preparation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T06:32:52.606585Z",
     "start_time": "2021-03-19T06:32:52.591571Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_cat(text):\n",
    "    try:\n",
    "        cats = text.split('/')\n",
    "        return cats[0], cats[1], cats[2], cats[0] + '/' + cats[1]\n",
    "    except:\n",
    "        print('no category')\n",
    "        return 'other', 'other', 'other', 'other/other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fluid-river",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T06:32:52.622599Z",
     "start_time": "2021-03-19T06:32:52.607586Z"
    }
   },
   "outputs": [],
   "source": [
    "def brands_filling(dataset):\n",
    "    vc = dataset['brand_name'].value_counts()\n",
    "    brands = vc[vc > 0].index\n",
    "    brand_word = r\"[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\"\n",
    "    \n",
    "    many_w_brands = brands[brands.str.contains(' ')]\n",
    "    one_w_brands = brands[~brands.str.contains(' ')]\n",
    "    \n",
    "    ss2 = SymSpell(max_edit_distance=0)\n",
    "    ss2.create_dictionary_from_arr(many_w_brands, token_pattern=r'.+')\n",
    "    \n",
    "    ss1 = SymSpell(max_edit_distance=0)\n",
    "    ss1.create_dictionary_from_arr(one_w_brands, token_pattern=r'.+')\n",
    "    \n",
    "    two_words_re = re.compile(r\"(?=(\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+))\")\n",
    "    \n",
    "    def find_in_str_ss2(row):\n",
    "        for doc_word in two_words_re.finditer(row):\n",
    "            print(doc_word)\n",
    "            suggestion = ss2.best_word(doc_word.group(1), silent=True)\n",
    "            if suggestion is not None:\n",
    "                return doc_word.group(1)\n",
    "        return ''\n",
    "    \n",
    "    def find_in_list_ss1(list):\n",
    "        for doc_word in list:\n",
    "            suggestion = ss1.best_word(doc_word, silent=True)\n",
    "            if suggestion is not None:\n",
    "                return doc_word\n",
    "        return ''\n",
    "    \n",
    "    def find_in_list_ss2(list):\n",
    "        for doc_word in list:\n",
    "            suggestion = ss2.best_word(doc_word, silent=True)\n",
    "            if suggestion is not None:\n",
    "                return doc_word\n",
    "        return ''\n",
    "    \n",
    "    print(f\"Before empty brand_name: {len(dataset[dataset['brand_name'] == ''].index)}\")\n",
    "    \n",
    "    n_name = dataset[dataset['brand_name'] == '']['name'].str.findall(\n",
    "        pat=r\"^[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\")\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss2(row) for row in n_name]\n",
    "\n",
    "    n_desc = dataset[dataset['brand_name'] == '']['item_description'].str.findall(\n",
    "        pat=r\"^[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\")\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss2(row) for row in n_desc]\n",
    "\n",
    "    n_name = dataset[dataset['brand_name'] == '']['name'].str.findall(pat=brand_word)\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss1(row) for row in n_name]\n",
    "\n",
    "    desc_lower = dataset[dataset['brand_name'] == '']['item_description'].str.findall(pat=brand_word)\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss1(row) for row in desc_lower]\n",
    "\n",
    "    print(f\"After empty brand_name: {len(dataset[dataset['brand_name'] == ''].index)}\")\n",
    "    \n",
    "    del ss1, ss2\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "noted-provincial",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T06:32:52.638614Z",
     "start_time": "2021-03-19T06:32:52.623600Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_regex(dataset, start_time=time()):\n",
    "    karats_regex = r'(\\d)([\\s-]?)(karat|karats|carat|carats|kt)([^\\w])'\n",
    "    karats_repl = r'\\1k\\4'\n",
    "    \n",
    "    unit_regex = r'(\\d+)[\\s-]([a-z]{2})(\\s)'\n",
    "    unit_repl = r'\\1\\2\\3'\n",
    "    \n",
    "    dataset['name'] = dataset['name'].str.replace(karats_regex, karats_repl)\n",
    "    dataset['item_description'] = dataset['item_description'].str.replace(karats_regex, karats_repl)\n",
    "    print(f'[{time() - start_time}] Karats normalized.')\n",
    "\n",
    "    dataset['name'] = dataset['name'].str.replace(unit_regex, unit_repl)\n",
    "    dataset['item_description'] = dataset['item_description'].str.replace(unit_regex, unit_repl)\n",
    "    print(f'[{time() - start_time}] Units glued.')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "absent-tyler",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T06:32:52.654628Z",
     "start_time": "2021-03-19T06:32:52.639615Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_pandas(train, test, start_time=time()):\n",
    "    train = train[train.price > 0.0].reset_index(drop=True)\n",
    "    print('Train shape without zero price: ', train.shape)\n",
    "    \n",
    "    nrow_train = train.shape[0]\n",
    "    y_train = np.log1p(train['price'])\n",
    "    merge: pd.DataFrame = pd.concat([train, test])\n",
    "        \n",
    "    del train\n",
    "    del test\n",
    "    gc.collect()\n",
    "    \n",
    "    merge['has_category'] = (merge['category_name'].notnull()).astype('category')\n",
    "    print(f'[{time() - start_time}] Has_category filled.')\n",
    "    \n",
    "    merge['category_name'] = merge['category_name'] \\\n",
    "        .fillna('other/other/other') \\\n",
    "        .str.lower() \\\n",
    "        .astype(str)\n",
    "    merge['general_cat'], merge['subcat_1'], merge['subcat_2'], merge['gen_subcat1'] = \\\n",
    "        zip(*merge['category_name'].apply(lambda x: split_cat(x)))\n",
    "    print(f'[{time() - start_time}] Split categories completed.')\n",
    "\n",
    "    merge['has_brand'] = (merge['brand_name'].notnull()).astype('category')\n",
    "    print(f'[{time() - start_time}] Has_brand filled.')\n",
    "\n",
    "    merge['gencat_cond'] = merge['general_cat'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
    "    merge['subcat_1_cond'] = merge['subcat_1'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
    "    merge['subcat_2_cond'] = merge['subcat_2'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
    "    print(f'[{time() - start_time}] Categories and item_condition_id concancenated.')\n",
    "\n",
    "    merge['name'] = merge['name'] \\\n",
    "        .fillna('') \\\n",
    "        .str.lower() \\\n",
    "        .astype(str)\n",
    "    merge['brand_name'] = merge['brand_name'] \\\n",
    "        .fillna('') \\\n",
    "        .str.lower() \\\n",
    "        .astype(str)\n",
    "    merge['item_description'] = merge['item_description'] \\\n",
    "        .fillna('') \\\n",
    "        .str.lower() \\\n",
    "        .replace(to_replace='No description yet', value='')\n",
    "    print(f'[{time() - start_time}] Missing filled.')\n",
    "\n",
    "    preprocess_regex(merge, start_time)\n",
    "\n",
    "    brands_filling(merge)\n",
    "    print(f'[{time() - start_time}] Brand name filled.')\n",
    "\n",
    "    merge['name'] = merge['name'] + ' ' + merge['brand_name']\n",
    "    print(f'[{time() - start_time}] Name concancenated.')\n",
    "\n",
    "    merge['item_description'] = merge['item_description'] \\\n",
    "                                + ' ' + merge['name'] \\\n",
    "                                + ' ' + merge['subcat_1'] \\\n",
    "                                + ' ' + merge['subcat_2'] \\\n",
    "                                + ' ' + merge['general_cat'] \\\n",
    "                                + ' ' + merge['brand_name']\n",
    "    print(f'[{time() - start_time}] Item description concatenated.')\n",
    "\n",
    "    merge.drop(['price', 'test_id', 'train_id'], axis=1, inplace=True)\n",
    "\n",
    "    return merge, y_train, nrow_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "victorian-pride",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T06:32:52.670643Z",
     "start_time": "2021-03-19T06:32:52.655630Z"
    }
   },
   "outputs": [],
   "source": [
    "def intersect_drop_columns(train: csr_matrix, valid: csr_matrix, min_df=0):\n",
    "    t = train.tocsc()\n",
    "    v = valid.tocsc()\n",
    "    nnz_train = ((t != 0).sum(axis=0) >= min_df).A1\n",
    "    nnz_valid = ((v != 0).sum(axis=0) >= min_df).A1\n",
    "    nnz_cols = nnz_train & nnz_valid\n",
    "    res = t[:, nnz_cols], v[:, nnz_cols]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "falling-leisure",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T07:03:30.752891Z",
     "start_time": "2021-03-19T06:45:21.438517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.369909048080444] Finished to load data\n",
      "Train shape: (1482535, 8)\n",
      "Test shape: (693359, 7)\n",
      "Train shape without zero price:  (1481661, 8)\n",
      "[8.373831510543823] Has_category filled.\n",
      "[15.09694504737854] Split categories completed.\n",
      "[15.187026977539062] Has_brand filled.\n",
      "[17.884315729141235] Categories and item_condition_id concancenated.\n",
      "[20.90005850791931] Missing filled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tens_2g\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\envs\\tens_2g\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.556979417800903] Karats normalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tens_2g\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if sys.path[0] == '':\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tens_2g\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45.33271527290344] Units glued.\n",
      "total words processed: 2671\n",
      "total unique words in corpus: 2671\n",
      "total items in dictionary (corpus words and deletions): 2671\n",
      "    edit distance for deletions: 0\n",
      "    length of longest word in corpus: 39\n",
      "total words processed: 2616\n",
      "total unique words in corpus: 2616\n",
      "total items in dictionary (corpus words and deletions): 2616\n",
      "    edit distance for deletions: 0\n",
      "    length of longest word in corpus: 15\n",
      "Before empty brand_name: 927861\n",
      "After empty brand_name: 252719\n",
      "[85.92554974555969] Brand name filled.\n",
      "[86.42800664901733] Name concancenated.\n",
      "[91.64425301551819] Item description concatenated.\n",
      "[92.28683829307556] select name\n",
      "[115.38197994232178] select category_name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tens_2g\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123.6528103351593] select brand_name\n",
      "[128.11686968803406] select gencat_cond\n",
      "[132.60345339775085] select subcat_1_cond\n",
      "[137.1613736152649] select subcat_2_cond\n",
      "[141.752051115036] select has_brand\n",
      "[141.84513592720032] select shipping\n",
      "[141.93822121620178] select item_condition_id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tens_2g\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  del sys.path[0]\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tens_2g\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  del sys.path[0]\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tens_2g\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142.04631900787354] select item_description\n",
      "[315.00609278678894] Merge vectorized\n",
      "(2175020, 8961796)\n",
      "[365.3662791252136] TF/IDF completed\n",
      "(1481661, 8961796)\n",
      "[390.54031229019165] Drop only in train or test cols: 5976503\n",
      "[1087.6648752689362] Train Ridge completed. Iterations: None\n",
      "[1087.9040923118591] Predict Ridge completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tens_2g\\lib\\site-packages\\pandas\\core\\indexing.py:1597: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tens_2g\\lib\\site-packages\\pandas\\core\\indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tens_2g\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    start_time = time()\n",
    "    \n",
    "    train = pd.read_table(os.path.join(INPUT_PATH, 'train.tsv'),\n",
    "                          engine='c',\n",
    "                          dtype={'item_condition_id': 'category',\n",
    "                                 'shipping': 'category'})\n",
    "    test = pd.read_table(os.path.join(INPUT_PATH, 'test.tsv'),\n",
    "                          engine='c',\n",
    "                          dtype={'item_condition_id': 'category',\n",
    "                                 'shipping': 'category'})\n",
    "    print(f'[{time() - start_time}] Finished to load data')\n",
    "    print('Train shape:', train.shape)\n",
    "    print('Test shape:', test.shape)\n",
    "    \n",
    "    submission: pd.DataFrame = test[['test_id']]\n",
    "        \n",
    "    merge, y_train, nrow_train = preprocess_pandas(train, test, start_time)\n",
    "    \n",
    "    meta_params = {'name_ngram': (1, 2),\n",
    "                   'name_max_f': 75000,\n",
    "                   'name_min_df': 10,\n",
    "                  \n",
    "                   'category_ngram': (2, 3),\n",
    "                   'category_token': '.+',\n",
    "                   'category_min_df': 10,\n",
    "                  \n",
    "                   'brand_min_df': 10,\n",
    "                  \n",
    "                   'desc_ngram': (1, 3),\n",
    "                   'desc_max_f': 150000,\n",
    "                   'desc_max_df': 0.5,\n",
    "                   'desc_min_df': 10}\n",
    "    \n",
    "    stopwords = frozenset(['the', 'a', 'an', 'is', 'it', 'this'])\n",
    "    \n",
    "    vectorizer = FeatureUnion([\n",
    "        ('name', Pipeline([\n",
    "            ('select', ItemSelector('name', start_time=start_time)),\n",
    "            ('transform', HashingVectorizer(\n",
    "                ngram_range=(1, 2),\n",
    "                n_features=2 ** 27,\n",
    "                norm='l2',\n",
    "                lowercase=False,\n",
    "                stop_words=stopwords\n",
    "            )),\n",
    "            ('drop_cols', DropColumnsByDf(min_df=2))\n",
    "        ])),\n",
    "        ('category_name', Pipeline([\n",
    "            ('select', ItemSelector('category_name', start_time=start_time)),\n",
    "            ('transform', HashingVectorizer(\n",
    "                ngram_range=(1, 1),\n",
    "                token_pattern='.+',\n",
    "                tokenizer=split_cat,\n",
    "                n_features=2 ** 27,\n",
    "                norm='l2',\n",
    "                lowercase=False\n",
    "            )),\n",
    "            ('drop_cols', DropColumnsByDf(min_df=2))\n",
    "        ])),\n",
    "        ('brand_name', Pipeline([\n",
    "            ('select', ItemSelector('brand_name', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('gencat_cond', Pipeline([\n",
    "            ('select', ItemSelector('gencat_cond', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('subcat_1_cond', Pipeline([\n",
    "            ('select', ItemSelector('subcat_1_cond', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('subcat_2_cond', Pipeline([\n",
    "            ('select', ItemSelector('subcat_2_cond', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('has_brand', Pipeline([\n",
    "            ('select', ItemSelector('has_brand', start_time=start_time)),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('shipping', Pipeline([\n",
    "            ('select', ItemSelector('shipping', start_time=start_time)),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('item_condition_id', Pipeline([\n",
    "            ('select', ItemSelector('item_condition_id', start_time=start_time)),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('item_description', Pipeline([\n",
    "            ('select', ItemSelector('item_description', start_time=start_time)),\n",
    "            ('hash', HashingVectorizer(\n",
    "                ngram_range=(1, 3),\n",
    "                n_features=2 ** 27,\n",
    "                dtype=np.float32,\n",
    "                norm='l2',\n",
    "                lowercase=False,\n",
    "                stop_words=stopwords\n",
    "            )),\n",
    "            ('drop_cols', DropColumnsByDf(min_df=2)),\n",
    "        ]))\n",
    "    ], n_jobs=1)\n",
    "    \n",
    "    sparse_merge = vectorizer.fit_transform(merge)\n",
    "    print(f'[{time() - start_time}] Merge vectorized')\n",
    "    print(sparse_merge.shape)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    \n",
    "    X = tfidf_transformer.fit_transform(sparse_merge)\n",
    "    print(f'[{time() - start_time}] TF/IDF completed')\n",
    "    \n",
    "    X_train = X[:nrow_train]\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    X_test = X[nrow_train:]\n",
    "    del merge\n",
    "    del sparse_merge\n",
    "    del vectorizer\n",
    "    del tfidf_transformer\n",
    "    gc.collect()\n",
    "    \n",
    "    X_train, X_test = intersect_drop_columns(X_train, X_test, min_df=1)\n",
    "    print(f'[{time() - start_time}] Drop only in train or test cols: {X_train.shape[1]}')\n",
    "    gc.collect()\n",
    "    \n",
    "    ridge = Ridge(solver='auto', fit_intercept=True, alpha=0.4, max_iter=200,\n",
    "                  normalize=False, tol=0.01)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    print(f'[{time() - start_time}] Train Ridge completed. Iterations: {ridge.n_iter_}')\n",
    "    \n",
    "    predsR = ridge.predict(X_test)\n",
    "    print(f'[{time() - start_time}] Predict Ridge completed.')\n",
    "    \n",
    "    submission.loc[:, 'price'] = np.expm1(predsR)\n",
    "    submission.loc[submission['price'] < 0.0, 'price'] = 0.0\n",
    "    submission.to_csv('submission_ridge.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "valid-residence",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T07:16:59.152991Z",
     "start_time": "2021-03-19T07:16:59.107950Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.786888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9.161285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>44.922333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>13.125871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9.971759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693354</th>\n",
       "      <td>693354</td>\n",
       "      <td>19.485424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693355</th>\n",
       "      <td>693355</td>\n",
       "      <td>25.444056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693356</th>\n",
       "      <td>693356</td>\n",
       "      <td>7.504093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693357</th>\n",
       "      <td>693357</td>\n",
       "      <td>13.236767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693358</th>\n",
       "      <td>693358</td>\n",
       "      <td>10.055232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>693359 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        test_id      price\n",
       "0             0   7.786888\n",
       "1             1   9.161285\n",
       "2             2  44.922333\n",
       "3             3  13.125871\n",
       "4             4   9.971759\n",
       "...         ...        ...\n",
       "693354   693354  19.485424\n",
       "693355   693355  25.444056\n",
       "693356   693356   7.504093\n",
       "693357   693357  13.236767\n",
       "693358   693358  10.055232\n",
       "\n",
       "[693359 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
