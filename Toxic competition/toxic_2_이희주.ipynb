{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- competition/dataset : [https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)\n",
    "- date : 2021/03/26\n",
    "- original : [https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop the S@#$ - Toxic Comments EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-19T23:23:23.804833Z",
     "iopub.status.busy": "2021-01-19T23:23:23.803834Z",
     "iopub.status.idle": "2021-01-19T23:23:23.811814Z",
     "shell.execute_reply": "2021-01-19T23:23:23.810817Z",
     "shell.execute_reply.started": "2021-01-19T23:23:23.804833Z"
    }
   },
   "source": [
    "**✏ 필사 1회** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "인터넷의 익명화가 때로는 사람들이 현실에서는 하지 않을 험악한 말들을 하게 만들 수 있습니다. 우리의 플랫폼에서 증오심을 걸러봅시다.\n",
    "\n",
    "**Objective:**  \n",
    "독설 분류를 위한 EDA와 feature-engineering  \n",
    "\n",
    "**Data Overview:**  \n",
    "이 대회의 데이터셋은 사람들이 악성에 대해 평가한 위키 말뭉치 데이터셋입니다. 2004~2015년 사이의 유저 페이지와 기사에 관한 토론으로부터 나온 6300만 개의 코멘트들을 포함합니다.  \n",
    "\n",
    "플랫폼/사이트마다 악성 댓글 검사 절차에 있어 기준이 다를 수 있습니다. 이런 이유로 코멘트들은 다음과 같은 다섯 가지 카테고리로 태그되어 있습니다:  \n",
    "* toxic (심한)\n",
    "* severe_toxic (극심한)\n",
    "* obsence (외설적인)\n",
    "* threat (위협적인)\n",
    "* insult (모욕적인)\n",
    "* identity_hate\n",
    "\n",
    "이 태깅은 crowdsourcing에 의해 수행되었으며, 이 데이터셋이 여러 사람들에 의해 평가되었고 태깅이 100% 정확도를 가지지 못할 것을 의미합니다. 이 부분에 대해 [이곳](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/46131)에서 논의되고 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-25T23:35:51.398331Z",
     "iopub.status.busy": "2021-03-25T23:35:51.398331Z",
     "iopub.status.idle": "2021-03-25T23:35:51.422268Z",
     "shell.execute_reply": "2021-03-25T23:35:51.421271Z",
     "shell.execute_reply.started": "2021-03-25T23:35:51.398331Z"
    }
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "# basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# misc\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# stats\n",
    "from matplotlib.pyplot import imread\n",
    "from scipy import sparse\n",
    "import scipy.stats as ss\n",
    "\n",
    "# viz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "import matplotlib_venn as venn\n",
    "\n",
    "# nlp\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tweet tokenizer는 아포스트로피에서 분할x\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# feature engineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# settings\n",
    "start_time = time.time()\n",
    "color = sns.color_palette()\n",
    "sns.set_style('dark')\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-25T23:36:13.237064Z",
     "iopub.status.busy": "2021-03-25T23:36:13.236067Z",
     "iopub.status.idle": "2021-03-25T23:36:17.516279Z",
     "shell.execute_reply": "2021-03-25T23:36:17.515277Z",
     "shell.execute_reply.started": "2021-03-25T23:36:13.237064Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing the dataset\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-25T23:36:23.076246Z",
     "iopub.status.busy": "2021-03-25T23:36:23.075248Z",
     "iopub.status.idle": "2021-03-25T23:36:23.099182Z",
     "shell.execute_reply": "2021-03-25T23:36:23.098186Z",
     "shell.execute_reply.started": "2021-03-25T23:36:23.076246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a peak\n",
    "train.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-25T23:38:31.534843Z",
     "iopub.status.busy": "2021-03-25T23:38:31.534843Z",
     "iopub.status.idle": "2021-03-25T23:38:31.542822Z",
     "shell.execute_reply": "2021-03-25T23:38:31.541827Z",
     "shell.execute_reply.started": "2021-03-25T23:38:31.534843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      : train  : test\n",
      "rows  : 159571 : 159571\n",
      "pers  : 50     : 50\n"
     ]
    }
   ],
   "source": [
    "nrow_train = train.shape[0]\n",
    "nrow_test = train.shape[0]\n",
    "sum = nrow_train + nrow_test\n",
    "print('      : train  : test')\n",
    "print('rows  :', nrow_train, ':', nrow_test)\n",
    "print('pers  :', round(nrow_train*100/sum), '    :', round(nrow_test*100/sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이후에 train:test 비율을 30:70으로 나누고 테스트셋 바꿀 수 있습니다.  \n",
    "\n",
    "트레이닝 셋에서 범주의 불균형에 대해 살펴봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
