{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:29:32.703023Z",
     "start_time": "2021-02-08T00:29:32.700231Z"
    }
   },
   "source": [
    "# kaggle study 16일차(costa-rican)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T00:30:38.043459Z",
     "start_time": "2021-02-08T00:30:38.037855Z"
    }
   },
   "source": [
    "코드출처 : https://www.kaggle.com/skooch/xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGMB with random split for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edites by Eric Antoine Scuccimara   \n",
    "Misha Losvyi의 https://www.kaggle.com/mlisovyi/feature-engineering-lighgbm-with-f1-macro, 포크입니다. 몇 가지 변경 사항이 있습니다.\n",
    "\n",
    "- LightGBM 모델은 XGBoost로 대체되었으며 그에 따라 코드가 업데이트되었습니다.\n",
    "- 또한 랜덤 포리스트의 투표 분류기를 장착하고 XGB의 결과를 RF와 결합합니다.\n",
    "- 일부 추가 기능이 추가되었습니다.\n",
    "- 이전에 삭제했던 일부 기능이 보존되었습니다.\n",
    "- 일부 코드가 재구성되었습니다.\n",
    "- 저는 데이터를 한 번 쪼개서 LGBM 조기 정지에 대한 검증 데이터를 사용하는 대신 전체 교육 세트를 훈련할 수 있도록 교육 중에 데이터를 분할했습니다. 저는 이것이 이 케이스에서 k-fold 분할보다 더 효과가 있다는 것을 발견했습니다.  \n",
    "  \n",
    "몇 가지 추가 기능은 쿠리야만의 https://www.kaggle.com/kuriyaman1002/reduce-features-140-84-keeping-f1-score,에서 가져왔습니다.\n",
    "\n",
    "원본 커널의 참고 사항(EAS에서 편집):\n",
    "\n",
    "이 커널은 https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro , 을  따르지만 하이퍼 파라미터 최적화를 실행하는 대신 해당 커널의 최적 값을 사용하여 더 빨리 실행됩니다.\n",
    "\n",
    "몇 가지 주요 요점은 다음과 같습니다.\n",
    "\n",
    "- 이 커널은 (가정에서 Aggregate를 추출한 후) 가장에 대해서만 교육을 실행합니다. 이는 발표된 채점 시작 단계에 따릅니다. 채점에는 가장만 사용됩니다. 모든 가구원은 테스트 + 샘플 제출에 포함되지만 (데이터 설명에서) 가장만 점수가 매겨집니다. 그러나 현재 평가는 비주임 가구원들에게도 달려 있는 것으로 보입니다. https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#360115을 참조하십시오.   \n",
    "실제로, 전체 예측은 ~0.4 PLB 점수를 제공하는 반면, 모든 비헤드 항목을 클래스 1로 대체하면 ~0.2 PLB 점수로 떨어집니다.  \n",
    "  \n",
    "- 수업 빈도의 균형을 맞추는 것이 매우 중요한 것 같습니다. 훈련된 모델의 밸런스를 맞추지 않으면 0.39 PLB/-0.43의 로컬 테스트가 제공되는 반면, 밸런싱을 추가하면 0.42 PLB/0.47의 로컬 테스트가 수행됩니다. 수작업으로 할 수 있고, 언더 샘플링으로 달성할 수 있습니다. 그러나 sklearn API의 LightGBM 모델 생성자에서 class_weight='balance'를 설정하는 것이 가장 간단합니다.\n",
    "\n",
    "- 이 커널은 매크로 F1 점수를 사용하여 교육을 조기에 중단합니다. 이 작업은 스코어링 전략에 맞추기 위해 수행됩니다.\n",
    "- 범주는 블라인드 레이블 인코딩 대신 적절한 매핑을 가진 숫자로 바뀝니다.\n",
    "- OHE는 트리 모델의 경우 더 쉽게 다이제스트할 수 있으므로 레이블 인코딩으로 반전됩니다. 이 트릭은 나무가 아닌 모델에게는 해로울 수 있으니 조심하세요.\n",
    "- idhogar는 훈련에서 사용되지 않습니다. 정보가 있을 수 있는 유일한 방법은 데이터 누출입니다. 우리는 여기서 가난과 싸우고 있습니다. 유출을 이용하는 것은 어떤 식으로든 가난을 감소시키지 않을 것입니다.\n",
    "- 가구 내에서 집계가 이루어지며 새로운 기능은 수작업으로 제작됩니다. 대부분의 기능이 이미 가정 수준에서 인용되었기 때문에 집계할 수 있는 기능이 많지 않습니다.\n",
    "- 투표 분류기는 여러 LightGBM 모델에서 평균을 내는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1회차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T03:46:51.259609Z",
     "start_time": "2021-02-08T03:46:51.250666Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "import joblib as extjoblib\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 범주 매핑은 이 커널에서 시작됩니다.  \n",
    "링크 : https://www.kaggle.com/mlisovyi/categorical-variables-encoding-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:01:20.799900Z",
     "start_time": "2021-02-08T01:01:20.787128Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# idhogar field만 변환, 나머지는 다른곳에서 바뀜\n",
    "def encode_data(df):\n",
    "    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n",
    "\n",
    "# sklearn 의사 결정 트리에 플롯 기능 중요도표시\n",
    "def feature_importance(forest, X_train, display_results=True):\n",
    "    ranked_list = []\n",
    "    zero_features = []\n",
    "    \n",
    "    importances = forest.feature_importances_\n",
    "\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    if display_results:\n",
    "        print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X_train.shape[1]):\n",
    "        if display_results:\n",
    "            print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]) + \" - \" + X_train.columns[indices[f]])\n",
    "        \n",
    "        ranked_list.append(X_train.columns[indices[f]])\n",
    "        \n",
    "        if importances[indices[f]] == 0.0:\n",
    "            zero_features.append(X_train.columns[indices[f]])\n",
    "            \n",
    "    return ranked_list, zero_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특성공학에 필요한 추가함수는 여기에도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:13:43.239431Z",
     "start_time": "2021-02-08T01:13:43.222183Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_features(df):\n",
    "    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n",
    "                 ('working_man_fraction', 'r4h2', 'r4t3'),\n",
    "                 ('all_man_fraction', 'r4h3', 'r4t3'),\n",
    "                 ('human_density', 'tamviv', 'rooms'),\n",
    "                 ('human_bed_density', 'tamviv', 'bedrooms'),\n",
    "                 ('rent_per_person', 'v2a1', 'r4t3'),\n",
    "                 ('rent_per_room', 'v2a1', 'rooms'),\n",
    "                 ('mobile_density', 'qmobilephone', 'r4t3'),\n",
    "                 ('tablet_density', 'v18q1', 'r4t3'),\n",
    "                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n",
    "                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n",
    "                ]\n",
    "    \n",
    "    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n",
    "                 ('people_weird_stat', 'tamhog', 'r4t3')]\n",
    "\n",
    "    for f_new, f1, f2 in feats_div:\n",
    "        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n",
    "    for f_new, f1, f2 in feats_sub:\n",
    "        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n",
    "    \n",
    "    # 가정에 대한 집계 규칙(나이와 교육수준에 따라)\n",
    "    aggs_num = {'age': ['min', 'max', 'mean'],\n",
    "                'escolari': ['min', 'max', 'mean']\n",
    "               }\n",
    "    \n",
    "    aggs_cat = {'dis': ['mean']}\n",
    "    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n",
    "        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n",
    "            aggs_cat[f_] = ['mean', 'count']\n",
    "\n",
    "    # 가구별 집계\n",
    "    for name_, df_ in [('18', df.query('age >= 18'))]:\n",
    "        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n",
    "        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n",
    "        df = df.join(df_agg, how='left', on='idhogar')\n",
    "        del df_agg\n",
    "\n",
    "    # id 삭제\n",
    "    df.drop(['Id'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:19:04.189150Z",
     "start_time": "2021-02-08T01:19:04.175540Z"
    }
   },
   "outputs": [],
   "source": [
    "# one-hot 인코딩된 필드 하나를 레이블 인코딩으로 변환\n",
    "def convert_OHE2LE(df):\n",
    "    tmp_df = df.copy(deep=True)\n",
    "    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n",
    "               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n",
    "               'instlevel', 'lugar', 'tipovivi',\n",
    "               'manual_elec']:\n",
    "        if 'manual_' not in s_:\n",
    "            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n",
    "        elif 'elec' in s_:\n",
    "            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n",
    "        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "        \n",
    "        #열의 합계가 0인 OHE를 처리\n",
    "        if 0 in sum_ohe:\n",
    "            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n",
    "                  .format(s_))\n",
    "            # 더미 컬럼 이름을 추가\n",
    "            col_dummy = s_+'_dummy'\n",
    "            # 데이터 프레임에 열을 추가\n",
    "            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n",
    "            # 라벨이 부착될 열 목록에 이름을 추가\n",
    "            cols_s_.append(col_dummy)\n",
    "            # 인증 확인, 이제 카테고리가 완료됨.\n",
    "            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "            if 0 in sum_ohe:\n",
    "                 print(\"The category completion did not work\")\n",
    "        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n",
    "        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n",
    "        if 'parentesco1' in cols_s_:\n",
    "            cols_s_.remove('parentesco1')\n",
    "        tmp_df.drop(cols_s_, axis=1, inplace=True)\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:19:50.291104Z",
     "start_time": "2021-02-08T01:19:49.771190Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/이동훈/Desktop/github/kaggle/kagglestudy/Data/costa-rican/train.csv')\n",
    "test = pd.read_csv('C:/Users/이동훈/Desktop/github/kaggle/kagglestudy/Data/costa-rican/test.csv')\n",
    "test_ids = test.Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:20:29.639815Z",
     "start_time": "2021-02-08T01:20:27.984134Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_df(df_):\n",
    "    encode_data(df_)\n",
    "    \n",
    "    return do_features(df_)\n",
    "\n",
    "train = process_df(train)\n",
    "test = process_df(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "누락된 데이터를 정리하고 개체를 숫자로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:26:15.578437Z",
     "start_time": "2021-02-08T01:26:15.472900Z"
    }
   },
   "outputs": [],
   "source": [
    "# dependency 일부는 Na입니다. 제곱근으로 채우십시오.\n",
    "train['dependency'] = np.sqrt(train['SQBdependency'])\n",
    "test['dependency'] = np.sqrt(test['SQBdependency'])\n",
    "\n",
    "#education에서 no를 0으로 변환\n",
    "train.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0\n",
    "train.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0\n",
    "test.loc[test['edjefa'] == \"no\", \"edjefa\"] = 0\n",
    "test.loc[test['edjefe'] == \"no\", \"edjefe\"] = 0\n",
    "\n",
    "# 만약 education이 \"yes\"이고, 해당 인원이 가장이라면, escolari를 채운다.\n",
    "train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"edjefa\"] = train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"edjefe\"] = train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "\n",
    "test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"edjefa\"] = test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"edjefe\"] = test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "\n",
    "# 이 필드는 성별과 에스콜라리 사이의 상호작용이어야 하지만 \"예\"가 무엇을 의미하는지 분명하지 않는다. 일단 4로 채우겠습니다.\n",
    "train.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
    "train.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
    "\n",
    "test.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
    "test.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
    "\n",
    "# int로 변환\n",
    "train['edjefe'] = train['edjefe'].astype(\"int\")\n",
    "train['edjefa'] = train['edjefa'].astype(\"int\")\n",
    "test['edjefe'] = test['edjefe'].astype(\"int\")\n",
    "test['edjefa'] = test['edjefa'].astype(\"int\")\n",
    "\n",
    "# 가장의 최대 교육으로 특성을 새로 만듭니다.\n",
    "train['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\n",
    "test['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n",
    "\n",
    "# na 채우기\n",
    "train['v2a1']=train['v2a1'].fillna(0)\n",
    "test['v2a1']=test['v2a1'].fillna(0)\n",
    "\n",
    "test['v18q1']=test['v18q1'].fillna(0)\n",
    "train['v18q1']=train['v18q1'].fillna(0)\n",
    "\n",
    "train['rez_esc']=train['rez_esc'].fillna(0)\n",
    "test['rez_esc']=test['rez_esc'].fillna(0)\n",
    "\n",
    "train.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "train.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "test.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "test.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "# 데이터의 불일치를 고칩니다 - 일부 행은 가정에 화장실이 없고 없는 것을 나타냅니다. \n",
    "# 물이 없다면, 우리는 그들이 물을 마시지 않는다고 가정할 것입니다.train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\n",
    "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\n",
    "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n",
    "\n",
    "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\n",
    "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:31:02.236027Z",
     "start_time": "2021-02-08T01:31:02.229045Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_apply_func(train_, test_, func_):\n",
    "    test_['Target'] = 0\n",
    "    xx = pd.concat([train_, test_])\n",
    "\n",
    "    xx_func = func_(xx)\n",
    "    train_ = xx_func.iloc[:train_.shape[0], :]\n",
    "    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n",
    "\n",
    "    del xx, xx_func\n",
    "    return train_, test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:31:34.903174Z",
     "start_time": "2021-02-08T01:31:32.417223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OHE in techo is incomplete. A new column will be added before label encoding\n",
      "The OHE in instlevel is incomplete. A new column will be added before label encoding\n",
      "The OHE in manual_elec is incomplete. A new column will be added before label encoding\n"
     ]
    }
   ],
   "source": [
    "train,test = train_test_apply_func(train,test,convert_OHE2LE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:37:41.629419Z",
     "start_time": "2021-02-08T01:37:41.315638Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n",
    "              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n",
    "              'pared_LE']\n",
    "cols_nums = ['age', 'meaneduc', 'dependency', \n",
    "             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n",
    "             'bedrooms', 'overcrowding']\n",
    "\n",
    "def convert_geo2aggs(df_):\n",
    "    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n",
    "                        pd.get_dummies(df_[cols_2_ohe], \n",
    "                                       columns=cols_2_ohe)],axis=1)\n",
    "\n",
    "    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n",
    "    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n",
    "    \n",
    "    del tmp_df\n",
    "    return df_.join(geo_agg, how='left', on='lugar_LE')\n",
    "\n",
    "# 지역별로 일부 집계를 추가\n",
    "train, test = train_test_apply_func(train, test, convert_geo2aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:45:27.005281Z",
     "start_time": "2021-02-08T01:45:26.495089Z"
    }
   },
   "outputs": [],
   "source": [
    "# 각 가정에 18세 이상의 인원을 추가\n",
    "train['num_over_18'] = 0\n",
    "train['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\")\n",
    "train['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "train['num_over_18'] = train['num_over_18'].fillna(0)\n",
    "\n",
    "test['num_over_18'] = 0\n",
    "test['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\")\n",
    "test['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "test['num_over_18'] = test['num_over_18'].fillna(0)\n",
    "\n",
    "# 추가 기능을 추가합니다.(이 기능은 다른 커널에서 가져온 것입니다.- 아마 )\n",
    "def extract_features(df):\n",
    "    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n",
    "    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n",
    "    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # tamhog - 가정의 크기\n",
    "    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3 - 가정의 총 인원\n",
    "    df['r4t3_to_rooms'] = df['r4t3']/df['rooms']\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/df['r4t3'] # 임대인의 수\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1']) # 12세 이하의 임대인의 수\n",
    "    df['hhsize_to_rooms'] = df['hhsize']/df['rooms'] # 방 당 사람 수\n",
    "    df['rent_to_hhsize'] = df['v2a1']/df['hhsize'] # 임대 크기\n",
    "    df['rent_to_over_18'] = df['v2a1']/df['num_over_18']\n",
    "    # 18세 이상의 사람이 없는 가구도 있습니다. 총 임대료를 사용합니다.\n",
    "    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n",
    "    \n",
    "extract_features(train)    \n",
    "extract_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:47:12.932231Z",
     "start_time": "2021-02-08T01:47:12.899355Z"
    }
   },
   "outputs": [],
   "source": [
    "#중복열 제거\n",
    "needless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n",
    "                 'mobilephone', 'female' ]\n",
    "instlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n",
    "\n",
    "needless_cols.extend(instlevel_cols)\n",
    "\n",
    "train = train.drop(needless_cols,axis=1)\n",
    "test = test.drop(needless_cols,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data\n",
    "동일한 가구에 속하는 행은 대개 동일한 대상을 가지므로, 누출을 방지하기 위해 가구별로 데이터를 분할합니다. 가장만 포함하도록 데이터를 필터링하기 때문에 기술적으로는 필요하지 않지만, 그렇게 하려면 전체 교육 데이터 세트를 쉽게 사용할 수 있습니다.\n",
    "\n",
    "데이터를 분할한 후에는 열차 데이터를 전체 데이터 세트로 덮어써 모든 데이터를 훈련할 수 있습니다. split_data 기능은 데이터를 덮어쓰지 않고 동일한 작업을 수행하며, 교육 루프 내에서 K-Fold 분할을 근사화하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:55:09.869912Z",
     "start_time": "2021-02-08T01:55:09.861928Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(train, y, sample_weight=None, households=None, test_percentage=0.20, seed=None):\n",
    "    \n",
    "    train2 = train.copy()\n",
    "    \n",
    "    # 테스트 데이터에 사용할 일부 랜덤 가구를 선택\n",
    "    cv_hhs = np.random.choice(households, size=int(len(households) * test_percentage), replace=False)\n",
    "    \n",
    "    # 랜덤 선택 대상 가구를 선택합니다.\n",
    "    cv_idx = np.isin(households, cv_hhs)\n",
    "    X_test = train2[cv_idx]\n",
    "    y_test = y[cv_idx]\n",
    "\n",
    "    X_train = train2[~cv_idx]\n",
    "    y_train = y[~cv_idx]\n",
    "    \n",
    "    if sample_weight is not None:\n",
    "        y_train_weights = sample_weight[~cv_idx]\n",
    "        return X_train, y_train, X_test, y_test, y_train_weights\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:58:24.287582Z",
     "start_time": "2021-02-08T01:58:24.237716Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train.query('parentesco1==1')\n",
    "\n",
    "# Target 변수를 빼내어 놓습니다.\n",
    "y = X['Target'] - 1\n",
    "X = X.drop(['Target'],axis=1)\n",
    "\n",
    "np.random.seed(seed=None)\n",
    "\n",
    "train2 = X.copy()\n",
    "train_hhs = train2.idhogar\n",
    "\n",
    "households = train2.idhogar.unique()\n",
    "cv_hhs = np.random.choice(households,size=int(len(households)*0.15),replace=False)\n",
    "\n",
    "cv_idx = np.isin(train2.idhogar,cv_hhs)\n",
    "\n",
    "X_test = train2[cv_idx]\n",
    "y_test = y[cv_idx]\n",
    "\n",
    "X_train = train2[~cv_idx]\n",
    "y_train = y[~cv_idx]\n",
    "\n",
    "X_train = train2\n",
    "y_train = y\n",
    "\n",
    "train_households = X_train.idhogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T01:58:55.049922Z",
     "start_time": "2021-02-08T01:58:55.041768Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_weights = class_weight.compute_sample_weight('balanced', y_train, indices=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T02:31:56.948969Z",
     "start_time": "2021-02-08T02:31:56.939994Z"
    }
   },
   "outputs": [],
   "source": [
    "extra_drop_features = [\n",
    " 'agg18_estadocivil1_MEAN',\n",
    " 'agg18_estadocivil6_COUNT',\n",
    " 'agg18_estadocivil7_COUNT',\n",
    " 'agg18_parentesco10_COUNT',\n",
    " 'agg18_parentesco11_COUNT',\n",
    " 'agg18_parentesco12_COUNT',\n",
    " 'agg18_parentesco1_COUNT',\n",
    " 'agg18_parentesco2_COUNT',\n",
    " 'agg18_parentesco3_COUNT',\n",
    " 'agg18_parentesco4_COUNT',\n",
    " 'agg18_parentesco5_COUNT',\n",
    " 'agg18_parentesco6_COUNT',\n",
    " 'agg18_parentesco7_COUNT',\n",
    " 'agg18_parentesco8_COUNT',\n",
    " 'agg18_parentesco9_COUNT',\n",
    " 'geo_elimbasu_LE_4',\n",
    " 'geo_energcocinar_LE_1',\n",
    " 'geo_energcocinar_LE_2',\n",
    " 'geo_epared_LE_0',\n",
    " 'geo_hogar_mayor',\n",
    " 'geo_manual_elec_LE_2',\n",
    " 'geo_pared_LE_3',\n",
    " 'geo_pared_LE_4',\n",
    " 'geo_pared_LE_5',\n",
    " 'geo_pared_LE_6',\n",
    " 'num_over_18',\n",
    " 'parentesco_LE',\n",
    " 'rez_esc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T02:45:09.429867Z",
     "start_time": "2021-02-08T02:45:09.426238Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_drop_cols = extra_drop_features + [\"idhogar\",  'parentesco1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a voting classifier\n",
    "조기 중지를 위해 fit_params를 전달할 수 있도록 파생된 VotingClassifier 클래스를 정의합니다. 매크로 F1에 따라 조기 중지되고 학습 속도가 저하되는 LGBM 모델을 기준으로 투표합니다.\n",
    "\n",
    "매개 변수는 이 커널에서 임의 검색을 통해 최적화됩니다.\n",
    "링크 : https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T03:00:58.963403Z",
     "start_time": "2021-02-08T03:00:58.947474Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_parameters = {'max_depth':35, 'eta':0.1, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 1, 'num_class': 4, 'gamma': 2.0, 'colsample_bylevel': 0.9, 'subsample': 0.84, 'colsample_bytree': 0.88, 'reg_lambda': 0.40 }\n",
    "opt_parameters = {'max_depth':35, 'eta':0.15, 'silent':1, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.5, 'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n",
    "# opt_parameters = {'max_depth':35, 'eta':0.15, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.75, 'colsample_bylevel': 0.95, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n",
    "# opt_parameters = {'max_depth':35, 'eta':0.12, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 3.25, 'colsample_bylevel': 0.95, 'subsample': 0.88, 'colsample_bytree': 0.88, 'reg_lambda': 0.35 }\n",
    "\n",
    "def evaluate_macroF1_lgb(predictions, truth):  \n",
    "    # 내용출처 : https://github.com/Microsoft/LightGBM/issues/1483\n",
    "    pred_labels = predictions.argmax(axis=1)\n",
    "    truth = truth.get_label()\n",
    "    f1 = f1_score(truth, pred_labels, average='macro')\n",
    "    return ('macroF1', 1-f1) \n",
    "\n",
    "fit_params={\"early_stopping_rounds\":500,\n",
    "            \"eval_metric\" : evaluate_macroF1_lgb, \n",
    "            \"eval_set\" : [(X_train,y_train), (X_test,y_test)],\n",
    "            'verbose': False,\n",
    "           }\n",
    "\n",
    "def learning_rate_power_0997(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    min_learning_rate = 0.02\n",
    "    lr = base_learning_rate  * np.power(.995, current_iter)\n",
    "    return max(lr, min_learning_rate)\n",
    "\n",
    "fit_params['verbose'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T03:40:02.073972Z",
     "start_time": "2021-02-08T03:40:00.949979Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "def _parallel_fit_estimator(estimator1, X, y, sample_weight=None, threshold=True, **fit_params):\n",
    "    estimator = clone(estimator1)\n",
    "    \n",
    "    # 데이터를 랜덤하게 분할하여 조기 정지를 위한 테스트 세트를 제공\n",
    "    if sample_weight is not None:\n",
    "        X_train, y_train, X_test, y_test, y_train_weight = split_data(X, y, sample_weight, households=train_households)\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test = split_data(X, y, None, households=train_households)\n",
    "        \n",
    "    # 새롭게 분할된 값으로 매개변수를 업데이트\n",
    "    fit_params[\"eval_set\"] = [(X_test,y_test)]\n",
    "    \n",
    "    # 측정기에 피팅\n",
    "    if sample_weight is not None:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, sample_weight=y_train_weight, **fit_params)\n",
    "    else:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, **fit_params)\n",
    "    \n",
    "    if not isinstance(estimator1, ExtraTreesClassifier) and not isinstance(estimator1, RandomForestClassifier) and not isinstance(estimator1, xgb.XGBClassifier):\n",
    "        best_cv_round = np.argmax(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_cv = np.max(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_train = estimator.evals_result_['train']['macroF1'][best_cv_round]\n",
    "    else:\n",
    "        best_train = f1_score(y_train, estimator.predict(X_train), average=\"macro\")\n",
    "        best_cv = f1_score(y_test, estimator.predict(X_test), average=\"macro\")\n",
    "        print(\"Train F1:\", best_train)\n",
    "        print(\"Test F1:\", best_cv)\n",
    "        \n",
    "    # 훈련 및 테스트 세트의 성능에 따라 일부 측정기를 삭제함\n",
    "    if threshold:\n",
    "        # 유효 점수가 매우 높으면 train 점수보다 조금 더 여유 있게 집어넣겠다.\n",
    "        if ((best_cv > 0.37) and (best_train > 0.75)) or ((best_cv > 0.44) and (best_train > 0.65)):\n",
    "            return estimator\n",
    "\n",
    "        # 그렇지 않으면 더 나은 것을 얻을 때까지 반복\n",
    "        else:\n",
    "            print(\"Unacceptable!!! Trying again...\")\n",
    "            return _parallel_fit_estimator(estimator1, X, y, sample_weight=sample_weight, **fit_params)\n",
    "    \n",
    "    else:\n",
    "        return estimator\n",
    "    \n",
    "class VotingClassifierLGBM(VotingClassifier):\n",
    "    def fit(self, X, y, sample_weight=None, threshold=True, **fit_params):\n",
    "        \n",
    "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
    "            raise NotImplementedError('Multilabel and multi-output'\n",
    "                                      ' classification is not supported.')\n",
    "\n",
    "        if self.voting not in ('soft', 'hard'):\n",
    "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n",
    "                             % self.voting)\n",
    "\n",
    "        if self.estimators is None or len(self.estimators) == 0:\n",
    "            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n",
    "                                 ' should be a list of (string, estimator)'\n",
    "                                 ' tuples')\n",
    "\n",
    "        if (self.weights is not None and\n",
    "                len(self.weights) != len(self.estimators)):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                             '; got %d weights, %d estimators'\n",
    "                             % (len(self.weights), len(self.estimators)))\n",
    "\n",
    "        names, clfs = zip(*self.estimators)\n",
    "        self._validate_names(names)\n",
    "\n",
    "        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n",
    "        if n_isnone == len(self.estimators):\n",
    "            raise ValueError('All estimators are None. At least one is '\n",
    "                             'required to be a classifier!')\n",
    "\n",
    "        self.le_ = LabelEncoder().fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        self.estimators_ = []\n",
    "\n",
    "        transformed_y = self.le_.transform(y)\n",
    "\n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n",
    "                                                 sample_weight=sample_weight, threshold=threshold, **fit_params)\n",
    "                for clf in clfs if clf is not None)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T03:46:51.249667Z",
     "start_time": "2021-02-08T03:41:45.372557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-merror:0.464646\tvalidation_0-macroF1:0.646101\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.578445\n",
      "[100]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.570407\n",
      "[150]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.583402\n",
      "[200]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.583715\n",
      "[250]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.583333\n",
      "[299]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.583474\n",
      "Train F1: 0.9153725233385168\n",
      "Test F1: 0.4355385068644192\n",
      "[0]\tvalidation_0-merror:0.469697\tvalidation_0-macroF1:0.640044\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.583417\n",
      "[100]\tvalidation_0-merror:0.385522\tvalidation_0-macroF1:0.599753\n",
      "[150]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.593972\n",
      "[200]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.597668\n",
      "[250]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.605199\n",
      "[299]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.594076\n",
      "Train F1: 0.8913538399552563\n",
      "Test F1: 0.42422233276003724\n",
      "[0]\tvalidation_0-merror:0.473064\tvalidation_0-macroF1:0.676192\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.387205\tvalidation_0-macroF1:0.593712\n",
      "[100]\tvalidation_0-merror:0.387205\tvalidation_0-macroF1:0.595794\n",
      "[150]\tvalidation_0-merror:0.385522\tvalidation_0-macroF1:0.589911\n",
      "[200]\tvalidation_0-merror:0.383838\tvalidation_0-macroF1:0.589821\n",
      "[250]\tvalidation_0-merror:0.387205\tvalidation_0-macroF1:0.597476\n",
      "[299]\tvalidation_0-merror:0.388889\tvalidation_0-macroF1:0.596718\n",
      "Train F1: 0.8724441306732305\n",
      "Test F1: 0.42075548221923303\n",
      "[0]\tvalidation_0-merror:0.506734\tvalidation_0-macroF1:0.686421\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.356902\tvalidation_0-macroF1:0.531409\n",
      "[100]\tvalidation_0-merror:0.360269\tvalidation_0-macroF1:0.541944\n",
      "[150]\tvalidation_0-merror:0.361953\tvalidation_0-macroF1:0.543682\n",
      "[200]\tvalidation_0-merror:0.360269\tvalidation_0-macroF1:0.537668\n",
      "[250]\tvalidation_0-merror:0.356902\tvalidation_0-macroF1:0.530024\n",
      "[299]\tvalidation_0-merror:0.355219\tvalidation_0-macroF1:0.539738\n",
      "Train F1: 0.9054438067880621\n",
      "Test F1: 0.47823529311636404\n",
      "[0]\tvalidation_0-merror:0.488215\tvalidation_0-macroF1:0.690363\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.407407\tvalidation_0-macroF1:0.601575\n",
      "[100]\tvalidation_0-merror:0.390572\tvalidation_0-macroF1:0.591384\n",
      "[150]\tvalidation_0-merror:0.390572\tvalidation_0-macroF1:0.593051\n",
      "[200]\tvalidation_0-merror:0.395623\tvalidation_0-macroF1:0.601453\n",
      "[250]\tvalidation_0-merror:0.397306\tvalidation_0-macroF1:0.604831\n",
      "[299]\tvalidation_0-merror:0.397306\tvalidation_0-macroF1:0.609499\n",
      "Train F1: 0.9108878975200805\n",
      "Test F1: 0.4173477361936218\n",
      "[0]\tvalidation_0-merror:0.459596\tvalidation_0-macroF1:0.633155\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.402357\tvalidation_0-macroF1:0.617957\n",
      "[100]\tvalidation_0-merror:0.395623\tvalidation_0-macroF1:0.614103\n",
      "[150]\tvalidation_0-merror:0.390572\tvalidation_0-macroF1:0.604858\n",
      "[200]\tvalidation_0-merror:0.393939\tvalidation_0-macroF1:0.612738\n",
      "[250]\tvalidation_0-merror:0.388889\tvalidation_0-macroF1:0.609966\n",
      "[299]\tvalidation_0-merror:0.388889\tvalidation_0-macroF1:0.610004\n",
      "Train F1: 0.8893336491231894\n",
      "Test F1: 0.4051414966703659\n",
      "[0]\tvalidation_0-merror:0.464646\tvalidation_0-macroF1:0.667382\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.383838\tvalidation_0-macroF1:0.601972\n",
      "[100]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.605368\n",
      "[150]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.613276\n",
      "[200]\tvalidation_0-merror:0.387205\tvalidation_0-macroF1:0.620321\n",
      "[250]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.61699\n",
      "[299]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.614077\n",
      "Train F1: 0.8972810131834246\n",
      "Test F1: 0.40066108970156944\n",
      "[0]\tvalidation_0-merror:0.442761\tvalidation_0-macroF1:0.627544\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.604169\n",
      "[100]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.601541\n",
      "[150]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.604035\n",
      "[200]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.606346\n",
      "[250]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.612084\n",
      "[299]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.611192\n",
      "Train F1: 0.9110203375242469\n",
      "Test F1: 0.4069266211820104\n",
      "[0]\tvalidation_0-merror:0.405724\tvalidation_0-macroF1:0.569948\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.546907\n",
      "[100]\tvalidation_0-merror:0.36532\tvalidation_0-macroF1:0.55562\n",
      "[150]\tvalidation_0-merror:0.360269\tvalidation_0-macroF1:0.55303\n",
      "[200]\tvalidation_0-merror:0.350168\tvalidation_0-macroF1:0.545353\n",
      "[250]\tvalidation_0-merror:0.351852\tvalidation_0-macroF1:0.546489\n",
      "[299]\tvalidation_0-merror:0.351852\tvalidation_0-macroF1:0.547837\n",
      "Train F1: 0.9023161714314298\n",
      "Test F1: 0.4669456786718976\n",
      "[0]\tvalidation_0-merror:0.442761\tvalidation_0-macroF1:0.656075\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.60366\n",
      "[100]\tvalidation_0-merror:0.363636\tvalidation_0-macroF1:0.596851\n",
      "[150]\tvalidation_0-merror:0.360269\tvalidation_0-macroF1:0.594956\n",
      "[200]\tvalidation_0-merror:0.355219\tvalidation_0-macroF1:0.59503\n",
      "[250]\tvalidation_0-merror:0.358586\tvalidation_0-macroF1:0.601222\n",
      "[299]\tvalidation_0-merror:0.356902\tvalidation_0-macroF1:0.60041\n",
      "Train F1: 0.891328566030215\n",
      "Test F1: 0.4133092608446235\n",
      "[0]\tvalidation_0-merror:0.442761\tvalidation_0-macroF1:0.635594\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.598722\n",
      "[100]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.586422\n",
      "[150]\tvalidation_0-merror:0.363636\tvalidation_0-macroF1:0.572078\n",
      "[200]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.581793\n",
      "[250]\tvalidation_0-merror:0.363636\tvalidation_0-macroF1:0.572344\n",
      "[299]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.580221\n",
      "Train F1: 0.9200869829778409\n",
      "Test F1: 0.42932198053495463\n",
      "[0]\tvalidation_0-merror:0.447811\tvalidation_0-macroF1:0.643927\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.591184\n",
      "[100]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.604466\n",
      "[150]\tvalidation_0-merror:0.383838\tvalidation_0-macroF1:0.613042\n",
      "[200]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.611924\n",
      "[250]\tvalidation_0-merror:0.382155\tvalidation_0-macroF1:0.609384\n",
      "[299]\tvalidation_0-merror:0.383838\tvalidation_0-macroF1:0.618041\n",
      "Train F1: 0.9006803320768149\n",
      "Test F1: 0.4254007528480266\n",
      "[0]\tvalidation_0-merror:0.442761\tvalidation_0-macroF1:0.615498\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.563908\n",
      "[100]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.561588\n",
      "[150]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.56399\n",
      "[200]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.564721\n",
      "[250]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.569799\n",
      "[299]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.565604\n",
      "Train F1: 0.868783685054759\n",
      "Test F1: 0.45728526150302823\n",
      "[0]\tvalidation_0-merror:0.442761\tvalidation_0-macroF1:0.599582\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.39899\tvalidation_0-macroF1:0.590061\n",
      "[100]\tvalidation_0-merror:0.40404\tvalidation_0-macroF1:0.60687\n",
      "[150]\tvalidation_0-merror:0.400673\tvalidation_0-macroF1:0.600754\n",
      "[200]\tvalidation_0-merror:0.392256\tvalidation_0-macroF1:0.597934\n",
      "[250]\tvalidation_0-merror:0.395623\tvalidation_0-macroF1:0.604446\n",
      "[299]\tvalidation_0-merror:0.393939\tvalidation_0-macroF1:0.601292\n",
      "Train F1: 0.8967214529450815\n",
      "Test F1: 0.4262237902576739\n",
      "[0]\tvalidation_0-merror:0.441077\tvalidation_0-macroF1:0.635276\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.348485\tvalidation_0-macroF1:0.564666\n",
      "[100]\tvalidation_0-merror:0.350168\tvalidation_0-macroF1:0.565395\n",
      "[150]\tvalidation_0-merror:0.351852\tvalidation_0-macroF1:0.569924\n",
      "[200]\tvalidation_0-merror:0.348485\tvalidation_0-macroF1:0.566517\n",
      "[250]\tvalidation_0-merror:0.348485\tvalidation_0-macroF1:0.571177\n",
      "[299]\tvalidation_0-merror:0.348485\tvalidation_0-macroF1:0.566762\n",
      "Train F1: 0.8935956630026125\n",
      "Test F1: 0.4472704708236759\n"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "for i in range(15):\n",
    "    clf = xgb.XGBClassifier(random_state=217+i, n_estimators=300, learning_rate=0.15, n_jobs=4, **opt_parameters)\n",
    "    \n",
    "    clfs.append(('xgb{}'.format(i), clf))\n",
    "    \n",
    "vc = VotingClassifierLGBM(clfs, voting='soft')\n",
    "del(clfs)\n",
    "\n",
    "# 최종 모델을 교육합니다.\n",
    "_ = vc.fit(X_train.drop(xgb_drop_cols, axis=1), y_train, sample_weight=y_train_weights, threshold=False, **fit_params)\n",
    "\n",
    "clf_final = vc.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T03:54:06.690975Z",
     "start_time": "2021-02-08T03:54:06.498491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a single LGBM Classifier: 0.8182\n",
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8901\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.8958\n"
     ]
    }
   ],
   "source": [
    "\n",
    "global_score = f1_score(y_test, clf_final.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "vc.voting = 'soft'\n",
    "global_score_soft = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "vc.voting = 'hard'\n",
    "global_score_hard = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "\n",
    "print('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T04:00:12.884784Z",
     "start_time": "2021-02-08T04:00:12.037023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agg18_estadocivil4_COUNT',\n",
       " 'agg18_estadocivil5_COUNT',\n",
       " 'geo_energcocinar_LE_0',\n",
       " 'geo_epared_LE_2',\n",
       " 'geo_manual_elec_LE_3'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모든 모델에서 사용되지 않는 특성 파악\n",
    "useless_features = []\n",
    "drop_features = set()\n",
    "counter = 0\n",
    "for est in vc.estimators_:\n",
    "    ranked_features, unused_features = feature_importance(est, X_train.drop(xgb_drop_cols, axis=1), display_results=False)\n",
    "    useless_features.append(unused_features)\n",
    "    if counter == 0:\n",
    "        drop_features = set(unused_features)\n",
    "    else:\n",
    "        drop_features = drop_features.intersection(set(unused_features))\n",
    "    counter += 1\n",
    "    \n",
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T04:02:54.665429Z",
     "start_time": "2021-02-08T04:02:54.591654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 59 (0.020874) - agg18_escolari_MAX\n",
      "2. feature 42 (0.019113) - fe_children_fraction\n",
      "3. feature 37 (0.017560) - SQBedjefe\n",
      "4. feature 126 (0.016810) - geo_sanitario_LE_3\n",
      "5. feature 74 (0.015333) - agg18_parentesco2_MEAN\n",
      "6. feature 46 (0.014168) - fe_human_bed_density\n",
      "7. feature 22 (0.013392) - dependency\n",
      "8. feature 60 (0.013138) - agg18_escolari_MEAN\n",
      "9. feature 40 (0.012836) - SQBdependency\n",
      "10. feature 135 (0.012545) - geo_pared_LE_7\n",
      "11. feature 112 (0.012330) - geo_etecho_LE_1\n",
      "12. feature 107 (0.011216) - geo_overcrowding\n",
      "13. feature 124 (0.010565) - geo_sanitario_LE_1\n",
      "14. feature 116 (0.010347) - geo_elimbasu_LE_0\n",
      "15. feature 19 (0.010287) - hogar_adul\n",
      "16. feature 34 (0.010142) - SQBescolari\n",
      "17. feature 105 (0.009962) - geo_hogar_total\n",
      "18. feature 23 (0.009942) - edjefe\n",
      "19. feature 94 (0.009886) - etecho_LE\n",
      "20. feature 17 (0.009594) - male\n",
      "21. feature 39 (0.009528) - SQBovercrowding\n",
      "22. feature 41 (0.009366) - SQBmeaned\n",
      "23. feature 44 (0.009305) - fe_all_man_fraction\n",
      "24. feature 96 (0.009216) - estadocivil_LE\n",
      "25. feature 6 (0.009131) - r4h1\n",
      "26. feature 97 (0.009013) - lugar_LE\n",
      "27. feature 65 (0.008972) - agg18_estadocivil3_MEAN\n",
      "28. feature 15 (0.008932) - cielorazo\n",
      "29. feature 16 (0.008922) - dis\n",
      "30. feature 120 (0.008912) - geo_elimbasu_LE_5\n",
      "31. feature 93 (0.008863) - epared_LE\n",
      "32. feature 106 (0.008822) - geo_bedrooms\n",
      "33. feature 104 (0.008821) - geo_hogar_adul\n",
      "34. feature 95 (0.008810) - eviv_LE\n",
      "35. feature 98 (0.008801) - tipovivi_LE\n",
      "36. feature 55 (0.008797) - agg18_age_MIN\n",
      "37. feature 12 (0.008775) - r4t1\n",
      "38. feature 92 (0.008711) - elimbasu_LE\n",
      "39. feature 131 (0.008691) - geo_manual_elec_LE_4\n",
      "40. feature 49 (0.008591) - fe_mobile_density\n",
      "41. feature 35 (0.008552) - SQBage\n",
      "42. feature 111 (0.008547) - geo_etecho_LE_0\n",
      "43. feature 27 (0.008483) - overcrowding\n",
      "44. feature 125 (0.008438) - geo_sanitario_LE_2\n",
      "45. feature 14 (0.008432) - escolari\n",
      "46. feature 5 (0.008364) - v18q1\n",
      "47. feature 33 (0.008358) - age\n",
      "48. feature 13 (0.008337) - r4t2\n",
      "49. feature 87 (0.008315) - piso_LE\n",
      "50. feature 86 (0.008305) - pared_LE\n",
      "51. feature 117 (0.008305) - geo_elimbasu_LE_1\n",
      "52. feature 25 (0.008275) - meaneduc\n",
      "53. feature 123 (0.008270) - geo_sanitario_LE_0\n",
      "54. feature 51 (0.008260) - fe_mobile_adult_density\n",
      "55. feature 69 (0.008140) - agg18_estadocivil5_MEAN\n",
      "56. feature 71 (0.008139) - agg18_estadocivil6_MEAN\n",
      "57. feature 72 (0.008112) - agg18_estadocivil7_MEAN\n",
      "58. feature 45 (0.008103) - fe_human_density\n",
      "59. feature 30 (0.008054) - qmobilephone\n",
      "60. feature 58 (0.008048) - agg18_escolari_MIN\n",
      "61. feature 43 (0.008048) - fe_working_man_fraction\n",
      "62. feature 102 (0.007966) - geo_dependency\n",
      "63. feature 0 (0.007906) - v2a1\n",
      "64. feature 128 (0.007870) - geo_manual_elec_LE_0\n",
      "65. feature 10 (0.007865) - r4m2\n",
      "66. feature 32 (0.007738) - area2\n",
      "67. feature 7 (0.007579) - r4h2\n",
      "68. feature 21 (0.007507) - hogar_total\n",
      "69. feature 109 (0.007466) - geo_eviv_LE_1\n",
      "70. feature 47 (0.007323) - fe_rent_per_person\n",
      "71. feature 142 (0.007268) - hhsize_to_rooms\n",
      "72. feature 63 (0.007225) - agg18_estadocivil2_MEAN\n",
      "73. feature 136 (0.007217) - bedrooms_to_rooms\n",
      "74. feature 1 (0.007192) - hacdor\n",
      "75. feature 138 (0.007158) - tamhog_to_rooms\n",
      "76. feature 50 (0.007148) - fe_tablet_density\n",
      "77. feature 2 (0.007079) - rooms\n",
      "78. feature 29 (0.007009) - television\n",
      "79. feature 91 (0.007006) - energcocinar_LE\n",
      "80. feature 99 (0.007003) - manual_elec_LE\n",
      "81. feature 57 (0.006932) - agg18_age_MEAN\n",
      "82. feature 62 (0.006899) - agg18_estadocivil1_COUNT\n",
      "83. feature 90 (0.006889) - sanitario_LE\n",
      "84. feature 100 (0.006777) - geo_age\n",
      "85. feature 31 (0.006771) - area1\n",
      "86. feature 61 (0.006766) - agg18_dis_MEAN\n",
      "87. feature 24 (0.006738) - edjefa\n",
      "88. feature 8 (0.006734) - r4h3\n",
      "89. feature 52 (0.006694) - fe_tablet_adult_density\n",
      "90. feature 143 (0.006689) - rent_to_hhsize\n",
      "91. feature 11 (0.006668) - r4m3\n",
      "92. feature 122 (0.006643) - geo_energcocinar_LE_3\n",
      "93. feature 48 (0.006560) - fe_rent_per_room\n",
      "94. feature 64 (0.006504) - agg18_estadocivil2_COUNT\n",
      "95. feature 56 (0.006441) - agg18_age_MAX\n",
      "96. feature 20 (0.006432) - hogar_mayor\n",
      "97. feature 26 (0.006391) - bedrooms\n",
      "98. feature 9 (0.006350) - r4m1\n",
      "99. feature 75 (0.006242) - agg18_parentesco3_MEAN\n",
      "100. feature 4 (0.006161) - refrig\n",
      "101. feature 18 (0.005876) - hogar_nin\n",
      "102. feature 101 (0.005755) - geo_meaneduc\n",
      "103. feature 38 (0.005725) - SQBhogar_nin\n",
      "104. feature 103 (0.005689) - geo_hogar_nin\n",
      "105. feature 28 (0.005391) - computer\n",
      "106. feature 140 (0.005380) - r4t3_to_rooms\n",
      "107. feature 88 (0.005258) - techo_LE\n",
      "108. feature 144 (0.005221) - rent_to_over_18\n",
      "109. feature 67 (0.005210) - agg18_estadocivil4_MEAN\n",
      "110. feature 110 (0.005132) - geo_eviv_LE_2\n",
      "111. feature 129 (0.005069) - geo_manual_elec_LE_1\n",
      "112. feature 53 (0.005035) - fe_people_not_living\n",
      "113. feature 79 (0.004939) - agg18_parentesco7_MEAN\n",
      "114. feature 77 (0.004880) - agg18_parentesco5_MEAN\n",
      "115. feature 137 (0.004457) - rent_to_rooms\n",
      "116. feature 114 (0.004299) - geo_epared_LE_1\n",
      "117. feature 81 (0.004282) - agg18_parentesco9_MEAN\n",
      "118. feature 141 (0.004248) - v2a1_to_r4t3\n",
      "119. feature 83 (0.004106) - agg18_parentesco11_MEAN\n",
      "120. feature 133 (0.003824) - geo_pared_LE_1\n",
      "121. feature 3 (0.003191) - hacapo\n",
      "122. feature 78 (0.002986) - agg18_parentesco6_MEAN\n",
      "123. feature 84 (0.002735) - agg18_parentesco12_MEAN\n",
      "124. feature 89 (0.002506) - abastagua_LE\n",
      "125. feature 139 (0.001633) - r4t3_to_tamhog\n",
      "126. feature 76 (0.001460) - agg18_parentesco4_MEAN\n",
      "127. feature 80 (0.000000) - agg18_parentesco8_MEAN\n",
      "128. feature 132 (0.000000) - geo_pared_LE_0\n",
      "129. feature 119 (0.000000) - geo_elimbasu_LE_3\n",
      "130. feature 36 (0.000000) - SQBhogar_total\n",
      "131. feature 82 (0.000000) - agg18_parentesco10_MEAN\n",
      "132. feature 73 (0.000000) - agg18_parentesco1_MEAN\n",
      "133. feature 118 (0.000000) - geo_elimbasu_LE_2\n",
      "134. feature 130 (0.000000) - geo_manual_elec_LE_3\n",
      "135. feature 70 (0.000000) - agg18_estadocivil5_COUNT\n",
      "136. feature 68 (0.000000) - agg18_estadocivil4_COUNT\n",
      "137. feature 115 (0.000000) - geo_epared_LE_2\n",
      "138. feature 66 (0.000000) - agg18_estadocivil3_COUNT\n",
      "139. feature 85 (0.000000) - edjef\n",
      "140. feature 121 (0.000000) - geo_energcocinar_LE_0\n",
      "141. feature 54 (0.000000) - fe_people_weird_stat\n",
      "142. feature 108 (0.000000) - geo_eviv_LE_0\n",
      "143. feature 134 (0.000000) - geo_pared_LE_2\n",
      "144. feature 113 (0.000000) - geo_etecho_LE_2\n",
      "145. feature 127 (0.000000) - geo_sanitario_LE_4\n"
     ]
    }
   ],
   "source": [
    "ranked_features = feature_importance(clf_final, X_train.drop(xgb_drop_cols, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T04:06:38.715419Z",
     "start_time": "2021-02-08T04:06:38.697467Z"
    }
   },
   "outputs": [],
   "source": [
    "et_drop_cols = ['agg18_age_MAX', 'agg18_age_MEAN', 'agg18_age_MIN', 'agg18_dis_MEAN',\n",
    "       'agg18_escolari_MAX', 'agg18_escolari_MEAN', 'agg18_escolari_MIN',\n",
    "       'agg18_estadocivil1_COUNT', 'agg18_estadocivil1_MEAN',\n",
    "       'agg18_estadocivil2_COUNT', 'agg18_estadocivil2_MEAN',\n",
    "       'agg18_estadocivil3_COUNT', 'agg18_estadocivil3_MEAN',\n",
    "       'agg18_estadocivil4_COUNT', 'agg18_estadocivil4_MEAN',\n",
    "       'agg18_estadocivil5_COUNT', 'agg18_estadocivil5_MEAN',\n",
    "       'agg18_estadocivil6_COUNT', 'agg18_estadocivil6_MEAN',\n",
    "       'agg18_estadocivil7_COUNT', 'agg18_estadocivil7_MEAN',\n",
    "       'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN',\n",
    "       'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN',\n",
    "       'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN',\n",
    "       'agg18_parentesco1_COUNT', 'agg18_parentesco1_MEAN',\n",
    "       'agg18_parentesco2_COUNT', 'agg18_parentesco2_MEAN',\n",
    "       'agg18_parentesco3_COUNT', 'agg18_parentesco3_MEAN',\n",
    "       'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN',\n",
    "       'agg18_parentesco5_COUNT', 'agg18_parentesco5_MEAN',\n",
    "       'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN',\n",
    "       'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN',\n",
    "       'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN',\n",
    "       'agg18_parentesco9_COUNT', 'agg18_parentesco9_MEAN']\n",
    "\n",
    "et_drop_cols.extend([\"idhogar\", \"parentesco1\", 'fe_rent_per_person', 'fe_rent_per_room',\n",
    "       'fe_tablet_adult_density', 'fe_tablet_density'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T04:12:43.796899Z",
     "start_time": "2021-02-08T04:12:20.130871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.8983848211561081\n",
      "Test F1: 0.4292190828609306\n",
      "Train F1: 0.8982569720976492\n",
      "Test F1: 0.4169636759882981\n",
      "Train F1: 0.8953245057366976\n",
      "Test F1: 0.3959736015904715\n",
      "Train F1: 0.8904009934948478\n",
      "Test F1: 0.4140651577809969\n",
      "Train F1: 0.8922849683370566\n",
      "Test F1: 0.4664528827812809\n",
      "Train F1: 0.895366590258067\n",
      "Test F1: 0.4334129309568262\n",
      "Train F1: 0.8964130336967895\n",
      "Test F1: 0.42637991299318534\n",
      "Train F1: 0.8978795660860878\n",
      "Test F1: 0.4619190301651779\n",
      "Train F1: 0.9041960041265171\n",
      "Test F1: 0.427861786122561\n",
      "Train F1: 0.8937864046617127\n",
      "Test F1: 0.4137717184547513\n"
     ]
    }
   ],
   "source": [
    "ets = []    \n",
    "for i in range(10):\n",
    "    rf = RandomForestClassifier(max_depth=None, random_state=217+i, n_jobs=4, n_estimators=700, min_impurity_decrease=1e-3, min_samples_leaf=2, verbose=0, class_weight=\"balanced\")\n",
    "    ets.append(('rf{}'.format(i), rf))   \n",
    "\n",
    "vc2 = VotingClassifierLGBM(ets, voting='soft')    \n",
    "_ = vc2.fit(X_train.drop(et_drop_cols, axis=1), y_train, threshold=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T04:14:35.767685Z",
     "start_time": "2021-02-08T04:14:31.598809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8622\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.8879\n"
     ]
    }
   ],
   "source": [
    "vc2.voting = 'soft'\n",
    "global_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "vc2.voting = 'hard'\n",
    "global_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T04:31:22.608579Z",
     "start_time": "2021-02-08T04:31:21.545386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parentesco_LE', 'rez_esc'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useless_features = []\n",
    "drop_features = set()\n",
    "counter = 0\n",
    "for est in vc2.estimators_:\n",
    "    ranked_features, unused_features = feature_importance(est, X_train.drop(et_drop_cols, axis=1), display_results=False)\n",
    "    useless_features.append(unused_features)\n",
    "    if counter == 0:\n",
    "        drop_features = set(unused_features)\n",
    "    else:\n",
    "        drop_features = drop_features.intersection(set(unused_features))\n",
    "    counter += 1\n",
    "    \n",
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T04:35:49.211052Z",
     "start_time": "2021-02-08T04:35:49.204069Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine_voters(data, weights=[0.5, 0.5]):\n",
    "    vc.voting=\"soft\"\n",
    "    vc1_probs = vc.predict_proba(data.drop(xgb_drop_cols, axis=1))\n",
    "    vc2.voting=\"soft\"\n",
    "    vc2_probs = vc2.predict_proba(data.drop(et_drop_cols, axis=1))\n",
    "    \n",
    "    final_vote = (vc1_probs * weights[0]) + (vc2_probs * weights[1])\n",
    "    predictions = np.argmax(final_vote, axis=1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T04:37:43.123811Z",
     "start_time": "2021-02-08T04:37:40.968580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8953997391497391"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.5, 0.5])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T04:37:58.557131Z",
     "start_time": "2021-02-08T04:37:56.392950Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8877354512743403"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.4, 0.6])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T04:38:11.316989Z",
     "start_time": "2021-02-08T04:38:08.996232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896336594691858"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.6, 0.4])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T04:43:05.083558Z",
     "start_time": "2021-02-08T04:43:05.044653Z"
    }
   },
   "outputs": [],
   "source": [
    "y_subm = pd.DataFrame()\n",
    "y_subm['Id'] = test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T04:46:13.046354Z",
     "start_time": "2021-02-08T04:45:52.055519Z"
    }
   },
   "outputs": [],
   "source": [
    "vc.voting = 'soft'\n",
    "y_subm_lgb = y_subm.copy(deep=True)\n",
    "y_subm_lgb['Target'] = vc.predict(test.drop(xgb_drop_cols, axis=1)) + 1\n",
    "\n",
    "vc2.voting = 'soft'\n",
    "y_subm_rf = y_subm.copy(deep=True)\n",
    "y_subm_rf['Target'] = vc2.predict(test.drop(et_drop_cols, axis=1)) + 1\n",
    "\n",
    "y_subm_ens = y_subm.copy(deep=True)\n",
    "y_subm_ens['Target'] = combine_voters(test) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:02:09.156372Z",
     "start_time": "2021-02-08T05:02:08.922030Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "sub_file_lgb = 'submission_soft_XGB_{:.4f}_{}.csv'.format(global_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "sub_file_rf = 'submission_soft_RF_{:.4f}_{}.csv'.format(global_rf_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "sub_file_ens = 'submission_ens_{:.4f}_{}.csv'.format(global_combo_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "\n",
    "y_subm_lgb.to_csv(sub_file_lgb, index=False)\n",
    "y_subm_rf.to_csv(sub_file_rf, index=False)\n",
    "y_subm_ens.to_csv(sub_file_ens, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2회차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:03:26.248221Z",
     "start_time": "2021-02-08T05:03:26.172391Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "import joblib as extjoblib\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:07:57.615871Z",
     "start_time": "2021-02-08T05:07:57.596955Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_data(df):\n",
    "    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n",
    "\n",
    "def feature_importance(forest, X_train, display_results=True):\n",
    "    ranked_list = []\n",
    "    zero_features = []\n",
    "    \n",
    "    importances = forest.feature_importances_\n",
    "\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    if display_results:\n",
    "        print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X_train.shape[1]):\n",
    "        if display_results:\n",
    "            print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]) + \" - \" + X_train.columns[indices[f]])\n",
    "        \n",
    "        ranked_list.append(X_train.columns[indices[f]])\n",
    "        \n",
    "        if importances[indices[f]] == 0.0:\n",
    "            zero_features.append(X_train.columns[indices[f]])\n",
    "            \n",
    "    return ranked_list, zero_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:11:54.113767Z",
     "start_time": "2021-02-08T05:11:54.086156Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_OHE2LE(df):\n",
    "    tmp_df = df.copy(deep=True)\n",
    "    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n",
    "               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n",
    "               'instlevel', 'lugar', 'tipovivi',\n",
    "               'manual_elec']:\n",
    "        if 'manual_' not in s_:\n",
    "            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n",
    "        elif 'elec' in s_:\n",
    "            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n",
    "        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "        if 0 in sum_ohe:\n",
    "            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n",
    "                  .format(s_))\n",
    "            col_dummy = s_+'_dummy'\n",
    "            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n",
    "            cols_s_.append(col_dummy)\n",
    "            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "            if 0 in sum_ohe:\n",
    "                 print(\"The category completion did not work\")\n",
    "        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n",
    "        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n",
    "        if 'parentesco1' in cols_s_:\n",
    "            cols_s_.remove('parentesco1')\n",
    "        tmp_df.drop(cols_s_, axis=1, inplace=True)\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:18:30.108893Z",
     "start_time": "2021-02-08T05:18:29.550389Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/이동훈/Desktop/github/kaggle/kagglestudy/Data/costa-rican/train.csv')\n",
    "test = pd.read_csv('C:/Users/이동훈/Desktop/github/kaggle/kagglestudy/Data/costa-rican/test.csv')\n",
    "test_ids = test.Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:20:29.889213Z",
     "start_time": "2021-02-08T05:20:28.584056Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_df(df_):\n",
    "    encode_data(df_)\n",
    "    return do_features(df_)\n",
    "\n",
    "train = process_df(train)\n",
    "test = process_df(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "누락된 데이터를 정리하고 개체를 숫자로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:28:12.451105Z",
     "start_time": "2021-02-08T05:28:12.286886Z"
    }
   },
   "outputs": [],
   "source": [
    "train['dependency'] = np.sqrt(train['SQBdependency'])\n",
    "test['dependency'] = np.sqrt(test['SQBdependency'])\n",
    "\n",
    "train.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0\n",
    "train.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0\n",
    "test.loc[test['edjefa'] == \"no\", \"edjefa\"] = 0\n",
    "test.loc[test['edjefe'] == \"no\", \"edjefe\"] = 0\n",
    "\n",
    "train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"edjefa\"] = train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"edjefe\"] = train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "\n",
    "test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"edjefa\"] = test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"edjefe\"] = test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "\n",
    "train.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
    "train.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
    "\n",
    "test.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
    "test.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
    "\n",
    "\n",
    "train['edjefe'] = train['edjefe'].astype(\"int\")\n",
    "train['edjefa'] = train['edjefa'].astype(\"int\")\n",
    "test['edjefe'] = test['edjefe'].astype(\"int\")\n",
    "test['edjefa'] = test['edjefa'].astype(\"int\")\n",
    "\n",
    "\n",
    "train['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\n",
    "test['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n",
    "\n",
    "\n",
    "train['v2a1']=train['v2a1'].fillna(0)\n",
    "test['v2a1']=test['v2a1'].fillna(0)\n",
    "\n",
    "test['v18q1']=test['v18q1'].fillna(0)\n",
    "train['v18q1']=train['v18q1'].fillna(0)\n",
    "\n",
    "train['rez_esc']=train['rez_esc'].fillna(0)\n",
    "test['rez_esc']=test['rez_esc'].fillna(0)\n",
    "\n",
    "train.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "train.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "test.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "test.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\n",
    "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n",
    "\n",
    "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\n",
    "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:31:15.566925Z",
     "start_time": "2021-02-08T05:31:15.558086Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_apply_func(train_, test_, func_):\n",
    "    test_['Target'] = 0\n",
    "    xx = pd.concat([train_, test_])\n",
    "\n",
    "    xx_func = func_(xx)\n",
    "    train_ = xx_func.iloc[:train_.shape[0], :]\n",
    "    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n",
    "\n",
    "    del xx, xx_func\n",
    "    return train_, test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:35:13.245201Z",
     "start_time": "2021-02-08T05:35:10.890549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OHE in techo is incomplete. A new column will be added before label encoding\n",
      "The OHE in instlevel is incomplete. A new column will be added before label encoding\n",
      "The OHE in manual_elec is incomplete. A new column will be added before label encoding\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_apply_func(train, test, convert_OHE2LE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:38:31.466551Z",
     "start_time": "2021-02-08T05:38:31.169034Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n",
    "              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n",
    "              'pared_LE']\n",
    "cols_nums = ['age', 'meaneduc', 'dependency', \n",
    "             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n",
    "             'bedrooms', 'overcrowding']\n",
    "\n",
    "def convert_geo2aggs(df_):\n",
    "    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n",
    "                        pd.get_dummies(df_[cols_2_ohe], \n",
    "                                       columns=cols_2_ohe)],axis=1)\n",
    "\n",
    "    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n",
    "    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n",
    "    \n",
    "    del tmp_df\n",
    "    return df_.join(geo_agg, how='left', on='lugar_LE')\n",
    "\n",
    "train, test = train_test_apply_func(train, test, convert_geo2aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:43:24.837689Z",
     "start_time": "2021-02-08T05:43:24.436527Z"
    }
   },
   "outputs": [],
   "source": [
    "train['num_over_18'] = 0\n",
    "train['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\")\n",
    "train['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "train['num_over_18'] = train['num_over_18'].fillna(0)\n",
    "\n",
    "test['num_over_18'] = 0\n",
    "test['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\")\n",
    "test['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "test['num_over_18'] = test['num_over_18'].fillna(0)\n",
    "\n",
    "\n",
    "def extract_features(df):\n",
    "    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n",
    "    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n",
    "    df['tamhog_to_rooms'] = df['tamhog']/df['rooms']\n",
    "    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog']\n",
    "    df['r4t3_to_rooms'] = df['r4t3']/df['rooms']\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/df['r4t3']\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1'])\n",
    "    df['hhsize_to_rooms'] = df['hhsize']/df['rooms']\n",
    "    df['rent_to_hhsize'] = df['v2a1']/df['hhsize']\n",
    "    df['rent_to_over_18'] = df['v2a1']/df['num_over_18']\n",
    "\n",
    "    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n",
    "    \n",
    "extract_features(train)    \n",
    "extract_features(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:45:28.308194Z",
     "start_time": "2021-02-08T05:45:28.273286Z"
    }
   },
   "outputs": [],
   "source": [
    "needless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n",
    "                 'mobilephone', 'female', ]\n",
    "\n",
    "instlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n",
    "\n",
    "needless_cols.extend(instlevel_cols)\n",
    "\n",
    "train = train.drop(needless_cols, axis=1)\n",
    "test = test.drop(needless_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data\n",
    "동일한 가구에 속하는 행은 대개 동일한 대상을 가지므로, 누출을 방지하기 위해 가구별로 데이터를 분할합니다. 가장만 포함하도록 데이터를 필터링하기 때문에 기술적으로는 필요하지 않지만, 그렇게 하려면 전체 교육 데이터 세트를 쉽게 사용할 수 있습니다.\n",
    "\n",
    "데이터를 분할한 후에는 열차 데이터를 전체 데이터 세트로 덮어써 모든 데이터를 훈련할 수 있습니다. split_data 기능은 데이터를 덮어쓰지 않고 동일한 작업을 수행하며, 교육 루프 내에서 K-Fold 분할을 근사화하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:48:43.186337Z",
     "start_time": "2021-02-08T05:48:43.175722Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(train, y, sample_weight=None, households=None, test_percentage=0.20, seed=None):\n",
    "    \n",
    "    train2 = train.copy()\n",
    "    \n",
    "    cv_hhs = np.random.choice(households, size=int(len(households) * test_percentage), replace=False)\n",
    "    \n",
    "    cv_idx = np.isin(households, cv_hhs)\n",
    "    X_test = train2[cv_idx]\n",
    "    y_test = y[cv_idx]\n",
    "\n",
    "    X_train = train2[~cv_idx]\n",
    "    y_train = y[~cv_idx]\n",
    "    \n",
    "    if sample_weight is not None:\n",
    "        y_train_weights = sample_weight[~cv_idx]\n",
    "        return X_train, y_train, X_test, y_test, y_train_weights\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:50:29.512233Z",
     "start_time": "2021-02-08T05:50:29.450816Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train.query('parentesco1==1')\n",
    "\n",
    "y = X['Target'] - 1\n",
    "X = X.drop(['Target'], axis=1)\n",
    "\n",
    "np.random.seed(seed=None)\n",
    "\n",
    "train2 = X.copy()\n",
    "\n",
    "train_hhs = train2.idhogar\n",
    "\n",
    "households = train2.idhogar.unique()\n",
    "cv_hhs = np.random.choice(households, size=int(len(households) * 0.15), replace=False)\n",
    "\n",
    "cv_idx = np.isin(train2.idhogar, cv_hhs)\n",
    "\n",
    "X_test = train2[cv_idx]\n",
    "y_test = y[cv_idx]\n",
    "\n",
    "X_train = train2[~cv_idx]\n",
    "y_train = y[~cv_idx]\n",
    "\n",
    "X_train = train2\n",
    "y_train = y\n",
    "\n",
    "train_households = X_train.idhogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:53:36.250432Z",
     "start_time": "2021-02-08T05:53:36.244395Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_weights = class_weight.compute_sample_weight('balanced', y_train, indices=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:56:22.174864Z",
     "start_time": "2021-02-08T05:56:22.164916Z"
    }
   },
   "outputs": [],
   "source": [
    "extra_drop_features = [\n",
    " 'agg18_estadocivil1_MEAN',\n",
    " 'agg18_estadocivil6_COUNT',\n",
    " 'agg18_estadocivil7_COUNT',\n",
    " 'agg18_parentesco10_COUNT',\n",
    " 'agg18_parentesco11_COUNT',\n",
    " 'agg18_parentesco12_COUNT',\n",
    " 'agg18_parentesco1_COUNT',\n",
    " 'agg18_parentesco2_COUNT',\n",
    " 'agg18_parentesco3_COUNT',\n",
    " 'agg18_parentesco4_COUNT',\n",
    " 'agg18_parentesco5_COUNT',\n",
    " 'agg18_parentesco6_COUNT',\n",
    " 'agg18_parentesco7_COUNT',\n",
    " 'agg18_parentesco8_COUNT',\n",
    " 'agg18_parentesco9_COUNT',\n",
    " 'geo_elimbasu_LE_4',\n",
    " 'geo_energcocinar_LE_1',\n",
    " 'geo_energcocinar_LE_2',\n",
    " 'geo_epared_LE_0',\n",
    " 'geo_hogar_mayor',\n",
    " 'geo_manual_elec_LE_2',\n",
    " 'geo_pared_LE_3',\n",
    " 'geo_pared_LE_4',\n",
    " 'geo_pared_LE_5',\n",
    " 'geo_pared_LE_6',\n",
    " 'num_over_18',\n",
    " 'parentesco_LE',\n",
    " 'rez_esc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T05:56:38.293565Z",
     "start_time": "2021-02-08T05:56:38.288579Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_drop_cols = extra_drop_features + [\"idhogar\",  'parentesco1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a voting classifier\n",
    "조기 중지를 위해 fit_params를 전달할 수 있도록 파생된 VotingClassifier 클래스를 정의합니다. 매크로 F1에 따라 조기 중지되고 학습 속도가 저하되는 LGBM 모델을 기준으로 투표합니다.\n",
    "\n",
    "매개 변수는 이 커널에서 임의 검색을 통해 최적화됩니다. https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T06:02:03.687678Z",
     "start_time": "2021-02-08T06:02:03.677701Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_parameters = {'max_depth':35, 'eta':0.1, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 1, 'num_class': 4, 'gamma': 2.0, 'colsample_bylevel': 0.9, 'subsample': 0.84, 'colsample_bytree': 0.88, 'reg_lambda': 0.40 }\n",
    "opt_parameters = {'max_depth':35, 'eta':0.15, 'silent':1, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.5, 'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n",
    "# opt_parameters = {'max_depth':35, 'eta':0.15, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.75, 'colsample_bylevel': 0.95, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n",
    "# opt_parameters = {'max_depth':35, 'eta':0.12, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 3.25, 'colsample_bylevel': 0.95, 'subsample': 0.88, 'colsample_bytree': 0.88, 'reg_lambda': 0.35 }\n",
    "\n",
    "def evaluate_macroF1_lgb(predictions, truth):  \n",
    "    pred_labels = predictions.argmax(axis=1)\n",
    "    truth = truth.get_label()\n",
    "    f1 = f1_score(truth, pred_labels, average='macro')\n",
    "    return ('macroF1', 1-f1) \n",
    "\n",
    "fit_params={\"early_stopping_rounds\":500,\n",
    "            \"eval_metric\" : evaluate_macroF1_lgb, \n",
    "            \"eval_set\" : [(X_train,y_train), (X_test,y_test)],\n",
    "            'verbose': False,\n",
    "           }\n",
    "\n",
    "def learning_rate_power_0997(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    min_learning_rate = 0.02\n",
    "    lr = base_learning_rate  * np.power(.995, current_iter)\n",
    "    return max(lr, min_learning_rate)\n",
    "\n",
    "fit_params['verbose'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T06:08:28.369768Z",
     "start_time": "2021-02-08T06:08:28.333838Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "def _parallel_fit_estimator(estimator1, X, y, sample_weight=None, threshold=True, **fit_params):\n",
    "    estimator = clone(estimator1)\n",
    "\n",
    "    if sample_weight is not None:\n",
    "        X_train, y_train, X_test, y_test, y_train_weight = split_data(X, y, sample_weight, households=train_households)\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test = split_data(X, y, None, households=train_households)\n",
    "        \n",
    "    fit_params[\"eval_set\"] = [(X_test,y_test)]\n",
    "\n",
    "    if sample_weight is not None:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, sample_weight=y_train_weight, **fit_params)\n",
    "    else:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, **fit_params)\n",
    "    \n",
    "    if not isinstance(estimator1, ExtraTreesClassifier) and not isinstance(estimator1, RandomForestClassifier) and not isinstance(estimator1, xgb.XGBClassifier):\n",
    "        best_cv_round = np.argmax(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_cv = np.max(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_train = estimator.evals_result_['train']['macroF1'][best_cv_round]\n",
    "    else:\n",
    "        best_train = f1_score(y_train, estimator.predict(X_train), average=\"macro\")\n",
    "        best_cv = f1_score(y_test, estimator.predict(X_test), average=\"macro\")\n",
    "        print(\"Train F1:\", best_train)\n",
    "        print(\"Test F1:\", best_cv)\n",
    "        \n",
    "    if threshold:\n",
    "\n",
    "        if ((best_cv > 0.37) and (best_train > 0.75)) or ((best_cv > 0.44) and (best_train > 0.65)):\n",
    "            return estimator\n",
    "        \n",
    "        else:\n",
    "            print(\"Unacceptable!!! Trying again...\")\n",
    "            return _parallel_fit_estimator(estimator1, X, y, sample_weight=sample_weight, **fit_params)\n",
    "    \n",
    "    else:\n",
    "        return estimator\n",
    "    \n",
    "class VotingClassifierLGBM(VotingClassifier):\n",
    "    def fit(self, X, y, sample_weight=None, threshold=True, **fit_params):\n",
    "        \n",
    "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
    "            raise NotImplementedError('Multilabel and multi-output'\n",
    "                                      ' classification is not supported.')\n",
    "\n",
    "        if self.voting not in ('soft', 'hard'):\n",
    "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n",
    "                             % self.voting)\n",
    "\n",
    "        if self.estimators is None or len(self.estimators) == 0:\n",
    "            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n",
    "                                 ' should be a list of (string, estimator)'\n",
    "                                 ' tuples')\n",
    "\n",
    "        if (self.weights is not None and\n",
    "                len(self.weights) != len(self.estimators)):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                             '; got %d weights, %d estimators'\n",
    "                             % (len(self.weights), len(self.estimators)))\n",
    "\n",
    "        names, clfs = zip(*self.estimators)\n",
    "        self._validate_names(names)\n",
    "\n",
    "        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n",
    "        if n_isnone == len(self.estimators):\n",
    "            raise ValueError('All estimators are None. At least one is '\n",
    "                             'required to be a classifier!')\n",
    "\n",
    "        self.le_ = LabelEncoder().fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        self.estimators_ = []\n",
    "\n",
    "        transformed_y = self.le_.transform(y)\n",
    "\n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n",
    "                                                 sample_weight=sample_weight, threshold=threshold, **fit_params)\n",
    "                for clf in clfs if clf is not None)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T06:28:03.216089Z",
     "start_time": "2021-02-08T06:12:25.735289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-merror:0.464646\tvalidation_0-macroF1:0.646101\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.578445\n",
      "[100]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.570407\n",
      "[150]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.583402\n",
      "[200]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.583715\n",
      "[250]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.583333\n",
      "[299]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.583474\n",
      "Train F1: 0.9153725233385168\n",
      "Test F1: 0.4355385068644192\n",
      "[0]\tvalidation_0-merror:0.469697\tvalidation_0-macroF1:0.640044\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.583417\n",
      "[100]\tvalidation_0-merror:0.385522\tvalidation_0-macroF1:0.599753\n",
      "[150]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.593972\n",
      "[200]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.597668\n",
      "[250]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.605199\n",
      "[299]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.594076\n",
      "Train F1: 0.8913538399552563\n",
      "Test F1: 0.42422233276003724\n",
      "[0]\tvalidation_0-merror:0.473064\tvalidation_0-macroF1:0.676192\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.387205\tvalidation_0-macroF1:0.593712\n",
      "[100]\tvalidation_0-merror:0.387205\tvalidation_0-macroF1:0.595794\n",
      "[150]\tvalidation_0-merror:0.385522\tvalidation_0-macroF1:0.589911\n",
      "[200]\tvalidation_0-merror:0.383838\tvalidation_0-macroF1:0.589821\n",
      "[250]\tvalidation_0-merror:0.387205\tvalidation_0-macroF1:0.597476\n",
      "[299]\tvalidation_0-merror:0.388889\tvalidation_0-macroF1:0.596718\n",
      "Train F1: 0.8724441306732305\n",
      "Test F1: 0.42075548221923303\n",
      "[0]\tvalidation_0-merror:0.506734\tvalidation_0-macroF1:0.686421\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.356902\tvalidation_0-macroF1:0.531409\n",
      "[100]\tvalidation_0-merror:0.360269\tvalidation_0-macroF1:0.541944\n",
      "[150]\tvalidation_0-merror:0.361953\tvalidation_0-macroF1:0.543682\n",
      "[200]\tvalidation_0-merror:0.360269\tvalidation_0-macroF1:0.537668\n",
      "[250]\tvalidation_0-merror:0.356902\tvalidation_0-macroF1:0.530024\n",
      "[299]\tvalidation_0-merror:0.355219\tvalidation_0-macroF1:0.539738\n",
      "Train F1: 0.9054438067880621\n",
      "Test F1: 0.47823529311636404\n",
      "[0]\tvalidation_0-merror:0.488215\tvalidation_0-macroF1:0.690363\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.407407\tvalidation_0-macroF1:0.601575\n",
      "[100]\tvalidation_0-merror:0.390572\tvalidation_0-macroF1:0.591384\n",
      "[150]\tvalidation_0-merror:0.390572\tvalidation_0-macroF1:0.593051\n",
      "[200]\tvalidation_0-merror:0.395623\tvalidation_0-macroF1:0.601453\n",
      "[250]\tvalidation_0-merror:0.397306\tvalidation_0-macroF1:0.604831\n",
      "[299]\tvalidation_0-merror:0.397306\tvalidation_0-macroF1:0.609499\n",
      "Train F1: 0.9108878975200805\n",
      "Test F1: 0.4173477361936218\n",
      "[0]\tvalidation_0-merror:0.459596\tvalidation_0-macroF1:0.633155\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.402357\tvalidation_0-macroF1:0.617957\n",
      "[100]\tvalidation_0-merror:0.395623\tvalidation_0-macroF1:0.614103\n",
      "[150]\tvalidation_0-merror:0.390572\tvalidation_0-macroF1:0.604858\n",
      "[200]\tvalidation_0-merror:0.393939\tvalidation_0-macroF1:0.612738\n",
      "[250]\tvalidation_0-merror:0.388889\tvalidation_0-macroF1:0.609966\n",
      "[299]\tvalidation_0-merror:0.388889\tvalidation_0-macroF1:0.610004\n",
      "Train F1: 0.8893336491231894\n",
      "Test F1: 0.4051414966703659\n",
      "[0]\tvalidation_0-merror:0.464646\tvalidation_0-macroF1:0.667382\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.383838\tvalidation_0-macroF1:0.601972\n",
      "[100]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.605368\n",
      "[150]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.613276\n",
      "[200]\tvalidation_0-merror:0.387205\tvalidation_0-macroF1:0.620321\n",
      "[250]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.61699\n",
      "[299]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.614077\n",
      "Train F1: 0.8972810131834246\n",
      "Test F1: 0.40066108970156944\n",
      "[0]\tvalidation_0-merror:0.442761\tvalidation_0-macroF1:0.627544\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.604169\n",
      "[100]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.601541\n",
      "[150]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.604035\n",
      "[200]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.606346\n",
      "[250]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.612084\n",
      "[299]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.611192\n",
      "Train F1: 0.9110203375242469\n",
      "Test F1: 0.4069266211820104\n",
      "[0]\tvalidation_0-merror:0.405724\tvalidation_0-macroF1:0.569948\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.546907\n",
      "[100]\tvalidation_0-merror:0.36532\tvalidation_0-macroF1:0.55562\n",
      "[150]\tvalidation_0-merror:0.360269\tvalidation_0-macroF1:0.55303\n",
      "[200]\tvalidation_0-merror:0.350168\tvalidation_0-macroF1:0.545353\n",
      "[250]\tvalidation_0-merror:0.351852\tvalidation_0-macroF1:0.546489\n",
      "[299]\tvalidation_0-merror:0.351852\tvalidation_0-macroF1:0.547837\n",
      "Train F1: 0.9023161714314298\n",
      "Test F1: 0.4669456786718976\n",
      "[0]\tvalidation_0-merror:0.442761\tvalidation_0-macroF1:0.656075\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.60366\n",
      "[100]\tvalidation_0-merror:0.363636\tvalidation_0-macroF1:0.596851\n",
      "[150]\tvalidation_0-merror:0.360269\tvalidation_0-macroF1:0.594956\n",
      "[200]\tvalidation_0-merror:0.355219\tvalidation_0-macroF1:0.59503\n",
      "[250]\tvalidation_0-merror:0.358586\tvalidation_0-macroF1:0.601222\n",
      "[299]\tvalidation_0-merror:0.356902\tvalidation_0-macroF1:0.60041\n",
      "Train F1: 0.891328566030215\n",
      "Test F1: 0.4133092608446235\n",
      "[0]\tvalidation_0-merror:0.442761\tvalidation_0-macroF1:0.635594\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.598722\n",
      "[100]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.586422\n",
      "[150]\tvalidation_0-merror:0.363636\tvalidation_0-macroF1:0.572078\n",
      "[200]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.581793\n",
      "[250]\tvalidation_0-merror:0.363636\tvalidation_0-macroF1:0.572344\n",
      "[299]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.580221\n",
      "Train F1: 0.9200869829778409\n",
      "Test F1: 0.42932198053495463\n",
      "[0]\tvalidation_0-merror:0.447811\tvalidation_0-macroF1:0.643927\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.591184\n",
      "[100]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.604466\n",
      "[150]\tvalidation_0-merror:0.383838\tvalidation_0-macroF1:0.613042\n",
      "[200]\tvalidation_0-merror:0.378788\tvalidation_0-macroF1:0.611924\n",
      "[250]\tvalidation_0-merror:0.382155\tvalidation_0-macroF1:0.609384\n",
      "[299]\tvalidation_0-merror:0.383838\tvalidation_0-macroF1:0.618041\n",
      "Train F1: 0.9006803320768149\n",
      "Test F1: 0.4254007528480266\n",
      "[0]\tvalidation_0-merror:0.442761\tvalidation_0-macroF1:0.615498\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.563908\n",
      "[100]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.561588\n",
      "[150]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.56399\n",
      "[200]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.564721\n",
      "[250]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.569799\n",
      "[299]\tvalidation_0-merror:0.372054\tvalidation_0-macroF1:0.565604\n",
      "Train F1: 0.868783685054759\n",
      "Test F1: 0.45728526150302823\n",
      "[0]\tvalidation_0-merror:0.442761\tvalidation_0-macroF1:0.599582\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.39899\tvalidation_0-macroF1:0.590061\n",
      "[100]\tvalidation_0-merror:0.40404\tvalidation_0-macroF1:0.60687\n",
      "[150]\tvalidation_0-merror:0.400673\tvalidation_0-macroF1:0.600754\n",
      "[200]\tvalidation_0-merror:0.392256\tvalidation_0-macroF1:0.597934\n",
      "[250]\tvalidation_0-merror:0.395623\tvalidation_0-macroF1:0.604446\n",
      "[299]\tvalidation_0-merror:0.393939\tvalidation_0-macroF1:0.601292\n",
      "Train F1: 0.8967214529450815\n",
      "Test F1: 0.4262237902576739\n",
      "[0]\tvalidation_0-merror:0.441077\tvalidation_0-macroF1:0.635276\n",
      "Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n",
      "[50]\tvalidation_0-merror:0.348485\tvalidation_0-macroF1:0.564666\n",
      "[100]\tvalidation_0-merror:0.350168\tvalidation_0-macroF1:0.565395\n",
      "[150]\tvalidation_0-merror:0.351852\tvalidation_0-macroF1:0.569924\n",
      "[200]\tvalidation_0-merror:0.348485\tvalidation_0-macroF1:0.566517\n",
      "[250]\tvalidation_0-merror:0.348485\tvalidation_0-macroF1:0.571177\n",
      "[299]\tvalidation_0-merror:0.348485\tvalidation_0-macroF1:0.566762\n",
      "Train F1: 0.8935956630026125\n",
      "Test F1: 0.4472704708236759\n"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "for i in range(15):\n",
    "    clf = xgb.XGBClassifier(random_state=217+i, n_estimators=300, learning_rate=0.15, n_jobs=4, **opt_parameters)\n",
    "    \n",
    "    clfs.append(('xgb{}'.format(i), clf))\n",
    "    \n",
    "vc = VotingClassifierLGBM(clfs, voting='soft')\n",
    "del(clfs)\n",
    "\n",
    "_ = vc.fit(X_train.drop(xgb_drop_cols, axis=1), y_train, sample_weight=y_train_weights, threshold=False, **fit_params)\n",
    "\n",
    "clf_final = vc.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T06:28:03.428095Z",
     "start_time": "2021-02-08T06:28:03.218130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a single LGBM Classifier: 0.8085\n",
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.9183\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.9162\n"
     ]
    }
   ],
   "source": [
    "global_score = f1_score(y_test, clf_final.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "vc.voting = 'soft'\n",
    "global_score_soft = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "vc.voting = 'hard'\n",
    "global_score_hard = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "\n",
    "print('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T06:28:04.866190Z",
     "start_time": "2021-02-08T06:28:03.430093Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agg18_estadocivil4_COUNT',\n",
       " 'agg18_estadocivil5_COUNT',\n",
       " 'geo_energcocinar_LE_0',\n",
       " 'geo_epared_LE_2',\n",
       " 'geo_manual_elec_LE_3'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useless_features = []\n",
    "drop_features = set()\n",
    "counter = 0\n",
    "for est in vc.estimators_:\n",
    "    ranked_features, unused_features = feature_importance(est, X_train.drop(xgb_drop_cols, axis=1), display_results=False)\n",
    "    useless_features.append(unused_features)\n",
    "    if counter == 0:\n",
    "        drop_features = set(unused_features)\n",
    "    else:\n",
    "        drop_features = drop_features.intersection(set(unused_features))\n",
    "    counter += 1\n",
    "    \n",
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T06:28:04.995915Z",
     "start_time": "2021-02-08T06:28:04.869146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 59 (0.020874) - agg18_escolari_MAX\n",
      "2. feature 42 (0.019113) - fe_children_fraction\n",
      "3. feature 37 (0.017560) - SQBedjefe\n",
      "4. feature 126 (0.016810) - geo_sanitario_LE_3\n",
      "5. feature 74 (0.015333) - agg18_parentesco2_MEAN\n",
      "6. feature 46 (0.014168) - fe_human_bed_density\n",
      "7. feature 22 (0.013392) - dependency\n",
      "8. feature 60 (0.013138) - agg18_escolari_MEAN\n",
      "9. feature 40 (0.012836) - SQBdependency\n",
      "10. feature 135 (0.012545) - geo_pared_LE_7\n",
      "11. feature 112 (0.012330) - geo_etecho_LE_1\n",
      "12. feature 107 (0.011216) - geo_overcrowding\n",
      "13. feature 124 (0.010565) - geo_sanitario_LE_1\n",
      "14. feature 116 (0.010347) - geo_elimbasu_LE_0\n",
      "15. feature 19 (0.010287) - hogar_adul\n",
      "16. feature 34 (0.010142) - SQBescolari\n",
      "17. feature 105 (0.009962) - geo_hogar_total\n",
      "18. feature 23 (0.009942) - edjefe\n",
      "19. feature 94 (0.009886) - etecho_LE\n",
      "20. feature 17 (0.009594) - male\n",
      "21. feature 39 (0.009528) - SQBovercrowding\n",
      "22. feature 41 (0.009366) - SQBmeaned\n",
      "23. feature 44 (0.009305) - fe_all_man_fraction\n",
      "24. feature 96 (0.009216) - estadocivil_LE\n",
      "25. feature 6 (0.009131) - r4h1\n",
      "26. feature 97 (0.009013) - lugar_LE\n",
      "27. feature 65 (0.008972) - agg18_estadocivil3_MEAN\n",
      "28. feature 15 (0.008932) - cielorazo\n",
      "29. feature 16 (0.008922) - dis\n",
      "30. feature 120 (0.008912) - geo_elimbasu_LE_5\n",
      "31. feature 93 (0.008863) - epared_LE\n",
      "32. feature 106 (0.008822) - geo_bedrooms\n",
      "33. feature 104 (0.008821) - geo_hogar_adul\n",
      "34. feature 95 (0.008810) - eviv_LE\n",
      "35. feature 98 (0.008801) - tipovivi_LE\n",
      "36. feature 55 (0.008797) - agg18_age_MIN\n",
      "37. feature 12 (0.008775) - r4t1\n",
      "38. feature 92 (0.008711) - elimbasu_LE\n",
      "39. feature 131 (0.008691) - geo_manual_elec_LE_4\n",
      "40. feature 49 (0.008591) - fe_mobile_density\n",
      "41. feature 35 (0.008552) - SQBage\n",
      "42. feature 111 (0.008547) - geo_etecho_LE_0\n",
      "43. feature 27 (0.008483) - overcrowding\n",
      "44. feature 125 (0.008438) - geo_sanitario_LE_2\n",
      "45. feature 14 (0.008432) - escolari\n",
      "46. feature 5 (0.008364) - v18q1\n",
      "47. feature 33 (0.008358) - age\n",
      "48. feature 13 (0.008337) - r4t2\n",
      "49. feature 87 (0.008315) - piso_LE\n",
      "50. feature 86 (0.008305) - pared_LE\n",
      "51. feature 117 (0.008305) - geo_elimbasu_LE_1\n",
      "52. feature 25 (0.008275) - meaneduc\n",
      "53. feature 123 (0.008270) - geo_sanitario_LE_0\n",
      "54. feature 51 (0.008260) - fe_mobile_adult_density\n",
      "55. feature 69 (0.008140) - agg18_estadocivil5_MEAN\n",
      "56. feature 71 (0.008139) - agg18_estadocivil6_MEAN\n",
      "57. feature 72 (0.008112) - agg18_estadocivil7_MEAN\n",
      "58. feature 45 (0.008103) - fe_human_density\n",
      "59. feature 30 (0.008054) - qmobilephone\n",
      "60. feature 58 (0.008048) - agg18_escolari_MIN\n",
      "61. feature 43 (0.008048) - fe_working_man_fraction\n",
      "62. feature 102 (0.007966) - geo_dependency\n",
      "63. feature 0 (0.007906) - v2a1\n",
      "64. feature 128 (0.007870) - geo_manual_elec_LE_0\n",
      "65. feature 10 (0.007865) - r4m2\n",
      "66. feature 32 (0.007738) - area2\n",
      "67. feature 7 (0.007579) - r4h2\n",
      "68. feature 21 (0.007507) - hogar_total\n",
      "69. feature 109 (0.007466) - geo_eviv_LE_1\n",
      "70. feature 47 (0.007323) - fe_rent_per_person\n",
      "71. feature 142 (0.007268) - hhsize_to_rooms\n",
      "72. feature 63 (0.007225) - agg18_estadocivil2_MEAN\n",
      "73. feature 136 (0.007217) - bedrooms_to_rooms\n",
      "74. feature 1 (0.007192) - hacdor\n",
      "75. feature 138 (0.007158) - tamhog_to_rooms\n",
      "76. feature 50 (0.007148) - fe_tablet_density\n",
      "77. feature 2 (0.007079) - rooms\n",
      "78. feature 29 (0.007009) - television\n",
      "79. feature 91 (0.007006) - energcocinar_LE\n",
      "80. feature 99 (0.007003) - manual_elec_LE\n",
      "81. feature 57 (0.006932) - agg18_age_MEAN\n",
      "82. feature 62 (0.006899) - agg18_estadocivil1_COUNT\n",
      "83. feature 90 (0.006889) - sanitario_LE\n",
      "84. feature 100 (0.006777) - geo_age\n",
      "85. feature 31 (0.006771) - area1\n",
      "86. feature 61 (0.006766) - agg18_dis_MEAN\n",
      "87. feature 24 (0.006738) - edjefa\n",
      "88. feature 8 (0.006734) - r4h3\n",
      "89. feature 52 (0.006694) - fe_tablet_adult_density\n",
      "90. feature 143 (0.006689) - rent_to_hhsize\n",
      "91. feature 11 (0.006668) - r4m3\n",
      "92. feature 122 (0.006643) - geo_energcocinar_LE_3\n",
      "93. feature 48 (0.006560) - fe_rent_per_room\n",
      "94. feature 64 (0.006504) - agg18_estadocivil2_COUNT\n",
      "95. feature 56 (0.006441) - agg18_age_MAX\n",
      "96. feature 20 (0.006432) - hogar_mayor\n",
      "97. feature 26 (0.006391) - bedrooms\n",
      "98. feature 9 (0.006350) - r4m1\n",
      "99. feature 75 (0.006242) - agg18_parentesco3_MEAN\n",
      "100. feature 4 (0.006161) - refrig\n",
      "101. feature 18 (0.005876) - hogar_nin\n",
      "102. feature 101 (0.005755) - geo_meaneduc\n",
      "103. feature 38 (0.005725) - SQBhogar_nin\n",
      "104. feature 103 (0.005689) - geo_hogar_nin\n",
      "105. feature 28 (0.005391) - computer\n",
      "106. feature 140 (0.005380) - r4t3_to_rooms\n",
      "107. feature 88 (0.005258) - techo_LE\n",
      "108. feature 144 (0.005221) - rent_to_over_18\n",
      "109. feature 67 (0.005210) - agg18_estadocivil4_MEAN\n",
      "110. feature 110 (0.005132) - geo_eviv_LE_2\n",
      "111. feature 129 (0.005069) - geo_manual_elec_LE_1\n",
      "112. feature 53 (0.005035) - fe_people_not_living\n",
      "113. feature 79 (0.004939) - agg18_parentesco7_MEAN\n",
      "114. feature 77 (0.004880) - agg18_parentesco5_MEAN\n",
      "115. feature 137 (0.004457) - rent_to_rooms\n",
      "116. feature 114 (0.004299) - geo_epared_LE_1\n",
      "117. feature 81 (0.004282) - agg18_parentesco9_MEAN\n",
      "118. feature 141 (0.004248) - v2a1_to_r4t3\n",
      "119. feature 83 (0.004106) - agg18_parentesco11_MEAN\n",
      "120. feature 133 (0.003824) - geo_pared_LE_1\n",
      "121. feature 3 (0.003191) - hacapo\n",
      "122. feature 78 (0.002986) - agg18_parentesco6_MEAN\n",
      "123. feature 84 (0.002735) - agg18_parentesco12_MEAN\n",
      "124. feature 89 (0.002506) - abastagua_LE\n",
      "125. feature 139 (0.001633) - r4t3_to_tamhog\n",
      "126. feature 76 (0.001460) - agg18_parentesco4_MEAN\n",
      "127. feature 80 (0.000000) - agg18_parentesco8_MEAN\n",
      "128. feature 132 (0.000000) - geo_pared_LE_0\n",
      "129. feature 119 (0.000000) - geo_elimbasu_LE_3\n",
      "130. feature 36 (0.000000) - SQBhogar_total\n",
      "131. feature 82 (0.000000) - agg18_parentesco10_MEAN\n",
      "132. feature 73 (0.000000) - agg18_parentesco1_MEAN\n",
      "133. feature 118 (0.000000) - geo_elimbasu_LE_2\n",
      "134. feature 130 (0.000000) - geo_manual_elec_LE_3\n",
      "135. feature 70 (0.000000) - agg18_estadocivil5_COUNT\n",
      "136. feature 68 (0.000000) - agg18_estadocivil4_COUNT\n",
      "137. feature 115 (0.000000) - geo_epared_LE_2\n",
      "138. feature 66 (0.000000) - agg18_estadocivil3_COUNT\n",
      "139. feature 85 (0.000000) - edjef\n",
      "140. feature 121 (0.000000) - geo_energcocinar_LE_0\n",
      "141. feature 54 (0.000000) - fe_people_weird_stat\n",
      "142. feature 108 (0.000000) - geo_eviv_LE_0\n",
      "143. feature 134 (0.000000) - geo_pared_LE_2\n",
      "144. feature 113 (0.000000) - geo_etecho_LE_2\n",
      "145. feature 127 (0.000000) - geo_sanitario_LE_4\n"
     ]
    }
   ],
   "source": [
    "ranked_features = feature_importance(clf_final, X_train.drop(xgb_drop_cols, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T06:31:21.845366Z",
     "start_time": "2021-02-08T06:31:21.834393Z"
    }
   },
   "outputs": [],
   "source": [
    "et_drop_cols = ['agg18_age_MAX', 'agg18_age_MEAN', 'agg18_age_MIN', 'agg18_dis_MEAN',\n",
    "       'agg18_escolari_MAX', 'agg18_escolari_MEAN', 'agg18_escolari_MIN',\n",
    "       'agg18_estadocivil1_COUNT', 'agg18_estadocivil1_MEAN',\n",
    "       'agg18_estadocivil2_COUNT', 'agg18_estadocivil2_MEAN',\n",
    "       'agg18_estadocivil3_COUNT', 'agg18_estadocivil3_MEAN',\n",
    "       'agg18_estadocivil4_COUNT', 'agg18_estadocivil4_MEAN',\n",
    "       'agg18_estadocivil5_COUNT', 'agg18_estadocivil5_MEAN',\n",
    "       'agg18_estadocivil6_COUNT', 'agg18_estadocivil6_MEAN',\n",
    "       'agg18_estadocivil7_COUNT', 'agg18_estadocivil7_MEAN',\n",
    "       'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN',\n",
    "       'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN',\n",
    "       'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN',\n",
    "       'agg18_parentesco1_COUNT', 'agg18_parentesco1_MEAN',\n",
    "       'agg18_parentesco2_COUNT', 'agg18_parentesco2_MEAN',\n",
    "       'agg18_parentesco3_COUNT', 'agg18_parentesco3_MEAN',\n",
    "       'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN',\n",
    "       'agg18_parentesco5_COUNT', 'agg18_parentesco5_MEAN',\n",
    "       'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN',\n",
    "       'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN',\n",
    "       'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN',\n",
    "       'agg18_parentesco9_COUNT', 'agg18_parentesco9_MEAN'] #+ ['parentesco_LE', 'rez_esc']\n",
    "\n",
    "et_drop_cols.extend([\"idhogar\", \"parentesco1\", 'fe_rent_per_person', 'fe_rent_per_room',\n",
    "       'fe_tablet_adult_density', 'fe_tablet_density'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T06:37:01.065258Z",
     "start_time": "2021-02-08T06:36:35.097885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.8983848211561081\n",
      "Test F1: 0.4292190828609306\n",
      "Train F1: 0.8982569720976492\n",
      "Test F1: 0.4169636759882981\n",
      "Train F1: 0.8953245057366976\n",
      "Test F1: 0.3959736015904715\n",
      "Train F1: 0.8904009934948478\n",
      "Test F1: 0.4140651577809969\n",
      "Train F1: 0.8922849683370566\n",
      "Test F1: 0.4664528827812809\n",
      "Train F1: 0.895366590258067\n",
      "Test F1: 0.4334129309568262\n",
      "Train F1: 0.8964130336967895\n",
      "Test F1: 0.42637991299318534\n",
      "Train F1: 0.8978795660860878\n",
      "Test F1: 0.4619190301651779\n",
      "Train F1: 0.9041960041265171\n",
      "Test F1: 0.427861786122561\n",
      "Train F1: 0.8937864046617127\n",
      "Test F1: 0.4137717184547513\n"
     ]
    }
   ],
   "source": [
    "ets = []    \n",
    "for i in range(10):\n",
    "    rf = RandomForestClassifier(max_depth=None, random_state=217+i, n_jobs=4, n_estimators=700, min_impurity_decrease=1e-3, min_samples_leaf=2, verbose=0, class_weight=\"balanced\")\n",
    "    ets.append(('rf{}'.format(i), rf))   \n",
    "\n",
    "vc2 = VotingClassifierLGBM(ets, voting='soft')    \n",
    "_ = vc2.fit(X_train.drop(et_drop_cols, axis=1), y_train, threshold=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T06:39:54.590205Z",
     "start_time": "2021-02-08T06:39:50.308024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8669\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.9069\n"
     ]
    }
   ],
   "source": [
    "vc2.voting = 'soft'\n",
    "global_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "vc2.voting = 'hard'\n",
    "global_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T06:48:00.423443Z",
     "start_time": "2021-02-08T06:47:59.045602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parentesco_LE', 'rez_esc'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useless_features = []\n",
    "drop_features = set()\n",
    "counter = 0\n",
    "for est in vc2.estimators_:\n",
    "    ranked_features, unused_features = feature_importance(est, X_train.drop(et_drop_cols, axis=1), display_results=False)\n",
    "    useless_features.append(unused_features)\n",
    "    if counter == 0:\n",
    "        drop_features = set(unused_features)\n",
    "    else:\n",
    "        drop_features = drop_features.intersection(set(unused_features))\n",
    "    counter += 1\n",
    "    \n",
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T06:50:26.471334Z",
     "start_time": "2021-02-08T06:50:26.464340Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine_voters(data, weights=[0.5, 0.5]):\n",
    "    vc.voting=\"soft\"\n",
    "    vc1_probs = vc.predict_proba(data.drop(xgb_drop_cols, axis=1))\n",
    "    vc2.voting=\"soft\"\n",
    "    vc2_probs = vc2.predict_proba(data.drop(et_drop_cols, axis=1))\n",
    "    \n",
    "    final_vote = (vc1_probs * weights[0]) + (vc2_probs * weights[1])\n",
    "    predictions = np.argmax(final_vote, axis=1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T06:50:46.773340Z",
     "start_time": "2021-02-08T06:50:44.549079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9104753693794789"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.5, 0.5])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T06:50:57.447863Z",
     "start_time": "2021-02-08T06:50:55.297193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9178777337902784"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.6, 0.4])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T06:51:04.853215Z",
     "start_time": "2021-02-08T06:51:02.597899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9092861305361306"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.4, 0.6])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T06:54:08.880085Z",
     "start_time": "2021-02-08T06:54:08.838163Z"
    }
   },
   "outputs": [],
   "source": [
    "y_subm = pd.DataFrame()\n",
    "y_subm['Id'] = test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T07:14:12.691848Z",
     "start_time": "2021-02-08T07:13:47.819982Z"
    }
   },
   "outputs": [],
   "source": [
    "vc.voting = 'soft'\n",
    "y_subm_lgb = y_subm.copy(deep=True)\n",
    "y_subm_lgb['Target'] = vc.predict(test.drop(xgb_drop_cols, axis=1)) + 1\n",
    "\n",
    "vc2.voting = 'soft'\n",
    "y_subm_rf = y_subm.copy(deep=True)\n",
    "y_subm_rf['Target'] = vc2.predict(test.drop(et_drop_cols, axis=1)) + 1\n",
    "\n",
    "y_subm_ens = y_subm.copy(deep=True)\n",
    "y_subm_ens['Target'] = combine_voters(test) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T07:18:37.510908Z",
     "start_time": "2021-02-08T07:18:37.319425Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "sub_file_lgb = 'submission_soft_XGB_{:.4f}_{}.csv'.format(global_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "sub_file_rf = 'submission_soft_RF_{:.4f}_{}.csv'.format(global_rf_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "sub_file_ens = 'submission_ens_{:.4f}_{}.csv'.format(global_combo_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "\n",
    "y_subm_lgb.to_csv(sub_file_lgb, index=False)\n",
    "y_subm_rf.to_csv(sub_file_rf, index=False)\n",
    "y_subm_ens.to_csv(sub_file_ens, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
