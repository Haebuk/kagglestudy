{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "illegal-personal",
   "metadata": {},
   "source": [
    "# Costa Rican Household Poverty Level Prediction - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-telescope",
   "metadata": {},
   "source": [
    "참고 : https://www.kaggle.com/skooch/xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-sector",
   "metadata": {},
   "source": [
    "# 1st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-difficulty",
   "metadata": {},
   "source": [
    "# LGMB with random split for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-december",
   "metadata": {},
   "source": [
    "- 이 커널은 (가구의 집계를 추출한 후) 가정의 가장만 학습시킨다. 점수는 가정의 가장으로만 평가된다. 모든 가족구성원은 test + sample submission에 포함되지만, 가정만 점수가 매겨진다. 하지만, 현재 평가는 가장이 아닌 가족구성원에게도 이루어지는 것으로 보인다.\n",
    "- 클래스 빈도의 균형을 맞추는 것은 아주 중요하다. 균형을 맞추지 않으면 학습된 모델은 성능이 낮다. 이 작업은 직접 할 수도 있고, 언더샘플링을 할 수도 있다. 하지만 가장 간단한 것은 (그리고 언더샘플링보다 더 강력한 것은) sklearn API의 LightGBM모델 생성시 class_weight='balanced'로 설정하는 것이다.\n",
    "- 이 커널은 macro F1 score를 사용해 학습을 조기중단한다. 이 작업은 scoring 전략에 맞춰 수행한다.\n",
    "- 범주형 변수는 blind label encoding 대신 적절히 매핑된 숫자로 변환된다.\n",
    "- OHE(One-Hot Encoding)가 label encoding되면, 트리 모델에서 digest하기 더 쉽다. 이 트릭은 트리모델이 아닌 모델에 안 좋을 수 있으니 조심해야한다.\n",
    "- idhogar은 학습에 사용되지 않는다. 정보를 얻는 유일한 방법은 데이터 누락이 없는 경우다. \n",
    "- 가정 내에서 집계가 이루어지며 새로운 변수를 직접 생성한다. 대부분의 변수는 가정 수준에서 이미 사용되기 때문에 집계될 수 있는 변수가 많지 않다.\n",
    "- voting classifier은 여러 LgithGBM 모델의 평균을 내는 데 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "congressional-belle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:55.242615Z",
     "start_time": "2021-02-08T18:24:47.492732Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np   # linear algebra\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "#from sklearn.externals.joblib import Parallel, delayed\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-greeting",
   "metadata": {},
   "source": [
    "범주형 변수 매핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "structured-newsletter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:55.258573Z",
     "start_time": "2021-02-08T18:24:55.245608Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 여기선 idhogar 필드만 변환. 이 함수는 다른 데에서도 사용\n",
    "def encode_data(df):\n",
    "    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n",
    "    \n",
    "# sklearn의 의사결정나무를 위한 변수 중요도 plot\n",
    "def feature_importance(forest, X_train, display_results=True):\n",
    "    ranked_list = []\n",
    "    zero_features = []\n",
    "    \n",
    "    importances = forest.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    if display_results:\n",
    "        # 변수 순위 출력\n",
    "        print(\"Feature ranking :\")\n",
    "    \n",
    "    for f in range(X_train.shape[1]):\n",
    "        if display_results:\n",
    "            print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]) \n",
    "                  + \" - \" + X_train.columns[indices[f]])\n",
    "        ranked_list.append(X_train.columns[indices[f]])\n",
    "        \n",
    "        if importances[indices[f]] == 0.0:\n",
    "            zero_features.append(X_train.columns[indices[f]])\n",
    "            \n",
    "    return ranked_list, zero_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "catholic-colorado",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:55.290489Z",
     "start_time": "2021-02-08T18:24:55.263560Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_features(df):\n",
    "    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n",
    "                 ('working_man_fraction', 'r4h2', 'r4t3'),\n",
    "                 ('all_man_fraction', 'r4h3', 'r4t3'),\n",
    "                 ('human_density', 'tamviv', 'rooms'),\n",
    "                 ('human_bed_density', 'tamviv', 'bedrooms'),\n",
    "                 ('rent_per_person', 'v2a1', 'r4t3'),\n",
    "                 ('rent_per_room', 'v2a1', 'rooms'),\n",
    "                 ('mobile_density', 'qmobilephone', 'r4t3'),\n",
    "                 ('tablet_density', 'v18q1', 'r4t3'),\n",
    "                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n",
    "                 ('tablet_adult_density', 'v18q1', 'r4t2')]\n",
    "    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n",
    "                 ('people_weird_stat', 'tamhog', 'r4t3')]\n",
    "    \n",
    "    for f_new, f1, f2 in feats_div:\n",
    "        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n",
    "    for f_new, f1, f2 in feats_sub:\n",
    "        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n",
    "        \n",
    "    # 가정으로 집계 규칙\n",
    "    aggs_num = {'age': ['min', 'max', 'mean'], 'escolari': ['min', 'max', 'mean']}\n",
    "    aggs_cat = {'dis': ['mean']}\n",
    "    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n",
    "        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n",
    "            aggs_cat[f_] = ['mean', 'count']\n",
    "            \n",
    "    # 가장으로 집계\n",
    "    for name_, df_ in [('18', df.query('age >= 18'))]:\n",
    "        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n",
    "        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n",
    "        df = df.join(df_agg, how='left', on='idhogar')\n",
    "        del df_agg\n",
    "        \n",
    "    # id 삭제\n",
    "    df.drop(['Id'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "chinese-paragraph",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:55.321405Z",
     "start_time": "2021-02-08T18:24:55.293481Z"
    }
   },
   "outputs": [],
   "source": [
    "# one hot encoding된 필드를 laebl encoding으로 변경\n",
    "def convert_OHE2LE(df):\n",
    "    tmp_df = df.copy(deep=True)\n",
    "    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n",
    "               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', 'instlevel', 'lugar', 'tipovivi',\n",
    "               'manual_elec']:\n",
    "        if 'manual_' not in s_:\n",
    "            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n",
    "        elif 'elec' in s_:\n",
    "            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n",
    "        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "        \n",
    "        # columns==0인 합계가 있는 OHE 열을 처리\n",
    "        if 0 in sum_ohe:\n",
    "            print('The OHE in {} is incomplete. A new column will be added before label encoding'.format(s_))\n",
    "            # 더미변수 이름 추가\n",
    "            col_dummy = s_+'_dummy'\n",
    "            # 데이터프레임에 열 추가\n",
    "            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n",
    "            # label encoding 위해 열 리스트에 이름 추가\n",
    "            cols_s_.append(col_dummy)\n",
    "            # 범주 인코딩이 완료된 것을 증명\n",
    "            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "            if 0 in sum_ohe:\n",
    "                print('The category completion did not work')\n",
    "                \n",
    "        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n",
    "        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n",
    "        if 'parentesco1' in cols_s_:\n",
    "            cols_s_.remove('parentesco1')\n",
    "        tmp_df.drop(cols_s_, axis=1, inplace=True)\n",
    "        \n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-steal",
   "metadata": {},
   "source": [
    "# Read in the data and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "phantom-virus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:55.764404Z",
     "start_time": "2021-02-08T18:24:55.324398Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "test_ids = test.Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "above-maintenance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:56.144624Z",
     "start_time": "2021-02-08T18:24:55.767331Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_df(df_):\n",
    "    # idhogar 인코딩\n",
    "    encode_data(df_)\n",
    "    # 집계 변수 생성\n",
    "    return do_features(df_)\n",
    "\n",
    "train = process_df(train)\n",
    "test = process_df(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-hybrid",
   "metadata": {},
   "source": [
    "결측 데이터를 처리하고 object를 numeric으로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adult-publication",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:56.237727Z",
     "start_time": "2021-02-08T18:24:56.146970Z"
    }
   },
   "outputs": [],
   "source": [
    "# dependency는 Na값이 있으므로 제곱근으로 대체\n",
    "train['dependency'] = np.sqrt(train['SQBdependency'])\n",
    "test['dependency'] = np.sqrt(test['SQBdependency'])\n",
    "\n",
    "# education의 no를 0으로 대체\n",
    "train.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0\n",
    "train.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0\n",
    "test.loc[test['edjefa'] == \"no\", \"edjefa\"] = 0\n",
    "test.loc[test['edjefe'] == \"no\", \"edjefe\"] = 0\n",
    "\n",
    "# education이 yes고 가장이면 escolari로 대체\n",
    "train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"edjefa\"] = train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"edjefe\"] = train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"edjefa\"] = test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"edjefe\"] = test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "\n",
    "# 이 필드는 gender와 escolari 사이의 상호작용이어야하지만, 'yes'의 의미가 분명하지 않다. 4로 대체한다.\n",
    "train.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
    "train.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
    "test.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
    "test.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
    "\n",
    "# 모델을 위해 int로 변환\n",
    "train['edjefe'] = train['edjefe'].astype(\"int\")\n",
    "train['edjefa'] = train['edjefa'].astype(\"int\")\n",
    "test['edjefe'] = test['edjefe'].astype(\"int\")\n",
    "test['edjefa'] = test['edjefa'].astype(\"int\")\n",
    "\n",
    "# 가장의 교육 최대값 변수 생성\n",
    "train['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\n",
    "test['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n",
    "\n",
    "# 일부 na 대체\n",
    "train['v2a1'] = train['v2a1'].fillna(0)\n",
    "test['v2a1'] = test['v2a1'].fillna(0)\n",
    "train['v18q1'] = train['v18q1'].fillna(0)\n",
    "test['v18q1'] = test['v18q1'].fillna(0)\n",
    "train['rez_esc']=train['rez_esc'].fillna(0)\n",
    "test['rez_esc']=test['rez_esc'].fillna(0)\n",
    "\n",
    "train.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "train.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "test.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "test.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "\n",
    "# 데이터 불일치 수정 - 일부 행은 가정에 화장실이 있는지 없는지를 나타냄\n",
    "# water가 없으면 물 마시지 않는다고 가정\n",
    "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\n",
    "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n",
    "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\n",
    "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lightweight-transparency",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:56.253684Z",
     "start_time": "2021-02-08T18:24:56.239722Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_apply_func(train_, test_, func_):\n",
    "    test_['Target'] = 0\n",
    "    xx = pd.concat([train_, test_])\n",
    "\n",
    "    xx_func = func_(xx)\n",
    "    train_ = xx_func.iloc[:train_.shape[0], :]\n",
    "    test_ = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n",
    "    \n",
    "    del xx, xx_func\n",
    "    return train_, test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "manufactured-indianapolis",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:57.477301Z",
     "start_time": "2021-02-08T18:24:56.259668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OHE in techo is incomplete. A new column will be added before label encoding\n",
      "The OHE in instlevel is incomplete. A new column will be added before label encoding\n",
      "The OHE in manual_elec is incomplete. A new column will be added before label encoding\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_apply_func(train, test, convert_OHE2LE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-telephone",
   "metadata": {},
   "source": [
    "# Geo aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hindu-representation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:57.680170Z",
     "start_time": "2021-02-08T18:24:57.480677Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n",
    "              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE', 'pared_LE']\n",
    "cols_nums = ['age', 'meaneduc', 'dependency', 'hogar_nin', 'hogar_adul', \n",
    "             'hogar_mayor', 'hogar_total', 'bedrooms', 'overcrowding']\n",
    "\n",
    "def convert_geo2aggs(df_):\n",
    "    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n",
    "                        pd.get_dummies(df_[cols_2_ohe], columns=cols_2_ohe)],axis=1)\n",
    "    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n",
    "    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n",
    "    \n",
    "    del tmp_df\n",
    "    return df_.join(geo_agg, how='left', on='lugar_LE')\n",
    "\n",
    "# geography 집계 추가\n",
    "train, test = train_test_apply_func(train, test, convert_geo2aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "welcome-tulsa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:57.992974Z",
     "start_time": "2021-02-08T18:24:57.682172Z"
    }
   },
   "outputs": [],
   "source": [
    "# 각 가정의 18세 이상 사람 수 추가\n",
    "train['num_over_18'] = 0\n",
    "train['num_over_18'] = train[train.age>=18].groupby('idhogar').transform(\"count\")\n",
    "train['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "train['num_over_18'] = train['num_over_18'].fillna(0)\n",
    "\n",
    "test['num_over_18'] = 0\n",
    "test['num_over_18'] = test[test.age>=18].groupby('idhogar').transform(\"count\")\n",
    "test['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "test['num_over_18'] = test['num_over_18'].fillna(0)\n",
    "\n",
    "\n",
    "# 변수 추가\n",
    "def extract_features(df):\n",
    "    df['bedrooms_to_rooms'] = df['bedrooms'] / df['rooms']\n",
    "    df['rent_to_rooms'] = df['v2a1'] / df['rooms']\n",
    "    df['tamhog_to_rooms'] = df['tamhog'] / df['rooms']  # tamhog : 가족규모\n",
    "    df['r4t3_to_tamhog'] = df['r4t3'] / df['tamhog'] # r4t3 : 가정 총 인원수\n",
    "    df['r4t3_to_rooms'] = df['r4t3'] / df['rooms'] \n",
    "    df['v2a1_to_r4t3'] = df['v2a1'] / df['r4t3'] # 가정 인원수 대비 임대\n",
    "    df['v2a1_to_r4t3'] = df['v2a1'] / (df['r4t3'] - df['r4t1']) # 12세 이하 인원 대비 임대\n",
    "    df['hhsize_to_rooms'] = df['hhsize'] / df['rooms'] # 사람 당 방\n",
    "    df['rent_to_hhsize'] = df['v2a1'] / df['hhsize'] # 가족규모 대비 임대\n",
    "    df['rent_to_over_18'] = df['v2a1'] / df['num_over_18']\n",
    "    # 18세 이상 인원이 없는 가정은 총 임대를 사용\n",
    "    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n",
    "    \n",
    "extract_features(train)\n",
    "extract_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sweet-trustee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:58.024895Z",
     "start_time": "2021-02-08T18:24:57.994976Z"
    }
   },
   "outputs": [],
   "source": [
    "# 중복 열 삭제\n",
    "needless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq', 'mobilephone', 'female', ]\n",
    "instlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n",
    "needless_cols.extend(instlevel_cols)\n",
    "\n",
    "train = train.drop(needless_cols, axis=1)\n",
    "test = test.drop(needless_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-lawsuit",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-twins",
   "metadata": {},
   "source": [
    "동일한 가정에 속하는 행은 보통 동일한 target을 가지므로, 누락을 방지하기 위해 가정별로 데이터를 분할한다. 가장만 포함하도록 데이터를 필터링해서 기술적으로 필요하진 않지만, 그렇게 하려면 전체 train 데이터를 쉽게 사용할 수 있다.\n",
    "\n",
    "데이터를 분할한 후, train 데이터를 전체 데이터셋으로 덮어써, 모든 데이터를 학습시킬 수 있다. split_data 함수는 데이터를 덮어쓰지 않고 동일한 작업을 하며, train 반복 내에서 K-Fold 분할을 근사화하는 데 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "seasonal-attribute",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:58.040855Z",
     "start_time": "2021-02-08T18:24:58.026892Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(train, y, sample_weight=None, households=None, test_percentage=0.20, seed=None):\n",
    "    # np.randome.seed(seed=seed)\n",
    "    train2 = train.copy()\n",
    "    # test 데이터에 사용할 가정 랜덤 선택\n",
    "    cv_hhs = np.random.choice(households, size=int(len(households) * test_percentage), replace=False)\n",
    "    # 랜덤 선택된 가정 선택\n",
    "    cv_idx = np.isin(households, cv_hhs)\n",
    "    X_test = train2[cv_idx]\n",
    "    y_test = y[cv_idx]\n",
    "    X_train = train2[~cv_idx]\n",
    "    y_train = y[~cv_idx]\n",
    "    \n",
    "    if sample_weight is not None:\n",
    "        y_train_weights = sample_weight[~cv_idx]\n",
    "        return X_train, y_train, X_test, y_test, y_train_weights\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "minor-productivity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:58.104722Z",
     "start_time": "2021-02-08T18:24:58.048833Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train.query('parentesco1==1')\n",
    "# X = train.copy()\n",
    "\n",
    "# target 변수를 추출하고 삭제\n",
    "y = X['Target'] - 1\n",
    "X = X.drop(['Target'], axis=1)\n",
    "\n",
    "np.random.seed(seed=None)\n",
    "train2 = X.copy()\n",
    "train_hhs = train2.idhogar\n",
    "\n",
    "households = train2.idhogar.unique()\n",
    "cv_hhs = np.random.choice(households, size=int(len(households) * 0.15), replace=False)\n",
    "\n",
    "cv_idx = np.isin(train2.idhogar, cv_hhs)\n",
    "\n",
    "X_test = train2[cv_idx]\n",
    "y_test = y[cv_idx]\n",
    "X_train = train2[~cv_idx]\n",
    "y_train = y[~cv_idx]\n",
    "\n",
    "# train에 전체 데이터셋 씌우기\n",
    "X_train = train2\n",
    "y_train = y\n",
    "\n",
    "train_households = X_train.idhogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "overall-oracle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:58.119642Z",
     "start_time": "2021-02-08T18:24:58.106677Z"
    }
   },
   "outputs": [],
   "source": [
    "# 불균형한 클래스를 학습시키기 위해 클래스 가중치 계산\n",
    "y_train_weights = class_weight.compute_sample_weight('balanced', y_train, indices=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "enormous-folks",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:58.135600Z",
     "start_time": "2021-02-08T18:24:58.122635Z"
    }
   },
   "outputs": [],
   "source": [
    "# LGBM에 사용되지 않거나 중요도가 매우 낮은 변수 제거\n",
    "extra_drop_features = [\n",
    " 'agg18_estadocivil1_MEAN',\n",
    " 'agg18_estadocivil6_COUNT',\n",
    " 'agg18_estadocivil7_COUNT',\n",
    " 'agg18_parentesco10_COUNT',\n",
    " 'agg18_parentesco11_COUNT',\n",
    " 'agg18_parentesco12_COUNT',\n",
    " 'agg18_parentesco1_COUNT',\n",
    " 'agg18_parentesco2_COUNT',\n",
    " 'agg18_parentesco3_COUNT',\n",
    " 'agg18_parentesco4_COUNT',\n",
    " 'agg18_parentesco5_COUNT',\n",
    " 'agg18_parentesco6_COUNT',\n",
    " 'agg18_parentesco7_COUNT',\n",
    " 'agg18_parentesco8_COUNT',\n",
    " 'agg18_parentesco9_COUNT',\n",
    " 'geo_elimbasu_LE_4',\n",
    " 'geo_energcocinar_LE_1',\n",
    " 'geo_energcocinar_LE_2',\n",
    " 'geo_epared_LE_0',\n",
    " 'geo_hogar_mayor',\n",
    " 'geo_manual_elec_LE_2',\n",
    " 'geo_pared_LE_3',\n",
    " 'geo_pared_LE_4',\n",
    " 'geo_pared_LE_5',\n",
    " 'geo_pared_LE_6',\n",
    " 'num_over_18',\n",
    " 'parentesco_LE',\n",
    " 'rez_esc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "finished-declaration",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:24:58.151557Z",
     "start_time": "2021-02-08T18:24:58.139589Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_drop_cols = extra_drop_features + [\"idhogar\", 'parentesco1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-sample",
   "metadata": {},
   "source": [
    "# Fit a voting classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-classic",
   "metadata": {},
   "source": [
    "조기중지를 위해 fit_params를 전달할 수 있도록 파생된 VotingClassifier 클래스를 정의한다. vote는 macro F1 score에 기반한 조기정지와 학습률 저하가 있는 LGBM 모델에 기반한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "beneficial-force",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:33:25.069973Z",
     "start_time": "2021-02-08T18:33:25.058004Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_parameters = {'max_depth':35, 'eta':0.1, 'objective':'multi:softmax', 'min_child_weight': 1, 'num_class': 4, 'gamma': 2.0, 'colsample_bylevel': 0.9, 'subsample': 0.84, 'colsample_bytree': 0.88, 'reg_lambda': 0.40 }\n",
    "opt_parameters = {'max_depth':35, 'eta':0.15, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.5, 'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n",
    "\n",
    "def evaluate_macroF1_lgb(predictions, truth):\n",
    "    pred_labels = predictions.argmax(axis=1)\n",
    "    truth = truth.get_label()\n",
    "    f1 = f1_score(truth, pred_labels, average='macro')\n",
    "    return ('macroF1', 1-f1) \n",
    "\n",
    "fit_params={\"early_stopping_rounds\":500,\n",
    "            \"eval_metric\" : evaluate_macroF1_lgb, \n",
    "            \"eval_set\" : [(X_train,y_train), (X_test,y_test)],\n",
    "            'verbose': False,}\n",
    "\n",
    "def learning_rate_power_0997(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    min_learning_rate = 0.02\n",
    "    lr = base_learning_rate * np.power(.995, current_iter)\n",
    "    return max(lr, min_learning_rate)\n",
    "\n",
    "fit_params['verbose'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "applicable-meter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:56:50.855388Z",
     "start_time": "2021-02-08T18:56:50.836442Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "def _parallel_fit_estimator(estimator1, X, y, sample_weight=None, threshold=True, **fit_params):\n",
    "    estimator = clone(estimator1)\n",
    "    \n",
    "    # 데이터를 랜덤 분할해 조기중지할 test 셋을 얻는다\n",
    "    if sample_weight is not None:\n",
    "        X_train, y_train, X_test, y_test, y_train_weight = split_data(X,y,sample_weight,households=train_households)\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test = split_data(X,y,None,households=train_households)\n",
    "        \n",
    "    # 새로 분할한 것에 대해 fit params 업데이트\n",
    "    fit_params['eval_set'] = [(X_test,y_test)]\n",
    "    \n",
    "    # estimator 적합\n",
    "    if sample_weight is not None:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)  ### isinstance(A,B) : A가 B인지 확인\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, sample_weight=y_train_weight, **fit_params)\n",
    "    else :\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, **fit_params)\n",
    "            \n",
    "    if not isinstance(estimator1, ExtraTreesClassifier) and not isinstance(estimator1, RandomForestClassifier) and not isinstance(estimator1, xgb.XGBClassifier):\n",
    "        best_cv_round = np.argmax(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_cv = np.max(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_train = estimator.evals_result_['train']['macroF1'][best_cv_round]\n",
    "    else:\n",
    "        best_train = f1_score(y_train, estimator.predict(X_train), average='macro')\n",
    "        best_cv = f1_score(y_test, estimator.predict(X_test), average='macro')\n",
    "        print('Train F1 :', best_train)\n",
    "        print('Test F1 :', best_cv)\n",
    "        \n",
    "    # train, test 셋의 성능에 기반한 일부 estimator 거부\n",
    "    if threshold:\n",
    "        # valid score가 너무 높으면 train score에 여유를 약간 줌\n",
    "        if ((best_cv>0.37) and (best_train>0.75)) or ((best_cv>0.44) and (best_train>0.64)):\n",
    "            return estimator\n",
    "        # 아니면 더 나은 점수를 얻을 때까지 반복\n",
    "        else:\n",
    "            print('Unacceptable!! Trying again...')\n",
    "            return _parallel_fit_estimator(estimator1, X, y, sample_weight=sample_weight, **fit_params)\n",
    "    \n",
    "    else :\n",
    "        return estimator\n",
    "        \n",
    "        \n",
    "# fit_params를 전파하는 VotingClassifier의 적합 방법 구현\n",
    "class VotingClassifierLGBM(VotingClassifier):\n",
    "    def fit(self, X, y, sample_weight=None, threshold=True, **fit_params):\n",
    "        if isinstance(y, np.ndarray) and len(y.shape)>1 and y.shape[1]>1:\n",
    "            raise NotImplementedError('Multilabel and multi-output'\n",
    "                                     ' classification is not supported.')\n",
    "        if self.voting not in ('soft','hard'):\n",
    "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\" %self.voting)\n",
    "        \n",
    "        if self.estimators is None or len(self.estimators)==0:\n",
    "            raise AttributeError(\"Invalid 'estimators' attribute, 'estimators' should be a list of (string, estimator)\"\" tuples\")\n",
    "            \n",
    "        if (self.weights is not None and len(self.weights)!=len(self.estimators)):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                            '; got %d weights %d estimators' %(len(self.weights),len(self.estimators)))\n",
    "    \n",
    "        names, clfs = zip(*self.estimators)\n",
    "        self._validate_names(names)\n",
    "        \n",
    "        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n",
    "        if n_isnone == len(self.estimators):\n",
    "            raise ValueError('All estimators are None. At least one is '\n",
    "                            'required to be a classifier!')\n",
    "        \n",
    "        self.le_ = LabelEncoder().fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        self.estimators_ = []\n",
    "        \n",
    "        transformed_y = self.le_.transform(y)\n",
    "        \n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(delayed(_parallel_fit_estimator)\n",
    "                                      (clone(clf),X,transformed_y,sample_weight=sample_weight,threshold=threshold,**fit_params)\n",
    "                                                        for clf in clfs if clf is not None)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "collect-posting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:46:32.317106Z",
     "start_time": "2021-02-08T18:36:37.162860Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:36:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29894\tvalidation_0-macroF1:0.63874\n",
      "[50]\tvalidation_0-mlogloss:0.89615\tvalidation_0-macroF1:0.56756\n",
      "[100]\tvalidation_0-mlogloss:0.89371\tvalidation_0-macroF1:0.55565\n",
      "[150]\tvalidation_0-mlogloss:0.89062\tvalidation_0-macroF1:0.56537\n",
      "[200]\tvalidation_0-mlogloss:0.89161\tvalidation_0-macroF1:0.56447\n",
      "[250]\tvalidation_0-mlogloss:0.89255\tvalidation_0-macroF1:0.56603\n",
      "[299]\tvalidation_0-mlogloss:0.89179\tvalidation_0-macroF1:0.56650\n",
      "Train F1 : 0.9115660908746297\n",
      "Test F1 : 0.44462641001160175\n",
      "[03:37:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30399\tvalidation_0-macroF1:0.62540\n",
      "[50]\tvalidation_0-mlogloss:0.91589\tvalidation_0-macroF1:0.58098\n",
      "[100]\tvalidation_0-mlogloss:0.91543\tvalidation_0-macroF1:0.59476\n",
      "[150]\tvalidation_0-mlogloss:0.91787\tvalidation_0-macroF1:0.60346\n",
      "[200]\tvalidation_0-mlogloss:0.91642\tvalidation_0-macroF1:0.60225\n",
      "[250]\tvalidation_0-mlogloss:0.91610\tvalidation_0-macroF1:0.60209\n",
      "[299]\tvalidation_0-mlogloss:0.91594\tvalidation_0-macroF1:0.60731\n",
      "Train F1 : 0.8976233528333496\n",
      "Test F1 : 0.4299722420279911\n",
      "[03:38:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30003\tvalidation_0-macroF1:0.60316\n",
      "[50]\tvalidation_0-mlogloss:0.94443\tvalidation_0-macroF1:0.59214\n",
      "[100]\tvalidation_0-mlogloss:0.93843\tvalidation_0-macroF1:0.57918\n",
      "[150]\tvalidation_0-mlogloss:0.93847\tvalidation_0-macroF1:0.58042\n",
      "[200]\tvalidation_0-mlogloss:0.93756\tvalidation_0-macroF1:0.57726\n",
      "[250]\tvalidation_0-mlogloss:0.93589\tvalidation_0-macroF1:0.58541\n",
      "[299]\tvalidation_0-mlogloss:0.93606\tvalidation_0-macroF1:0.58968\n",
      "Train F1 : 0.9128351179853464\n",
      "Test F1 : 0.4326931582349539\n",
      "[03:38:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30547\tvalidation_0-macroF1:0.62321\n",
      "[50]\tvalidation_0-mlogloss:0.92165\tvalidation_0-macroF1:0.56081\n",
      "[100]\tvalidation_0-mlogloss:0.91869\tvalidation_0-macroF1:0.55783\n",
      "[150]\tvalidation_0-mlogloss:0.91620\tvalidation_0-macroF1:0.54999\n",
      "[200]\tvalidation_0-mlogloss:0.91636\tvalidation_0-macroF1:0.55482\n",
      "[250]\tvalidation_0-mlogloss:0.91588\tvalidation_0-macroF1:0.55061\n",
      "[299]\tvalidation_0-mlogloss:0.91556\tvalidation_0-macroF1:0.54736\n",
      "Train F1 : 0.9273238446441445\n",
      "Test F1 : 0.45331548653917075\n",
      "[03:39:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30109\tvalidation_0-macroF1:0.63195\n",
      "[50]\tvalidation_0-mlogloss:0.92433\tvalidation_0-macroF1:0.60566\n",
      "[100]\tvalidation_0-mlogloss:0.92053\tvalidation_0-macroF1:0.59748\n",
      "[150]\tvalidation_0-mlogloss:0.91912\tvalidation_0-macroF1:0.60108\n",
      "[200]\tvalidation_0-mlogloss:0.91950\tvalidation_0-macroF1:0.61223\n",
      "[250]\tvalidation_0-mlogloss:0.91865\tvalidation_0-macroF1:0.61383\n",
      "[299]\tvalidation_0-mlogloss:0.91853\tvalidation_0-macroF1:0.60909\n",
      "Train F1 : 0.8604328364026029\n",
      "Test F1 : 0.4192263582825215\n",
      "[03:39:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30172\tvalidation_0-macroF1:0.61078\n",
      "[50]\tvalidation_0-mlogloss:0.90473\tvalidation_0-macroF1:0.58278\n",
      "[100]\tvalidation_0-mlogloss:0.90000\tvalidation_0-macroF1:0.57990\n",
      "[150]\tvalidation_0-mlogloss:0.89396\tvalidation_0-macroF1:0.58043\n",
      "[200]\tvalidation_0-mlogloss:0.89226\tvalidation_0-macroF1:0.57862\n",
      "[250]\tvalidation_0-mlogloss:0.89228\tvalidation_0-macroF1:0.58604\n",
      "[299]\tvalidation_0-mlogloss:0.89270\tvalidation_0-macroF1:0.58820\n",
      "Train F1 : 0.927086687566341\n",
      "Test F1 : 0.42500017325799716\n",
      "[03:40:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30351\tvalidation_0-macroF1:0.64047\n",
      "[50]\tvalidation_0-mlogloss:0.90510\tvalidation_0-macroF1:0.58320\n",
      "[100]\tvalidation_0-mlogloss:0.90194\tvalidation_0-macroF1:0.58600\n",
      "[150]\tvalidation_0-mlogloss:0.90231\tvalidation_0-macroF1:0.58331\n",
      "[200]\tvalidation_0-mlogloss:0.90319\tvalidation_0-macroF1:0.57922\n",
      "[250]\tvalidation_0-mlogloss:0.90326\tvalidation_0-macroF1:0.58032\n",
      "[299]\tvalidation_0-mlogloss:0.90476\tvalidation_0-macroF1:0.57784\n",
      "Train F1 : 0.9131085061977701\n",
      "Test F1 : 0.42540312297831107\n",
      "[03:41:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30457\tvalidation_0-macroF1:0.66229\n",
      "[50]\tvalidation_0-mlogloss:0.92731\tvalidation_0-macroF1:0.62286\n",
      "[100]\tvalidation_0-mlogloss:0.92147\tvalidation_0-macroF1:0.62384\n",
      "[150]\tvalidation_0-mlogloss:0.92070\tvalidation_0-macroF1:0.63108\n",
      "[200]\tvalidation_0-mlogloss:0.91867\tvalidation_0-macroF1:0.63529\n",
      "[250]\tvalidation_0-mlogloss:0.91730\tvalidation_0-macroF1:0.62598\n",
      "[299]\tvalidation_0-mlogloss:0.91654\tvalidation_0-macroF1:0.63122\n",
      "Train F1 : 0.9039317978478034\n",
      "Test F1 : 0.3870640568511129\n",
      "[03:41:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29708\tvalidation_0-macroF1:0.62412\n",
      "[50]\tvalidation_0-mlogloss:0.88409\tvalidation_0-macroF1:0.56213\n",
      "[100]\tvalidation_0-mlogloss:0.88135\tvalidation_0-macroF1:0.55805\n",
      "[150]\tvalidation_0-mlogloss:0.88174\tvalidation_0-macroF1:0.55302\n",
      "[200]\tvalidation_0-mlogloss:0.87978\tvalidation_0-macroF1:0.54801\n",
      "[250]\tvalidation_0-mlogloss:0.87887\tvalidation_0-macroF1:0.54403\n",
      "[299]\tvalidation_0-mlogloss:0.87894\tvalidation_0-macroF1:0.54671\n",
      "Train F1 : 0.926590037660807\n",
      "Test F1 : 0.45596696580867907\n",
      "[03:42:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30941\tvalidation_0-macroF1:0.67391\n",
      "[50]\tvalidation_0-mlogloss:0.90405\tvalidation_0-macroF1:0.60191\n",
      "[100]\tvalidation_0-mlogloss:0.89729\tvalidation_0-macroF1:0.59552\n",
      "[150]\tvalidation_0-mlogloss:0.89797\tvalidation_0-macroF1:0.58308\n",
      "[200]\tvalidation_0-mlogloss:0.89707\tvalidation_0-macroF1:0.60787\n",
      "[250]\tvalidation_0-mlogloss:0.89636\tvalidation_0-macroF1:0.60327\n",
      "[299]\tvalidation_0-mlogloss:0.89568\tvalidation_0-macroF1:0.59413\n",
      "Train F1 : 0.9177266968021762\n",
      "Test F1 : 0.4169205363356127\n",
      "[03:43:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.29873\tvalidation_0-macroF1:0.64866\n",
      "[50]\tvalidation_0-mlogloss:0.93059\tvalidation_0-macroF1:0.57796\n",
      "[100]\tvalidation_0-mlogloss:0.93042\tvalidation_0-macroF1:0.58456\n",
      "[150]\tvalidation_0-mlogloss:0.93141\tvalidation_0-macroF1:0.58464\n",
      "[200]\tvalidation_0-mlogloss:0.93280\tvalidation_0-macroF1:0.58348\n",
      "[250]\tvalidation_0-mlogloss:0.93234\tvalidation_0-macroF1:0.58326\n",
      "[299]\tvalidation_0-mlogloss:0.93234\tvalidation_0-macroF1:0.58357\n",
      "Train F1 : 0.8890905288434153\n",
      "Test F1 : 0.4291004079449764\n",
      "[03:43:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29814\tvalidation_0-macroF1:0.63167\n",
      "[50]\tvalidation_0-mlogloss:0.93109\tvalidation_0-macroF1:0.60363\n",
      "[100]\tvalidation_0-mlogloss:0.92695\tvalidation_0-macroF1:0.60890\n",
      "[150]\tvalidation_0-mlogloss:0.92862\tvalidation_0-macroF1:0.61470\n",
      "[200]\tvalidation_0-mlogloss:0.92751\tvalidation_0-macroF1:0.60809\n",
      "[250]\tvalidation_0-mlogloss:0.92977\tvalidation_0-macroF1:0.60797\n",
      "[299]\tvalidation_0-mlogloss:0.93202\tvalidation_0-macroF1:0.61026\n",
      "Train F1 : 0.9022537754659561\n",
      "Test F1 : 0.42152794153781975\n",
      "[03:44:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29890\tvalidation_0-macroF1:0.59884\n",
      "[50]\tvalidation_0-mlogloss:0.89680\tvalidation_0-macroF1:0.57203\n",
      "[100]\tvalidation_0-mlogloss:0.89428\tvalidation_0-macroF1:0.58095\n",
      "[150]\tvalidation_0-mlogloss:0.89336\tvalidation_0-macroF1:0.57380\n",
      "[200]\tvalidation_0-mlogloss:0.89353\tvalidation_0-macroF1:0.57258\n",
      "[250]\tvalidation_0-mlogloss:0.89322\tvalidation_0-macroF1:0.57108\n",
      "[299]\tvalidation_0-mlogloss:0.89235\tvalidation_0-macroF1:0.57199\n",
      "Train F1 : 0.8938569029876557\n",
      "Test F1 : 0.43980449162180346\n",
      "[03:44:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30963\tvalidation_0-macroF1:0.63537\n",
      "[50]\tvalidation_0-mlogloss:0.92115\tvalidation_0-macroF1:0.58287\n",
      "[100]\tvalidation_0-mlogloss:0.91136\tvalidation_0-macroF1:0.58107\n",
      "[150]\tvalidation_0-mlogloss:0.90792\tvalidation_0-macroF1:0.56804\n",
      "[200]\tvalidation_0-mlogloss:0.90564\tvalidation_0-macroF1:0.56718\n",
      "[250]\tvalidation_0-mlogloss:0.90621\tvalidation_0-macroF1:0.56815\n",
      "[299]\tvalidation_0-mlogloss:0.90505\tvalidation_0-macroF1:0.57516\n",
      "Train F1 : 0.9216674147144617\n",
      "Test F1 : 0.43368257176436975\n",
      "[03:45:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29451\tvalidation_0-macroF1:0.62019\n",
      "[50]\tvalidation_0-mlogloss:0.87714\tvalidation_0-macroF1:0.57123\n",
      "[100]\tvalidation_0-mlogloss:0.87027\tvalidation_0-macroF1:0.57247\n",
      "[150]\tvalidation_0-mlogloss:0.86424\tvalidation_0-macroF1:0.56200\n",
      "[200]\tvalidation_0-mlogloss:0.86369\tvalidation_0-macroF1:0.55931\n",
      "[250]\tvalidation_0-mlogloss:0.86174\tvalidation_0-macroF1:0.56405\n",
      "[299]\tvalidation_0-mlogloss:0.86261\tvalidation_0-macroF1:0.56751\n",
      "Train F1 : 0.9216788990707238\n",
      "Test F1 : 0.4421686411590497\n"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "for i in range(15):\n",
    "    clf = xgb.XGBClassifier(random_state=217+i, n_estimators=300, learning_rate=0.15, n_jobs=4, **opt_parameters)\n",
    "    clfs.append(('xgb{}'.format(i), clf))\n",
    "    \n",
    "vc = VotingClassifierLGBM(clfs, voting='soft')\n",
    "del(clfs)\n",
    "\n",
    "# 학습률 저하로 최종 모델 학습\n",
    "_ = vc.fit(X_train.drop(xgb_drop_cols, axis=1), y_train, sample_weight=y_train_weights, threshold=False, **fit_params)\n",
    "\n",
    "clf_final = vc.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "partial-removal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:46:49.396884Z",
     "start_time": "2021-02-08T18:46:48.985984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a single LGBM Classifier: 0.8403\n",
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.9253\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.9321\n"
     ]
    }
   ],
   "source": [
    "# 400 early stop - 15 estimators - l1 used features - weighted\n",
    "global_score = f1_score(y_test, clf_final.predict(X_test.drop(xgb_drop_cols,axis=1)),average='macro')\n",
    "vc.voting = 'soft'\n",
    "global_score_soft = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "vc.voting = 'hard'\n",
    "global_score_hard = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "\n",
    "print('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "twelve-trailer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:48:14.342831Z",
     "start_time": "2021-02-08T18:48:12.366702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agg18_estadocivil4_COUNT',\n",
       " 'agg18_estadocivil5_COUNT',\n",
       " 'geo_energcocinar_LE_0',\n",
       " 'geo_epared_LE_2'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어느 모델에도 사용되지 않는 변수 확인\n",
    "useless_features = []\n",
    "drop_features = set()\n",
    "counter = 0\n",
    "\n",
    "for est in vc.estimators_:\n",
    "    ranked_features, unused_features = feature_importance(est, X_train.drop(xgb_drop_cols, axis=1), display_results=False)\n",
    "    useless_features.append(unused_features)\n",
    "    if counter == 0:\n",
    "        drop_features = set(unused_features)\n",
    "    else:\n",
    "        drop_features = drop_features.intersection(set(unused_features))\n",
    "    counter += 1\n",
    "    \n",
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "forced-snake",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:48:38.783113Z",
     "start_time": "2021-02-08T18:48:38.601491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking :\n",
      "1. feature 114 (0.030279) - geo_epared_LE_1\n",
      "2. feature 42 (0.019682) - fe_children_fraction\n",
      "3. feature 74 (0.018983) - agg18_parentesco2_MEAN\n",
      "4. feature 59 (0.017248) - agg18_escolari_MAX\n",
      "5. feature 133 (0.016442) - geo_pared_LE_1\n",
      "6. feature 60 (0.014158) - agg18_escolari_MEAN\n",
      "7. feature 40 (0.013327) - SQBdependency\n",
      "8. feature 22 (0.012823) - dependency\n",
      "9. feature 34 (0.012632) - SQBescolari\n",
      "10. feature 12 (0.011761) - r4t1\n",
      "11. feature 37 (0.011632) - SQBedjefe\n",
      "12. feature 112 (0.011575) - geo_etecho_LE_1\n",
      "13. feature 126 (0.010669) - geo_sanitario_LE_3\n",
      "14. feature 96 (0.010495) - estadocivil_LE\n",
      "15. feature 116 (0.010293) - geo_elimbasu_LE_0\n",
      "16. feature 11 (0.010211) - r4m3\n",
      "17. feature 100 (0.010105) - geo_age\n",
      "18. feature 94 (0.010058) - etecho_LE\n",
      "19. feature 17 (0.009895) - male\n",
      "20. feature 105 (0.009877) - geo_hogar_total\n",
      "21. feature 87 (0.009871) - piso_LE\n",
      "22. feature 39 (0.009789) - SQBovercrowding\n",
      "23. feature 63 (0.009531) - agg18_estadocivil2_MEAN\n",
      "24. feature 117 (0.009491) - geo_elimbasu_LE_1\n",
      "25. feature 41 (0.009429) - SQBmeaned\n",
      "26. feature 95 (0.009387) - eviv_LE\n",
      "27. feature 49 (0.009266) - fe_mobile_density\n",
      "28. feature 104 (0.009116) - geo_hogar_adul\n",
      "29. feature 15 (0.009112) - cielorazo\n",
      "30. feature 23 (0.008963) - edjefe\n",
      "31. feature 93 (0.008881) - epared_LE\n",
      "32. feature 65 (0.008877) - agg18_estadocivil3_MEAN\n",
      "33. feature 124 (0.008846) - geo_sanitario_LE_1\n",
      "34. feature 109 (0.008796) - geo_eviv_LE_1\n",
      "35. feature 98 (0.008791) - tipovivi_LE\n",
      "36. feature 107 (0.008772) - geo_overcrowding\n",
      "37. feature 19 (0.008759) - hogar_adul\n",
      "38. feature 13 (0.008726) - r4t2\n",
      "39. feature 106 (0.008656) - geo_bedrooms\n",
      "40. feature 51 (0.008552) - fe_mobile_adult_density\n",
      "41. feature 27 (0.008478) - overcrowding\n",
      "42. feature 55 (0.008476) - agg18_age_MIN\n",
      "43. feature 31 (0.008450) - area1\n",
      "44. feature 58 (0.008433) - agg18_escolari_MIN\n",
      "45. feature 119 (0.008352) - geo_elimbasu_LE_3\n",
      "46. feature 14 (0.008237) - escolari\n",
      "47. feature 25 (0.008114) - meaneduc\n",
      "48. feature 86 (0.008095) - pared_LE\n",
      "49. feature 33 (0.008083) - age\n",
      "50. feature 7 (0.008060) - r4h2\n",
      "51. feature 120 (0.007983) - geo_elimbasu_LE_5\n",
      "52. feature 69 (0.007941) - agg18_estadocivil5_MEAN\n",
      "53. feature 97 (0.007922) - lugar_LE\n",
      "54. feature 123 (0.007897) - geo_sanitario_LE_0\n",
      "55. feature 35 (0.007885) - SQBage\n",
      "56. feature 71 (0.007873) - agg18_estadocivil6_MEAN\n",
      "57. feature 44 (0.007848) - fe_all_man_fraction\n",
      "58. feature 102 (0.007817) - geo_dependency\n",
      "59. feature 92 (0.007798) - elimbasu_LE\n",
      "60. feature 62 (0.007746) - agg18_estadocivil1_COUNT\n",
      "61. feature 45 (0.007711) - fe_human_density\n",
      "62. feature 0 (0.007631) - v2a1\n",
      "63. feature 56 (0.007629) - agg18_age_MAX\n",
      "64. feature 122 (0.007584) - geo_energcocinar_LE_3\n",
      "65. feature 8 (0.007526) - r4h3\n",
      "66. feature 136 (0.007525) - bedrooms_to_rooms\n",
      "67. feature 91 (0.007479) - energcocinar_LE\n",
      "68. feature 10 (0.007478) - r4m2\n",
      "69. feature 43 (0.007475) - fe_working_man_fraction\n",
      "70. feature 5 (0.007457) - v18q1\n",
      "71. feature 47 (0.007435) - fe_rent_per_person\n",
      "72. feature 4 (0.007360) - refrig\n",
      "73. feature 137 (0.007317) - rent_to_rooms\n",
      "74. feature 26 (0.007297) - bedrooms\n",
      "75. feature 128 (0.007238) - geo_manual_elec_LE_0\n",
      "76. feature 61 (0.007208) - agg18_dis_MEAN\n",
      "77. feature 24 (0.007182) - edjefa\n",
      "78. feature 30 (0.007106) - qmobilephone\n",
      "79. feature 85 (0.007052) - edjef\n",
      "80. feature 72 (0.006995) - agg18_estadocivil7_MEAN\n",
      "81. feature 36 (0.006962) - SQBhogar_total\n",
      "82. feature 90 (0.006927) - sanitario_LE\n",
      "83. feature 29 (0.006899) - television\n",
      "84. feature 21 (0.006897) - hogar_total\n",
      "85. feature 2 (0.006891) - rooms\n",
      "86. feature 125 (0.006886) - geo_sanitario_LE_2\n",
      "87. feature 16 (0.006851) - dis\n",
      "88. feature 18 (0.006810) - hogar_nin\n",
      "89. feature 143 (0.006710) - rent_to_hhsize\n",
      "90. feature 57 (0.006687) - agg18_age_MEAN\n",
      "91. feature 138 (0.006636) - tamhog_to_rooms\n",
      "92. feature 64 (0.006620) - agg18_estadocivil2_COUNT\n",
      "93. feature 52 (0.006611) - fe_tablet_adult_density\n",
      "94. feature 48 (0.006435) - fe_rent_per_room\n",
      "95. feature 6 (0.006235) - r4h1\n",
      "96. feature 1 (0.005999) - hacdor\n",
      "97. feature 99 (0.005923) - manual_elec_LE\n",
      "98. feature 129 (0.005903) - geo_manual_elec_LE_1\n",
      "99. feature 50 (0.005864) - fe_tablet_density\n",
      "100. feature 144 (0.005774) - rent_to_over_18\n",
      "101. feature 79 (0.005734) - agg18_parentesco7_MEAN\n",
      "102. feature 20 (0.005656) - hogar_mayor\n",
      "103. feature 89 (0.005596) - abastagua_LE\n",
      "104. feature 38 (0.005586) - SQBhogar_nin\n",
      "105. feature 111 (0.005451) - geo_etecho_LE_0\n",
      "106. feature 67 (0.005196) - agg18_estadocivil4_MEAN\n",
      "107. feature 88 (0.005127) - techo_LE\n",
      "108. feature 3 (0.005073) - hacapo\n",
      "109. feature 75 (0.004926) - agg18_parentesco3_MEAN\n",
      "110. feature 28 (0.004834) - computer\n",
      "111. feature 81 (0.004800) - agg18_parentesco9_MEAN\n",
      "112. feature 141 (0.004748) - v2a1_to_r4t3\n",
      "113. feature 9 (0.004701) - r4m1\n",
      "114. feature 140 (0.004519) - r4t3_to_rooms\n",
      "115. feature 77 (0.004361) - agg18_parentesco5_MEAN\n",
      "116. feature 53 (0.004182) - fe_people_not_living\n",
      "117. feature 32 (0.004033) - area2\n",
      "118. feature 78 (0.003952) - agg18_parentesco6_MEAN\n",
      "119. feature 83 (0.003865) - agg18_parentesco11_MEAN\n",
      "120. feature 101 (0.003449) - geo_meaneduc\n",
      "121. feature 76 (0.002761) - agg18_parentesco4_MEAN\n",
      "122. feature 142 (0.002618) - hhsize_to_rooms\n",
      "123. feature 84 (0.002575) - agg18_parentesco12_MEAN\n",
      "124. feature 139 (0.001424) - r4t3_to_tamhog\n",
      "125. feature 80 (0.001382) - agg18_parentesco8_MEAN\n",
      "126. feature 113 (0.000000) - geo_etecho_LE_2\n",
      "127. feature 134 (0.000000) - geo_pared_LE_2\n",
      "128. feature 68 (0.000000) - agg18_estadocivil4_COUNT\n",
      "129. feature 70 (0.000000) - agg18_estadocivil5_COUNT\n",
      "130. feature 46 (0.000000) - fe_human_bed_density\n",
      "131. feature 73 (0.000000) - agg18_parentesco1_MEAN\n",
      "132. feature 135 (0.000000) - geo_pared_LE_7\n",
      "133. feature 132 (0.000000) - geo_pared_LE_0\n",
      "134. feature 115 (0.000000) - geo_epared_LE_2\n",
      "135. feature 110 (0.000000) - geo_eviv_LE_2\n",
      "136. feature 130 (0.000000) - geo_manual_elec_LE_3\n",
      "137. feature 103 (0.000000) - geo_hogar_nin\n",
      "138. feature 54 (0.000000) - fe_people_weird_stat\n",
      "139. feature 127 (0.000000) - geo_sanitario_LE_4\n",
      "140. feature 66 (0.000000) - agg18_estadocivil3_COUNT\n",
      "141. feature 108 (0.000000) - geo_eviv_LE_0\n",
      "142. feature 82 (0.000000) - agg18_parentesco10_MEAN\n",
      "143. feature 121 (0.000000) - geo_energcocinar_LE_0\n",
      "144. feature 118 (0.000000) - geo_elimbasu_LE_2\n",
      "145. feature 131 (0.000000) - geo_manual_elec_LE_4\n"
     ]
    }
   ],
   "source": [
    "ranked_feautres = feature_importance(clf_final, X_train.drop(xgb_drop_cols, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-intelligence",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "nutritional-bottle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:49:31.893222Z",
     "start_time": "2021-02-08T18:49:31.877265Z"
    }
   },
   "outputs": [],
   "source": [
    "et_drop_cols = ['agg18_age_MAX', 'agg18_age_MEAN', 'agg18_age_MIN', 'agg18_dis_MEAN',\n",
    "       'agg18_escolari_MAX', 'agg18_escolari_MEAN', 'agg18_escolari_MIN',\n",
    "       'agg18_estadocivil1_COUNT', 'agg18_estadocivil1_MEAN',\n",
    "       'agg18_estadocivil2_COUNT', 'agg18_estadocivil2_MEAN',\n",
    "       'agg18_estadocivil3_COUNT', 'agg18_estadocivil3_MEAN',\n",
    "       'agg18_estadocivil4_COUNT', 'agg18_estadocivil4_MEAN',\n",
    "       'agg18_estadocivil5_COUNT', 'agg18_estadocivil5_MEAN',\n",
    "       'agg18_estadocivil6_COUNT', 'agg18_estadocivil6_MEAN',\n",
    "       'agg18_estadocivil7_COUNT', 'agg18_estadocivil7_MEAN',\n",
    "       'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN',\n",
    "       'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN',\n",
    "       'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN',\n",
    "       'agg18_parentesco1_COUNT', 'agg18_parentesco1_MEAN',\n",
    "       'agg18_parentesco2_COUNT', 'agg18_parentesco2_MEAN',\n",
    "       'agg18_parentesco3_COUNT', 'agg18_parentesco3_MEAN',\n",
    "       'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN',\n",
    "       'agg18_parentesco5_COUNT', 'agg18_parentesco5_MEAN',\n",
    "       'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN',\n",
    "       'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN',\n",
    "       'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN',\n",
    "       'agg18_parentesco9_COUNT', 'agg18_parentesco9_MEAN'] #+ ['parentesco_LE', 'rez_esc']\n",
    "\n",
    "et_drop_cols.extend([\"idhogar\", \"parentesco1\", 'fe_rent_per_person', 'fe_rent_per_room',\n",
    "       'fe_tablet_adult_density', 'fe_tablet_density'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "nutritional-seventh",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:57:43.126541Z",
     "start_time": "2021-02-08T18:57:01.896460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 : 0.8970955314751967\n",
      "Test F1 : 0.4339837350335132\n",
      "Train F1 : 0.8978154737944584\n",
      "Test F1 : 0.40102587275854407\n",
      "Train F1 : 0.8984260514089941\n",
      "Test F1 : 0.4178603966114821\n",
      "Train F1 : 0.8989646974584208\n",
      "Test F1 : 0.4474071002684059\n",
      "Train F1 : 0.8946277247021607\n",
      "Test F1 : 0.4040623090134163\n",
      "Train F1 : 0.8942347190248382\n",
      "Test F1 : 0.4420524924852226\n",
      "Train F1 : 0.883377868986868\n",
      "Test F1 : 0.38759359143567657\n",
      "Train F1 : 0.8830889573677272\n",
      "Test F1 : 0.3968373439696289\n",
      "Train F1 : 0.8889972541497917\n",
      "Test F1 : 0.4310054243471582\n",
      "Train F1 : 0.8857021263049742\n",
      "Test F1 : 0.4316117388522157\n"
     ]
    }
   ],
   "source": [
    "# 추가적인 트리 분류기에 같은 작업 수행\n",
    "ets = []\n",
    "for i in range(10):\n",
    "    rf = RandomForestClassifier(max_depth=None, random_state=217+i, n_jobs=1, n_estimators=700, min_impurity_decrease=1e-3,\n",
    "                               min_samples_leaf=2, verbose=0, class_weight='balanced')\n",
    "    ets.append(('rf{}'.format(i), rf))\n",
    "    \n",
    "vc2 = VotingClassifierLGBM(ets, voting='soft')    \n",
    "_ = vc2.fit(X_train.drop(et_drop_cols, axis=1), y_train, threshold=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "conditional-receptor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T18:58:38.504615Z",
     "start_time": "2021-02-08T18:58:35.629303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8620\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.8883\n"
     ]
    }
   ],
   "source": [
    "# w/ threshold, 추가 제거 열\n",
    "vc2.voting = 'soft'\n",
    "global_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols,axis=1)),average='macro')\n",
    "vc2.voting = 'hard'\n",
    "global_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "exempt-discount",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:00:54.818751Z",
     "start_time": "2021-02-08T19:00:52.401218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8620\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.8883\n"
     ]
    }
   ],
   "source": [
    "# w/o threshold, 추가 제거 열\n",
    "vc2.voting = 'soft'\n",
    "global_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "vc2.voting = 'hard'\n",
    "global_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "agricultural-stroke",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:01:29.570072Z",
     "start_time": "2021-02-08T19:01:28.875929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parentesco_LE', 'rez_esc'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어떤 모델에서 사용되지 않는 변수 확인\n",
    "useless_features = []\n",
    "drop_features = set()\n",
    "\n",
    "counter = 0\n",
    "for est in vc2.estimators_:\n",
    "    ranked_features, unused_features = feature_importance(est, X_train.drop(et_drop_cols, axis=1), display_results=False)\n",
    "    useless_features.append(unused_features)\n",
    "    if counter == 0:\n",
    "        drop_features = set(unused_features)\n",
    "    else:\n",
    "        drop_features = drop_features.intersection(set(unused_features))\n",
    "    counter += 1\n",
    "    \n",
    "drop_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "simple-postcard",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:04:41.313380Z",
     "start_time": "2021-02-08T19:04:41.294391Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine_voters(data, weights=[0.5,0.5]):\n",
    "    # 두 분류기로 soft voting\n",
    "    vc.voting='soft'\n",
    "    vc1_probs = vc.predict_proba(data.drop(xgb_drop_cols,axis=1))\n",
    "    vc2.voting='soft'\n",
    "    vc2_probs = vc2.predict_proba(data.drop(et_drop_cols,axis=1))\n",
    "    \n",
    "    final_vote = (vc1_probs*weights[0]) + (vc2_probs*weights[1])\n",
    "    predictions = np.argmax(final_vote, axis=1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "abandoned-youth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:04:43.829985Z",
     "start_time": "2021-02-08T19:04:42.270153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9058558642748031"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.5,0.5])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "derived-power",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:04:53.350106Z",
     "start_time": "2021-02-08T19:04:51.300591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89532954063404"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.4, 0.6])\n",
    "global_combo_score_soft= f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "beginning-moment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:05:05.484765Z",
     "start_time": "2021-02-08T19:05:03.534980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9121894008700175"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.6, 0.4])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-israel",
   "metadata": {},
   "source": [
    "# Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "organic-clearing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:05:16.863612Z",
     "start_time": "2021-02-08T19:05:16.843662Z"
    }
   },
   "outputs": [],
   "source": [
    "y_subm = pd.DataFrame()\n",
    "y_subm['Id'] = test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "tutorial-burning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:07:11.390534Z",
     "start_time": "2021-02-08T19:06:23.937420Z"
    }
   },
   "outputs": [],
   "source": [
    "vc.voting = 'soft'\n",
    "y_subm_lgb = y_subm.copy(deep=True)\n",
    "y_subm_lgb['Target'] = vc.predict(test.drop(xgb_drop_cols, axis=1)) + 1\n",
    "\n",
    "vc2.voting = 'soft'\n",
    "y_subm_rf = y_subm.copy(deep=True)\n",
    "y_subm_rf['Target'] = vc2.predict(test.drop(et_drop_cols, axis=1)) + 1\n",
    "\n",
    "y_subm_ens = y_subm.copy(deep=True)\n",
    "y_subm_ens['Target'] = combine_voters(test) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "framed-enterprise",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T19:08:27.642081Z",
     "start_time": "2021-02-08T19:08:27.479515Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "sub_file_lgb = 'submission_soft_XGB_{:.4f}_{}.csv'.format(global_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "sub_file_rf = 'submission_soft_RF_{:.4f}_{}.csv'.format(global_rf_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "sub_file_ens = 'submission_ens_{:.4f}_{}.csv'.format(global_combo_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "\n",
    "y_subm_lgb.to_csv(sub_file_lgb, index=False)\n",
    "y_subm_rf.to_csv(sub_file_rf, index=False)\n",
    "y_subm_ens.to_csv(sub_file_ens, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-vision",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-integrity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-corner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-carol",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-chapel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-collector",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-leisure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-cambodia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-dream",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-technology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "564px",
    "left": "236px",
    "top": "371.8px",
    "width": "150.875px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
