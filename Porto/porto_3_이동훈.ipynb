{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Study 8일차(porto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드출처 : https://www.kaggle.com/aharless/xgboost-cv-lb-284"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1회차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T01:37:55.040972Z",
     "start_time": "2021-01-27T01:37:55.036976Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_ROUNDS = 400\n",
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음에는 MAX_ROUNDS를 상당히 높게 설정하고 OPTIMIZE_ROUNDS를 사용하여 적절한 수의 라운드를 설정하려고 했다.(내 판단으로는 모든 폴드 중에서 best_ntree_limit의 최대 값에 가까워야 하며, 모델이 적절히 정규화되었다면 조금 더 높을 수도 있고... 또는 상세 =을 설정할 수도 있습니다.참이고 세부 사항을 살펴본 후 모든 접힘에 적합한 여러 라운드를 찾아보십시오.) 그런 다음 Optimize_ROUNDs를 끄고 MAX_ROUNDs를 적절한 총 라운드 수로 설정하려 했다.  \n",
    "  \n",
    "각 폴드에 가장 적합한 라운드를 선택하여 \"조기 중지\"할 때의 문제는 검증 데이터에 지나치게 적합하다는 것이다.  \n",
    "  \n",
    "따라서 테스트 데이터를 예측하기 위한 최적의 모델을 생성하지 않을 수 있으며, 다른 모델과의 스택/결합을 위한 검증 데이터를 생성하는 데 사용될 경우 이 모델의 앙상블에 너무 많은 무게가 실리게 된다.  \n",
    "  \n",
    "또 다른 가능성(XGBoost의 경우 기본값인 것 같음)은 최적 라운드가 아닌 조기 스톱이 실제로 발생하는 라운드(개선 부족을 확인하는 지연 시간 포함)를 사용하는 것이다. 이렇게 하면 과적합 문제가 해결되지만(지체가 충분히 길다면) 아직까지는 도움이 되지 않은 것 같다. (모든 접힘에 대해 일정한 회차 수보다 접힘당 20라운드 조기 정지 점수를 더 못 받아 조기 정지가 실제로 맞지 않는 것 같았다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T01:41:07.557006Z",
     "start_time": "2021-01-27T01:41:01.616814Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numba import jit\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T02:04:11.187502Z",
     "start_time": "2021-01-27T02:04:11.179514Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T02:59:20.192832Z",
     "start_time": "2021-01-27T02:59:20.177909Z"
    }
   },
   "outputs": [],
   "source": [
    "def gini_xgb(pred,dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score=-eval_gini(labels,preds)\n",
    "    return[('gini',gini_score)]\n",
    "\n",
    "def add_noise(series,noise_level):\n",
    "    return series * (1+noise_level*np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None,\n",
    "                 val_series=None,\n",
    "                 tst_series=None,\n",
    "                 target=None,\n",
    "                 min_samples_leaf=1,\n",
    "                 smoothing=1,\n",
    "                 noise_level=0):\n",
    "    assert len(trn_series)==len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series,target],axis=1)\n",
    "    \n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg(['mean','count'])\n",
    "    \n",
    "    smoothing = 1 / (1+np.exp(-(averages['count']-min_samples_leaf)/smoothing))\n",
    "    \n",
    "    prior = target.mean()\n",
    "    \n",
    "    averages[target.name] = prior * ( 1- smoothing) + averages['mean'] * smoothing\n",
    "    averages.drop(['mean','count'],axis=1,inplace=True)\n",
    "    \n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    \n",
    "    ft_val_series = pd.merge(\n",
    "        val_series.to_frame(val_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=val_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    ft_val_series.index = val_series.index\n",
    "    \n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    \n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T02:12:56.989840Z",
     "start_time": "2021-01-27T02:12:51.561566Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('C:/Users/이동훈/Desktop/github/kaggle/kagglestudy/Data/porto/train.csv',na_values='-1')\n",
    "test_df = pd.read_csv('C:/Users/이동훈/Desktop/github/kaggle/kagglestudy/Data/porto/test.csv',na_values='-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T02:42:20.827615Z",
     "start_time": "2021-01-27T02:42:20.822630Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features = [\"ps_car_13\",\"ps_reg_03\",\"ps_ind_05_cat\",\"ps_ind_03\",\n",
    "                  \"ps_ind_15\",\"ps_reg_02\",\"ps_car_14\",\"ps_car_12\",\n",
    "                  \"ps_car_01_cat\",\"ps_car_07_cat\",\"ps_ind_17_bin\",\"ps_car_03_cat\",\n",
    "                  \"ps_reg_01\",\"ps_car_15\",\"ps_ind_01\",\"ps_ind_16_bin\",\n",
    "                  \"ps_ind_07_bin\",\"ps_car_06_cat\",\"ps_car_04_cat\",\"ps_ind_06_bin\",\n",
    "                  \"ps_car_09_cat\",\"ps_car_02_cat\",\"ps_ind_02_cat\",\"ps_car_11\",\n",
    "                  \"ps_car_05_cat\",\"ps_calc_09\",\"ps_calc_05\",\"ps_ind_08_bin\",\n",
    "                  \"ps_car_08_cat\",\"ps_ind_09_bin\",\"ps_ind_04_cat\",\"ps_ind_18_bin\",\n",
    "                  \"ps_ind_12_bin\",\"ps_ind_14\"]\n",
    "combs = [('ps_reg_01','ps_car_02_cat'),\n",
    "        ('ps_reg_01','ps_car_04_cat')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T02:42:29.500147Z",
     "start_time": "2021-01-27T02:42:23.445228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current feature                                 ps_reg_01_plus_ps_car_04_cat    2 in   0.1"
     ]
    }
   ],
   "source": [
    "id_test = test_df['id'].values\n",
    "id_train = train_df['id'].values\n",
    "y = train_df['target']\n",
    "\n",
    "start = time.time()\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.1f'\n",
    "          % (name1, n_c + 1, (time.time() - start) / 60), end='')\n",
    "    print('\\r' * 75, end='')\n",
    "    train_df[name1] = train_df[f1].apply(lambda x: str(x)) + \"_\" + train_df[f2].apply(lambda x: str(x))\n",
    "    test_df[name1] = test_df[f1].apply(lambda x: str(x)) + \"_\" + test_df[f2].apply(lambda x: str(x))\n",
    "    \n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "    train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "    test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "\n",
    "    train_features.append(name1)\n",
    "    \n",
    "X = train_df[train_features]\n",
    "test_df = test_df[train_features]\n",
    "\n",
    "f_cats = [f for f in X.columns if \"_cat\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T02:42:52.046289Z",
     "start_time": "2021-01-27T02:42:52.037309Z"
    }
   },
   "outputs": [],
   "source": [
    "y_valid_pred=0*y\n",
    "y_test_pred=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T02:43:32.858730Z",
     "start_time": "2021-01-27T02:43:32.851750Z"
    }
   },
   "outputs": [],
   "source": [
    "K=5\n",
    "kf = KFold(n_splits=K,random_state=1,shuffle=True)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T02:45:19.710024Z",
     "start_time": "2021-01-27T02:45:19.707030Z"
    }
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(n_estimators=MAX_ROUNDS,\n",
    "                     max_depth=4,\n",
    "                     objective='binary:logistic',\n",
    "                     learning_rate=LEARNING_RATE,\n",
    "                     subsample=.8,\n",
    "                     min_child_weight=6,\n",
    "                     colsample_bytree=.8,\n",
    "                     scale_pos_weight=1.6,\n",
    "                     gamma=10,\n",
    "                     reg_alpha=8,\n",
    "                     reg_lambda=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T03:30:33.407392Z",
     "start_time": "2021-01-27T03:03:55.152338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-e1bbcc8b7298>:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"eval_gini\" failed type inference due to: \u001b[1m\u001b[1mnon-precise type pyobject\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: typing of argument at <ipython-input-3-e1bbcc8b7298> (3)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-3-e1bbcc8b7298>\", line 3:\u001b[0m\n",
      "\u001b[1mdef eval_gini(y_true, y_prob):\n",
      "\u001b[1m    y_true = np.asarray(y_true)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit\n",
      "<ipython-input-3-e1bbcc8b7298>:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"eval_gini\" failed type inference due to: \u001b[1m\u001b[1mcannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-3-e1bbcc8b7298>\", line 9:\u001b[0m\n",
      "\u001b[1mdef eval_gini(y_true, y_prob):\n",
      "    <source elided>\n",
      "    n = len(y_true)\n",
      "\u001b[1m    for i in range(n-1, -1, -1):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "C:\\user\\lib\\site-packages\\numba\\object_mode_passes.py:178: NumbaWarning: \u001b[1mFunction \"eval_gini\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-3-e1bbcc8b7298>\", line 3:\u001b[0m\n",
      "\u001b[1mdef eval_gini(y_true, y_prob):\n",
      "\u001b[1m    y_true = np.asarray(y_true)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n",
      "C:\\user\\lib\\site-packages\\numba\\object_mode_passes.py:188: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-3-e1bbcc8b7298>\", line 3:\u001b[0m\n",
      "\u001b[1mdef eval_gini(y_true, y_prob):\n",
      "\u001b[1m    y_true = np.asarray(y_true)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gini =  0.2856959531750338\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2825270426290394\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.2744124272744569\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29925913576337726\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28468083823013424\n",
      "\n",
      "Gini for full training set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-e1bbcc8b7298>:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"eval_gini\" failed type inference due to: \u001b[1m\u001b[1mnon-precise type pyobject\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: typing of argument at <ipython-input-3-e1bbcc8b7298> (3)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-3-e1bbcc8b7298>\", line 3:\u001b[0m\n",
      "\u001b[1mdef eval_gini(y_true, y_prob):\n",
      "\u001b[1m    y_true = np.asarray(y_true)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit\n",
      "<ipython-input-3-e1bbcc8b7298>:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"eval_gini\" failed type inference due to: \u001b[1m\u001b[1mcannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-3-e1bbcc8b7298>\", line 9:\u001b[0m\n",
      "\u001b[1mdef eval_gini(y_true, y_prob):\n",
      "    <source elided>\n",
      "    n = len(y_true)\n",
      "\u001b[1m    for i in range(n-1, -1, -1):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "C:\\user\\lib\\site-packages\\numba\\object_mode_passes.py:178: NumbaWarning: \u001b[1mFunction \"eval_gini\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-3-e1bbcc8b7298>\", line 3:\u001b[0m\n",
      "\u001b[1mdef eval_gini(y_true, y_prob):\n",
      "\u001b[1m    y_true = np.asarray(y_true)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n",
      "C:\\user\\lib\\site-packages\\numba\\object_mode_passes.py:188: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-3-e1bbcc8b7298>\", line 3:\u001b[0m\n",
      "\u001b[1mdef eval_gini(y_true, y_prob):\n",
      "\u001b[1m    y_true = np.asarray(y_true)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28509183378958614"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "    \n",
    "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "    X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "    X_test = test_df.copy()\n",
    "    print( \"\\nFold \", i)\n",
    "    \n",
    "    for f in f_cats:\n",
    "        X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "                                                        trn_series=X_train[f],\n",
    "                                                        val_series=X_valid[f],\n",
    "                                                        tst_series=X_test[f],\n",
    "                                                        target=y_train,\n",
    "                                                        min_samples_leaf=200,\n",
    "                                                        smoothing=10,\n",
    "                                                        noise_level=0\n",
    "                                                        )\n",
    "    \n",
    "    if OPTIMIZE_ROUNDS:\n",
    "        eval_set=[(X_valid,y_valid)]\n",
    "        fit_model = model.fit( X_train, y_train, \n",
    "                               eval_set=eval_set,\n",
    "                               eval_metric=gini_xgb,\n",
    "                               early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                               verbose=False\n",
    "                             )\n",
    "        print( \"  Best N trees = \", model.best_ntree_limit )\n",
    "        print( \"  Best gini = \", model.best_score )\n",
    "    else:\n",
    "        fit_model = model.fit( X_train, y_train )\n",
    "        \n",
    "    \n",
    "    pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "    print( \"  Gini = \", eval_gini(y_valid, pred) )\n",
    "    y_valid_pred.iloc[test_index] = pred\n",
    "    \n",
    "    y_test_pred += fit_model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    del X_test, X_train, X_valid, y_train\n",
    "    \n",
    "y_test_pred /= K  \n",
    "\n",
    "print( \"\\nGini for full training set:\" )\n",
    "eval_gini(y, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T03:30:51.933333Z",
     "start_time": "2021-01-27T03:30:50.263711Z"
    }
   },
   "outputs": [],
   "source": [
    "val = pd.DataFrame()\n",
    "val['id'] = id_train\n",
    "val['target'] = y_valid_pred.values\n",
    "val.to_csv('xgb_valid.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T03:38:22.464454Z",
     "start_time": "2021-01-27T03:38:17.600987Z"
    }
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = id_test\n",
    "sub['target'] = y_test_pred\n",
    "sub.to_csv('xgb_submit.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2회차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T03:39:24.997704Z",
     "start_time": "2021-01-27T03:39:24.990210Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_ROUNDS = 400\n",
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음에는 MAX_ROUNDS를 상당히 높게 설정하고 OPTIMIZE_ROUNDS를 사용하여 적절한 수의 라운드를 설정하려고 했다.(내 판단으로는 모든 폴드 중에서 best_ntree_limit의 최대 값에 가까워야 하며, 모델이 적절히 정규화되었다면 조금 더 높을 수도 있고... 또는 상세 =을 설정할 수도 있습니다.참이고 세부 사항을 살펴본 후 모든 접힘에 적합한 여러 라운드를 찾아보십시오.) 그런 다음 Optimize_ROUNDs를 끄고 MAX_ROUNDs를 적절한 총 라운드 수로 설정하려 했다.  \n",
    "  \n",
    "각 폴드에 가장 적합한 라운드를 선택하여 \"조기 중지\"할 때의 문제는 검증 데이터에 지나치게 적합하다는 것이다.  \n",
    "  \n",
    "따라서 테스트 데이터를 예측하기 위한 최적의 모델을 생성하지 않을 수 있으며, 다른 모델과의 스택/결합을 위한 검증 데이터를 생성하는 데 사용될 경우 이 모델의 앙상블에 너무 많은 무게가 실리게 된다.  \n",
    "  \n",
    "또 다른 가능성(XGBoost의 경우 기본값인 것 같음)은 최적 라운드가 아닌 조기 스톱이 실제로 발생하는 라운드(개선 부족을 확인하는 지연 시간 포함)를 사용하는 것이다. 이렇게 하면 과적합 문제가 해결되지만(지체가 충분히 길다면) 아직까지는 도움이 되지 않은 것 같다. (모든 접힘에 대해 일정한 회차 수보다 접힘당 20라운드 조기 정지 점수를 더 못 받아 조기 정지가 실제로 맞지 않는 것 같았다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T03:41:05.527830Z",
     "start_time": "2021-01-27T03:41:05.516858Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numba import jit\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T03:44:58.519491Z",
     "start_time": "2021-01-27T03:44:58.507441Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T03:50:51.004490Z",
     "start_time": "2021-01-27T03:50:50.977927Z"
    }
   },
   "outputs": [],
   "source": [
    "def gini_xgb(pred,dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score=-eval_gini(labels,preds)\n",
    "    return[('gini',gini_score)]\n",
    "\n",
    "def add_noise(series,noise_level):\n",
    "    return series * (1+noise_level*np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None,\n",
    "                 val_series=None,\n",
    "                 tst_series=None,\n",
    "                 target=None,\n",
    "                 min_samples_leaf=1,\n",
    "                 smoothing=1,\n",
    "                 noise_level=0):\n",
    "    assert len(trn_series)==len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series,target],axis=1)\n",
    "    \n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg(['mean','count'])\n",
    "    \n",
    "    smoothing = 1 / (1+np.exp(-(averages['count']-min_samples_leaf)/smoothing))\n",
    "    \n",
    "    prior = target.mean()\n",
    "    \n",
    "    averages[target.name] = prior * ( 1- smoothing) + averages['mean'] * smoothing\n",
    "    averages.drop(['mean','count'],axis=1,inplace=True)\n",
    "    \n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    \n",
    "    ft_val_series = pd.merge(\n",
    "        val_series.to_frame(val_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=val_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    ft_val_series.index = val_series.index\n",
    "    \n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    \n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T03:58:07.347243Z",
     "start_time": "2021-01-27T03:57:55.367683Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('C:/Users/이동훈/Desktop/github/kaggle/kagglestudy/Data/porto/train.csv',na_values='-1')\n",
    "test_df = pd.read_csv('C:/Users/이동훈/Desktop/github/kaggle/kagglestudy/Data/porto/test.csv',na_values='-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T04:02:18.065074Z",
     "start_time": "2021-01-27T04:02:18.052110Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features = [\"ps_car_13\",\"ps_reg_03\",\"ps_ind_05_cat\",\"ps_ind_03\",\n",
    "                  \"ps_ind_15\",\"ps_reg_02\",\"ps_car_14\",\"ps_car_12\",\n",
    "                  \"ps_car_01_cat\",\"ps_car_07_cat\",\"ps_ind_17_bin\",\"ps_car_03_cat\",\n",
    "                  \"ps_reg_01\",\"ps_car_15\",\"ps_ind_01\",\"ps_ind_16_bin\",\n",
    "                  \"ps_ind_07_bin\",\"ps_car_06_cat\",\"ps_car_04_cat\",\"ps_ind_06_bin\",\n",
    "                  \"ps_car_09_cat\",\"ps_car_02_cat\",\"ps_ind_02_cat\",\"ps_car_11\",\n",
    "                  \"ps_car_05_cat\",\"ps_calc_09\",\"ps_calc_05\",\"ps_ind_08_bin\",\n",
    "                  \"ps_car_08_cat\",\"ps_ind_09_bin\",\"ps_ind_04_cat\",\"ps_ind_18_bin\",\n",
    "                  \"ps_ind_12_bin\",\"ps_ind_14\"]\n",
    "combs = [('ps_reg_01','ps_car_02_cat'),\n",
    "        ('ps_reg_01','ps_car_04_cat')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T04:08:07.142872Z",
     "start_time": "2021-01-27T04:07:53.724379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current feature                                 ps_reg_01_plus_ps_car_04_cat    2 in   0.1"
     ]
    }
   ],
   "source": [
    "id_test = test_df['id'].values\n",
    "id_train = train_df['id'].values\n",
    "y = train_df['target']\n",
    "\n",
    "start = time.time()\n",
    "for n_c, (f1,f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.1f'%(name1,n_c+1,(time.time()-start)/60),end='')\n",
    "    print('\\r'*75,end='')\n",
    "    train_df[name1] = train_df[f1].apply(lambda x:str(x)) + '_' +train_df[f2].apply(lambda x:str(x))\n",
    "    test_df[name1] = test_df[f1].apply(lambda x:str(x)) + '_' +test_df[f2].apply(lambda x:str(x))\n",
    "    \n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train_df[name1].values)+list(test_df[name1].values))\n",
    "    train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "    test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "    \n",
    "    train_features.append(name1)\n",
    "    \n",
    "X= train_df[train_features]\n",
    "test_df = test_df[train_features]\n",
    "\n",
    "f_cats = [f for f in X.columns if '_cat'in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T04:11:03.251824Z",
     "start_time": "2021-01-27T04:11:03.237861Z"
    }
   },
   "outputs": [],
   "source": [
    "y_valid_pred = 0*y\n",
    "y_test_pred = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T04:12:40.870151Z",
     "start_time": "2021-01-27T04:12:40.854583Z"
    }
   },
   "outputs": [],
   "source": [
    "K=5\n",
    "kf = KFold(n_splits=K,random_state=1,shuffle=True)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T04:19:02.346681Z",
     "start_time": "2021-01-27T04:19:02.329611Z"
    }
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(n_estimators=MAX_ROUNDS,\n",
    "                     max_depth=4,\n",
    "                     objective='binary:logistic',\n",
    "                     learning_rate=LEARNING_RATE,\n",
    "                     subsample=.8,\n",
    "                     min_child_weight=6,\n",
    "                     colsample_bytree=.8,\n",
    "                     scale_pos_weight=1.6,\n",
    "                     gamma=10,\n",
    "                     reg_alpha=8,\n",
    "                     reg_lambda=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T05:07:40.103029Z",
     "start_time": "2021-01-27T04:19:09.029923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-e1bbcc8b7298>:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"eval_gini\" failed type inference due to: \u001b[1m\u001b[1mnon-precise type pyobject\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: typing of argument at <ipython-input-30-e1bbcc8b7298> (3)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-30-e1bbcc8b7298>\", line 3:\u001b[0m\n",
      "\u001b[1mdef eval_gini(y_true, y_prob):\n",
      "\u001b[1m    y_true = np.asarray(y_true)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit\n",
      "<ipython-input-30-e1bbcc8b7298>:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"eval_gini\" failed type inference due to: \u001b[1m\u001b[1mcannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-30-e1bbcc8b7298>\", line 9:\u001b[0m\n",
      "\u001b[1mdef eval_gini(y_true, y_prob):\n",
      "    <source elided>\n",
      "    n = len(y_true)\n",
      "\u001b[1m    for i in range(n-1, -1, -1):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "C:\\user\\lib\\site-packages\\numba\\object_mode_passes.py:178: NumbaWarning: \u001b[1mFunction \"eval_gini\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-30-e1bbcc8b7298>\", line 3:\u001b[0m\n",
      "\u001b[1mdef eval_gini(y_true, y_prob):\n",
      "\u001b[1m    y_true = np.asarray(y_true)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n",
      "C:\\user\\lib\\site-packages\\numba\\object_mode_passes.py:188: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-30-e1bbcc8b7298>\", line 3:\u001b[0m\n",
      "\u001b[1mdef eval_gini(y_true, y_prob):\n",
      "\u001b[1m    y_true = np.asarray(y_true)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gini =  0.2856959531750338\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.2825270426290394\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.2744124272744569\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.29925913576337726\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28468083823013424\n",
      "\n",
      "Gini for full training set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-e1bbcc8b7298>:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"eval_gini\" failed type inference due to: \u001b[1m\u001b[1mnon-precise type pyobject\u001b[0m\n",
      "\u001b[0m\u001b[1m[1] During: typing of argument at <ipython-input-30-e1bbcc8b7298> (3)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-30-e1bbcc8b7298>\", line 3:\u001b[0m\n",
      "\u001b[1mdef eval_gini(y_true, y_prob):\n",
      "\u001b[1m    y_true = np.asarray(y_true)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit\n",
      "<ipython-input-30-e1bbcc8b7298>:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"eval_gini\" failed type inference due to: \u001b[1m\u001b[1mcannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"<ipython-input-30-e1bbcc8b7298>\", line 9:\u001b[0m\n",
      "\u001b[1mdef eval_gini(y_true, y_prob):\n",
      "    <source elided>\n",
      "    n = len(y_true)\n",
      "\u001b[1m    for i in range(n-1, -1, -1):\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit\n",
      "C:\\user\\lib\\site-packages\\numba\\object_mode_passes.py:178: NumbaWarning: \u001b[1mFunction \"eval_gini\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"<ipython-input-30-e1bbcc8b7298>\", line 3:\u001b[0m\n",
      "\u001b[1mdef eval_gini(y_true, y_prob):\n",
      "\u001b[1m    y_true = np.asarray(y_true)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n",
      "C:\\user\\lib\\site-packages\\numba\\object_mode_passes.py:188: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"<ipython-input-30-e1bbcc8b7298>\", line 3:\u001b[0m\n",
      "\u001b[1mdef eval_gini(y_true, y_prob):\n",
      "\u001b[1m    y_true = np.asarray(y_true)\n",
      "\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28509183378958614"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "    \n",
    "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "    X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "    X_test = test_df.copy()\n",
    "    print( \"\\nFold \", i)\n",
    "    \n",
    "    for f in f_cats:\n",
    "        X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "                                                        trn_series=X_train[f],\n",
    "                                                        val_series=X_valid[f],\n",
    "                                                        tst_series=X_test[f],\n",
    "                                                        target=y_train,\n",
    "                                                        min_samples_leaf=200,\n",
    "                                                        smoothing=10,\n",
    "                                                        noise_level=0\n",
    "                                                        )\n",
    "    \n",
    "    if OPTIMIZE_ROUNDS:\n",
    "        eval_set=[(X_valid,y_valid)]\n",
    "        fit_model = model.fit( X_train, y_train, \n",
    "                               eval_set=eval_set,\n",
    "                               eval_metric=gini_xgb,\n",
    "                               early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                               verbose=False\n",
    "                             )\n",
    "        print( \"  Best N trees = \", model.best_ntree_limit )\n",
    "        print( \"  Best gini = \", model.best_score )\n",
    "    else:\n",
    "        fit_model = model.fit( X_train, y_train )\n",
    "        \n",
    "    \n",
    "    pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "    print( \"  Gini = \", eval_gini(y_valid, pred) )\n",
    "    y_valid_pred.iloc[test_index] = pred\n",
    "    \n",
    "    y_test_pred += fit_model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    del X_test, X_train, X_valid, y_train\n",
    "    \n",
    "y_test_pred /= K  \n",
    "\n",
    "print( \"\\nGini for full training set:\" )\n",
    "eval_gini(y, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T05:09:14.239031Z",
     "start_time": "2021-01-27T05:09:10.074430Z"
    }
   },
   "outputs": [],
   "source": [
    "val = pd.DataFrame()\n",
    "val['id'] = id_train\n",
    "val['target'] = y_valid_pred.values\n",
    "val.to_csv('xgb_valid.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T05:15:52.793889Z",
     "start_time": "2021-01-27T05:15:46.841959Z"
    }
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = id_test\n",
    "sub['target'] = y_test_pred\n",
    "sub.to_csv('xgb_submit.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
