{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- competition/dataset : [https://www.kaggle.com/c/spooky-author-identification](https://www.kaggle.com/c/spooky-author-identification)\n",
    "- date : 2021/03/10\n",
    "- original : [https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaching (Almost) Any NLP Problem on Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-19T23:23:23.804833Z",
     "iopub.status.busy": "2021-01-19T23:23:23.803834Z",
     "iopub.status.idle": "2021-01-19T23:23:23.811814Z",
     "shell.execute_reply": "2021-01-19T23:23:23.810817Z",
     "shell.execute_reply.started": "2021-01-19T23:23:23.804833Z"
    }
   },
   "source": [
    "**✏ 필사 1회** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 포스팅에서는 Kaggle의 NLP 문제에 대해 이야기하고자 합니다. 예제로 우리는 이 대회에서 주어진 데이터를 사용합니다. 먼저 기본 모델을 만들고, 다른 feature들을 이용하여 개선합니다. 또한 DNN이 어떻게 사용되는지 확인하고, 일반적인 앙상블에 대한 몇 가지 아이디어로 포스팅을 마무리할 예정입니다.  \n",
    "\n",
    "**This covers:**  \n",
    "* tfidf\n",
    "* count features\n",
    "* logistic regression\n",
    "* naive bayes\n",
    "* svm\n",
    "* xgboost\n",
    "* grid search\n",
    "* word vectors\n",
    "* LSTM\n",
    "* GRU\n",
    "* Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:16:56.773885Z",
     "iopub.status.busy": "2021-03-10T22:16:56.772888Z",
     "iopub.status.idle": "2021-03-10T22:17:15.887767Z",
     "shell.execute_reply": "2021-03-10T22:17:15.886769Z",
     "shell.execute_reply.started": "2021-03-10T22:16:56.772888Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:17:15.889762Z",
     "iopub.status.busy": "2021-03-10T22:17:15.888764Z",
     "iopub.status.idle": "2021-03-10T22:17:16.179986Z",
     "shell.execute_reply": "2021-03-10T22:17:16.178987Z",
     "shell.execute_reply.started": "2021-03-10T22:17:15.889762Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "sample = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:17:16.182978Z",
     "iopub.status.busy": "2021-03-10T22:17:16.182978Z",
     "iopub.status.idle": "2021-03-10T22:17:16.232843Z",
     "shell.execute_reply": "2021-03-10T22:17:16.231845Z",
     "shell.execute_reply.started": "2021-03-10T22:17:16.182978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:17:16.234838Z",
     "iopub.status.busy": "2021-03-10T22:17:16.234838Z",
     "iopub.status.idle": "2021-03-10T22:17:16.258774Z",
     "shell.execute_reply": "2021-03-10T22:17:16.257777Z",
     "shell.execute_reply.started": "2021-03-10T22:17:16.234838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:17:16.260770Z",
     "iopub.status.busy": "2021-03-10T22:17:16.259773Z",
     "iopub.status.idle": "2021-03-10T22:17:16.290690Z",
     "shell.execute_reply": "2021-03-10T22:17:16.288696Z",
     "shell.execute_reply.started": "2021-03-10T22:17:16.260770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.403494  0.287808  0.308698\n",
       "1  id24541  0.403494  0.287808  0.308698\n",
       "2  id00134  0.403494  0.287808  0.308698\n",
       "3  id27757  0.403494  0.287808  0.308698\n",
       "4  id04081  0.403494  0.287808  0.308698"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 문제는 텍스트에 주어진 EAP, HPL, MWS 저자를 예측해야 합니다. 간단히 말해서, 텍스트는 세 가지의 클래스로 분류되어 있습니다.  \n",
    "\n",
    "이 문제에 대해 Kaggle은 multi-class log-loss를 평가 메트릭으로 지정했습니다. 이는 다음과 같은 방법으로 구현됩니다: [https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/util.py](https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/util.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:17:16.291688Z",
     "iopub.status.busy": "2021-03-10T22:17:16.291688Z",
     "iopub.status.idle": "2021-03-10T22:17:16.304652Z",
     "shell.execute_reply": "2021-03-10T22:17:16.303655Z",
     "shell.execute_reply.started": "2021-03-10T22:17:16.291688Z"
    }
   },
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    '''\n",
    "    log loss 메트릭의 멀티 클래스 버전\n",
    "    :param actual: 실제 타겟 클래스를 포함한 array\n",
    "    :param predicted: 클래스 예측 값이 포함된 matrix, 한 클래스당 하나의 확률\n",
    "    '''\n",
    "    # actual이 binary array가 아니면 변환\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "    clip = np.clip(predicted, eps, 1-eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text label을 정수형 0, 1, 2로 변환하기 위해 scikit-learn의 `LabelEncoder`를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:17:16.305650Z",
     "iopub.status.busy": "2021-03-10T22:17:16.305650Z",
     "iopub.status.idle": "2021-03-10T22:17:16.320609Z",
     "shell.execute_reply": "2021-03-10T22:17:16.319611Z",
     "shell.execute_reply.started": "2021-03-10T22:17:16.305650Z"
    }
   },
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train['author'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 진행하기 앞서 훈련 데이터 셋과 검증 데이터 셋을 분리하는 것이 중요합니다. scikit-learn 모듈의 `model_selection`의 `train_test_split`을 사용하여 실행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:17:16.327591Z",
     "iopub.status.busy": "2021-03-10T22:17:16.325598Z",
     "iopub.status.idle": "2021-03-10T22:17:16.368482Z",
     "shell.execute_reply": "2021-03-10T22:17:16.367483Z",
     "shell.execute_reply.started": "2021-03-10T22:17:16.327591Z"
    }
   },
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(\n",
    "    train['text'].values, y, stratify=y, random_state=42,\n",
    "    test_size=0.1, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:17:16.370476Z",
     "iopub.status.busy": "2021-03-10T22:17:16.370476Z",
     "iopub.status.idle": "2021-03-10T22:17:16.385436Z",
     "shell.execute_reply": "2021-03-10T22:17:16.383441Z",
     "shell.execute_reply.started": "2021-03-10T22:17:16.370476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Basic Models\n",
    "첫 번째 모델을 생성해봅시다.  \n",
    "\n",
    "우리의 첫 모델은 TF-IDF(Tearm Frequency - Inverse Document Frequency)와 간단한 로지스틱 회귀입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:17:16.386434Z",
     "iopub.status.busy": "2021-03-10T22:17:16.386434Z",
     "iopub.status.idle": "2021-03-10T22:17:18.722186Z",
     "shell.execute_reply": "2021-03-10T22:17:18.721189Z",
     "shell.execute_reply.started": "2021-03-10T22:17:16.386434Z"
    }
   },
   "outputs": [],
   "source": [
    "# 항상 이 feature들로 시작\n",
    "tfv = TfidfVectorizer(\n",
    "    min_df=3, max_features=None, strip_accents='unicode',\n",
    "    analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 3),\n",
    "    use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words='english'\n",
    ")\n",
    "\n",
    "# training과 test 셋에 TF-IDF fitting (준지도학습)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv = tfv.transform(xtrain)\n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:17:18.724182Z",
     "iopub.status.busy": "2021-03-10T22:17:18.724182Z",
     "iopub.status.idle": "2021-03-10T22:17:20.457545Z",
     "shell.execute_reply": "2021-03-10T22:17:20.456548Z",
     "shell.execute_reply.started": "2021-03-10T22:17:18.724182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.572\n"
     ]
    }
   ],
   "source": [
    "# TFIDF에 로지스틱 회귀 적용\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "print('logloss: %0.3f'%multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 모델의 multiclass logloss는 0.572입니다.  \n",
    "\n",
    "동일한 모델을 다른 데이터로 살펴보겠습니다.  \n",
    "\n",
    "TF-DIF 대신에 단어 수를 feature로 사용할 수 있습니다. scikit-learn의 `CountVectorizer`을 사용하면 간단하게 해결할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:17:20.459540Z",
     "iopub.status.busy": "2021-03-10T22:17:20.458542Z",
     "iopub.status.idle": "2021-03-10T22:17:24.484775Z",
     "shell.execute_reply": "2021-03-10T22:17:24.483777Z",
     "shell.execute_reply.started": "2021-03-10T22:17:20.459540Z"
    }
   },
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(\n",
    "    analyzer='word', token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 3), stop_words='english'\n",
    ")\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv = ctv.transform(xtrain)\n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:17:24.487768Z",
     "iopub.status.busy": "2021-03-10T22:17:24.486770Z",
     "iopub.status.idle": "2021-03-10T22:17:45.992255Z",
     "shell.execute_reply": "2021-03-10T22:17:45.991258Z",
     "shell.execute_reply.started": "2021-03-10T22:17:24.486770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.527\n"
     ]
    }
   ],
   "source": [
    "# Counts에 로지스틱 회귀 적용\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "print('logloss: %0.3f'%multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.048 개선되었습니다.  \n",
    "\n",
    "다음으로, 예전에 꽤 유명했던 Naive Bayes 모델을 시도해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:17:45.994249Z",
     "iopub.status.busy": "2021-03-10T22:17:45.993253Z",
     "iopub.status.idle": "2021-03-10T22:17:46.024169Z",
     "shell.execute_reply": "2021-03-10T22:17:46.023173Z",
     "shell.execute_reply.started": "2021-03-10T22:17:45.994249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.578\n"
     ]
    }
   ],
   "source": [
    "# TFIDF에 Naive Bayes 적용\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "print('logloss: %0.3f'%multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts에 로지스틱 회귀를 적용한 것이 더 성능이 좋습니다. 위 모델을 counts data에 사용하면 어떻게 될까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:17:46.027162Z",
     "iopub.status.busy": "2021-03-10T22:17:46.026164Z",
     "iopub.status.idle": "2021-03-10T22:17:46.117920Z",
     "shell.execute_reply": "2021-03-10T22:17:46.116924Z",
     "shell.execute_reply.started": "2021-03-10T22:17:46.026164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.485\n"
     ]
    }
   ],
   "source": [
    "# Counts에 Naive Bayes 적용\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "print('logloss: %0.3f'%multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오래된 것이 더 잘 작동합니다!  \n",
    "\n",
    "오래된 알고리즘 중에 SVM이 있으며, 일부 사람들은 SVM을 매우 사랑합니다. 따라서, 데이터 셋에 SVM을 시도해보겠습니다.  \n",
    "\n",
    "SVM은 시간이 오래 걸리기 때문에 Singular Value Decomposition을 사용하여 TF-IDF의 feature의 수를 줄이겠습니다.  \n",
    "\n",
    "SVM을 적용하기 전에 반드시 표준화를 해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:17:46.122907Z",
     "iopub.status.busy": "2021-03-10T22:17:46.120911Z",
     "iopub.status.idle": "2021-03-10T22:17:49.036114Z",
     "shell.execute_reply": "2021-03-10T22:17:49.036114Z",
     "shell.execute_reply.started": "2021-03-10T22:17:46.122907Z"
    }
   },
   "outputs": [],
   "source": [
    "# SVD를 적용하여 120개의 컴포넌트 선택\n",
    "# SVM 모델에는 120 - 200개의 컴포넌트가 적절함\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "# SVD로 얻은 데이터 스케일링\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 SVM을 적용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:17:49.038109Z",
     "iopub.status.busy": "2021-03-10T22:17:49.037111Z",
     "iopub.status.idle": "2021-03-10T22:22:52.708562Z",
     "shell.execute_reply": "2021-03-10T22:22:52.707564Z",
     "shell.execute_reply.started": "2021-03-10T22:17:49.038109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.734\n"
     ]
    }
   ],
   "source": [
    "# SVM fitting\n",
    "# 오!래!걸!림!\n",
    "clf = SVC(C=1.0, probability=True) # 확률 값을 구하는 것이기 때문\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "print('logloss: %0.3f'%multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능이 좋지 않습니다. \n",
    "\n",
    "또 인기있는 알고리즘 중 하나인 xgboost를 적용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:22:52.710557Z",
     "iopub.status.busy": "2021-03-10T22:22:52.709560Z",
     "iopub.status.idle": "2021-03-10T22:23:09.643400Z",
     "shell.execute_reply": "2021-03-10T22:23:09.642402Z",
     "shell.execute_reply.started": "2021-03-10T22:22:52.709560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:22:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.781\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF에 xgboost 적용\n",
    "clf = xgb.XGBClassifier(\n",
    "    max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8,\n",
    "    nthread=10, learning_rate=0.1\n",
    ")\n",
    "clf.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "\n",
    "print ('logloss: %0.3f'% multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:23:09.644397Z",
     "iopub.status.busy": "2021-03-10T22:23:09.644397Z",
     "iopub.status.idle": "2021-03-10T22:24:50.019152Z",
     "shell.execute_reply": "2021-03-10T22:24:50.019152Z",
     "shell.execute_reply.started": "2021-03-10T22:23:09.644397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:23:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.772\n"
     ]
    }
   ],
   "source": [
    "# Counts에 xgboost 적용\n",
    "clf = xgb.XGBClassifier(\n",
    "    max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8,\n",
    "    nthread=10, learning_rate=0.1\n",
    ")\n",
    "clf.fit(xtrain_ctv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv.tocsc())\n",
    "\n",
    "print ('logloss: %0.3f'% multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:24:50.021147Z",
     "iopub.status.busy": "2021-03-10T22:24:50.021147Z",
     "iopub.status.idle": "2021-03-10T22:26:09.816604Z",
     "shell.execute_reply": "2021-03-10T22:26:09.815606Z",
     "shell.execute_reply.started": "2021-03-10T22:24:50.021147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:24:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.770\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF svd feature에 xgboost 적용\n",
    "clf = xgb.XGBClassifier(\n",
    "    max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8,\n",
    "    nthread=10, learning_rate=0.1\n",
    ")\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print ('logloss: %0.3f'% multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:26:09.817602Z",
     "iopub.status.busy": "2021-03-10T22:26:09.817602Z",
     "iopub.status.idle": "2021-03-10T22:26:59.377573Z",
     "shell.execute_reply": "2021-03-10T22:26:59.376576Z",
     "shell.execute_reply.started": "2021-03-10T22:26:09.817602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:26:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.803\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF svd feature에 xgboost 적용\n",
    "clf = xgb.XGBClassifier(nthread=10)\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print ('logloss: %0.3f'% multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "이것은 하이퍼파라미터 최적화를 위한 기술입니다. 효과적이지는 않지만 사용하고자 하는 그리드를 알고 있다면 좋은 결과를 얻을 수 있습니다. 다음 포스트에 일반적으로 사용해야 할 파라미터에 대해 정리해 두었습니다: [http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/](http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/)  \n",
    "\n",
    "이 섹션에서는 로지스틱 회귀를 사용한 grid search에 대해 이야기하겠습니다.  \n",
    "\n",
    "grid search를 시작하기 전에 먼저 스코어링 함수를 만들어야 합니다. scikit-learn의 `make_scorer` 함수를 사용하여 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:26:59.379569Z",
     "iopub.status.busy": "2021-03-10T22:26:59.378572Z",
     "iopub.status.idle": "2021-03-10T22:26:59.407493Z",
     "shell.execute_reply": "2021-03-10T22:26:59.406496Z",
     "shell.execute_reply.started": "2021-03-10T22:26:59.379569Z"
    }
   },
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(\n",
    "    multiclass_logloss, greater_is_better=False, needs_proba=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 파이프라인이 필요합니다. 시연을 위해 SVD, 스케일링, 로지스틱 회귀로 구성된 파이프라인을 사용합니다. 많은 모듈들을 파이프라인에 적용하여 이해하는 것이 하나만 사용하는 것보다 더 낫습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:26:59.410486Z",
     "iopub.status.busy": "2021-03-10T22:26:59.410486Z",
     "iopub.status.idle": "2021-03-10T22:26:59.422454Z",
     "shell.execute_reply": "2021-03-10T22:26:59.421456Z",
     "shell.execute_reply.started": "2021-03-10T22:26:59.410486Z"
    }
   },
   "outputs": [],
   "source": [
    "# SVD 초기화\n",
    "svd = TruncatedSVD()\n",
    "\n",
    "# Standard Scaler 초기화\n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "# 로지스틱 회귀 초기화\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# 파이프라인 생성\n",
    "clf = pipeline.Pipeline([('svd', svd), ('scl', scl), ('lr', lr_model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파라미터 그리드가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:26:59.424449Z",
     "iopub.status.busy": "2021-03-10T22:26:59.423451Z",
     "iopub.status.idle": "2021-03-10T22:26:59.437415Z",
     "shell.execute_reply": "2021-03-10T22:26:59.436416Z",
     "shell.execute_reply.started": "2021-03-10T22:26:59.423451Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components':[120, 180],\n",
    "              'lr__C':[0.1, 1.0, 10],\n",
    "              'lr__penalty':['l1', 'l2']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD는 120개와 180개의 컴포넌트를 평가하고, 로지스틱 회귀의 경우 l1과 l2 패널티로 세 가지 다른 C의 값들을 평가합니다. 이제 이 파라미터들로 grid search를 시작할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:26:59.439409Z",
     "iopub.status.busy": "2021-03-10T22:26:59.438413Z",
     "iopub.status.idle": "2021-03-10T22:27:31.720186Z",
     "shell.execute_reply": "2021-03-10T22:27:31.719189Z",
     "shell.execute_reply.started": "2021-03-10T22:26:59.439409Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
      "[CV 1/2; 1/12] START lr__C=0.1, lr__penalty=l1, svd__n_components=120...........\n",
      "[CV 1/2; 1/12] END lr__C=0.1, lr__penalty=l1, svd__n_components=120; total time=   1.2s\n",
      "[CV 2/2; 1/12] START lr__C=0.1, lr__penalty=l1, svd__n_components=120...........\n",
      "[CV 2/2; 1/12] END lr__C=0.1, lr__penalty=l1, svd__n_components=120; total time=   0.7s\n",
      "[CV 1/2; 2/12] START lr__C=0.1, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 1/2; 2/12] END lr__C=0.1, lr__penalty=l1, svd__n_components=180; total time=   1.2s\n",
      "[CV 2/2; 2/12] START lr__C=0.1, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 2/2; 2/12] END lr__C=0.1, lr__penalty=l1, svd__n_components=180; total time=   1.2s\n",
      "[CV 1/2; 3/12] START lr__C=0.1, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 1/2; 3/12] END lr__C=0.1, lr__penalty=l2, svd__n_components=120; total time=   1.1s\n",
      "[CV 2/2; 3/12] START lr__C=0.1, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 2/2; 3/12] END lr__C=0.1, lr__penalty=l2, svd__n_components=120; total time=   1.1s\n",
      "[CV 1/2; 4/12] START lr__C=0.1, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 1/2; 4/12] END lr__C=0.1, lr__penalty=l2, svd__n_components=180; total time=   1.4s\n",
      "[CV 2/2; 4/12] START lr__C=0.1, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 2/2; 4/12] END lr__C=0.1, lr__penalty=l2, svd__n_components=180; total time=   1.4s\n",
      "[CV 1/2; 5/12] START lr__C=1.0, lr__penalty=l1, svd__n_components=120...........\n",
      "[CV 1/2; 5/12] END lr__C=1.0, lr__penalty=l1, svd__n_components=120; total time=   0.8s\n",
      "[CV 2/2; 5/12] START lr__C=1.0, lr__penalty=l1, svd__n_components=120...........\n",
      "[CV 2/2; 5/12] END lr__C=1.0, lr__penalty=l1, svd__n_components=120; total time=   0.8s\n",
      "[CV 1/2; 6/12] START lr__C=1.0, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 1/2; 6/12] END lr__C=1.0, lr__penalty=l1, svd__n_components=180; total time=   1.1s\n",
      "[CV 2/2; 6/12] START lr__C=1.0, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 2/2; 6/12] END lr__C=1.0, lr__penalty=l1, svd__n_components=180; total time=   1.1s\n",
      "[CV 1/2; 7/12] START lr__C=1.0, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 1/2; 7/12] END lr__C=1.0, lr__penalty=l2, svd__n_components=120; total time=   1.0s\n",
      "[CV 2/2; 7/12] START lr__C=1.0, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 2/2; 7/12] END lr__C=1.0, lr__penalty=l2, svd__n_components=120; total time=   0.9s\n",
      "[CV 1/2; 8/12] START lr__C=1.0, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 1/2; 8/12] END lr__C=1.0, lr__penalty=l2, svd__n_components=180; total time=   1.5s\n",
      "[CV 2/2; 8/12] START lr__C=1.0, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 2/2; 8/12] END lr__C=1.0, lr__penalty=l2, svd__n_components=180; total time=   1.4s\n",
      "[CV 1/2; 9/12] START lr__C=10, lr__penalty=l1, svd__n_components=120............\n",
      "[CV 1/2; 9/12] END lr__C=10, lr__penalty=l1, svd__n_components=120; total time=   0.8s\n",
      "[CV 2/2; 9/12] START lr__C=10, lr__penalty=l1, svd__n_components=120............\n",
      "[CV 2/2; 9/12] END lr__C=10, lr__penalty=l1, svd__n_components=120; total time=   1.0s\n",
      "[CV 1/2; 10/12] START lr__C=10, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 1/2; 10/12] END lr__C=10, lr__penalty=l1, svd__n_components=180; total time=   1.3s\n",
      "[CV 2/2; 10/12] START lr__C=10, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 2/2; 10/12] END lr__C=10, lr__penalty=l1, svd__n_components=180; total time=   1.2s\n",
      "[CV 1/2; 11/12] START lr__C=10, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 1/2; 11/12] END lr__C=10, lr__penalty=l2, svd__n_components=120; total time=   0.9s\n",
      "[CV 2/2; 11/12] START lr__C=10, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 2/2; 11/12] END lr__C=10, lr__penalty=l2, svd__n_components=120; total time=   1.0s\n",
      "[CV 1/2; 12/12] START lr__C=10, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 1/2; 12/12] END lr__C=10, lr__penalty=l2, svd__n_components=180; total time=   1.4s\n",
      "[CV 2/2; 12/12] START lr__C=10, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 2/2; 12/12] END lr__C=10, lr__penalty=l2, svd__n_components=180; total time=   1.7s\n",
      "Best score: -0.741\n",
      "Best parameters set:\n",
      "\tlr__C: 10\n",
      "\tlr__penalty: 'l2'\n",
      "\tsvd__n_components: 180\n"
     ]
    }
   ],
   "source": [
    "# Grid Search 모델 초기화\n",
    "model = GridSearchCV(\n",
    "    estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "    verbose=10, n_jobs=1, refit=True, cv=2\n",
    ")\n",
    "\n",
    "# fitting\n",
    "model.fit(xtrain_tfv, ytrain) # 전체 데이터를 사용해도 됨\n",
    "print('Best score: %0.3f'%model.best_score_)\n",
    "print('Best parameters set:')\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print('\\t%s: %r'%(param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "점수는 SVM과 비슷합니다. 이 기술은 다음과 같이 xgboost나 다항 naive bayes를 정교하게 튜닝하는데 사용될 수 있습니다. 여기서는 tfidf 데이터를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:27:31.721183Z",
     "iopub.status.busy": "2021-03-10T22:27:31.721183Z",
     "iopub.status.idle": "2021-03-10T22:27:34.941266Z",
     "shell.execute_reply": "2021-03-10T22:27:34.940271Z",
     "shell.execute_reply.started": "2021-03-10T22:27:31.721183Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "Best score: -0.492\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "# 파이프라인 생성\n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha':[0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Grid Search 모델 초기화\n",
    "model = GridSearchCV(\n",
    "    estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "    verbose=10, n_jobs=-1, refit=True, cv=2\n",
    ")\n",
    "\n",
    "# fitting\n",
    "model.fit(xtrain_tfv, ytrain) # 전체 데이터를 사용해도 됨\n",
    "print('Best score: %0.3f'%model.best_score_)\n",
    "print('Best parameters set:')\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print('\\t%s: %r'%(param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP 문제에서는 일반적으로 단어 벡터를 살펴봅니다. 단어 벡터는 데이터에 대한 많은 인사이트를 제공합니다. 살펴봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word Vectors\n",
    "깊게 들어가지 않고, 어떻게 문장 벡터를 만들고 이것을 머신러닝 모델에 어떻게 사용하는지에 대해 설명해보겠습니다. 저는 GloVe vertors, word2vec, fasttext를 굉장히 좋아합니다. 이 포스트에서는 GloVe vectors를 사용할 것입니다. 이것은 [http://www-nlp.stanford.edu/data/glove.840B.300d.zip](http://www-nlp.stanford.edu/data/glove.840B.300d.zip)에서 다운받을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:27:34.943260Z",
     "iopub.status.busy": "2021-03-10T22:27:34.942263Z",
     "iopub.status.idle": "2021-03-10T22:31:31.152647Z",
     "shell.execute_reply": "2021-03-10T22:31:31.151648Z",
     "shell.execute_reply.started": "2021-03-10T22:27:34.943260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d23dd66c91d4eafb192ff5a5b59631f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195864 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# 딕셔너리에 GloVe vector 로드\n",
    "embeddings_index = {}\n",
    "f = open('data/glove.840B.300d.txt', encoding='utf-8')\n",
    "for line in tqdm_notebook(f):\n",
    "    try:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        f.__next__()\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.'%len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:31:31.154641Z",
     "iopub.status.busy": "2021-03-10T22:31:31.153644Z",
     "iopub.status.idle": "2021-03-10T22:31:31.167607Z",
     "shell.execute_reply": "2021-03-10T22:31:31.166608Z",
     "shell.execute_reply.started": "2021-03-10T22:31:31.154641Z"
    }
   },
   "outputs": [],
   "source": [
    "# 전체 문장에 대해 표준화된 벡터를 생성하는 함수\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt(v**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:31:31.169600Z",
     "iopub.status.busy": "2021-03-10T22:31:31.168604Z",
     "iopub.status.idle": "2021-03-10T22:31:38.301213Z",
     "shell.execute_reply": "2021-03-10T22:31:38.300215Z",
     "shell.execute_reply.started": "2021-03-10T22:31:31.169600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ffb07e2d4b49eebb75a8574adf354c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9466b609cb004a70978f24aba20ada2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training과 validation 데이터 셋에 대해 위 함수를 적용하여 문장 벡터 생성\n",
    "xtrain_glove = np.array([sent2vec(x) for x in tqdm_notebook(xtrain)])\n",
    "xvalid_glove = np.array([sent2vec(x) for x in tqdm_notebook(xvalid)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove feature에 대해 xgboost의 성능을 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:31:38.303213Z",
     "iopub.status.busy": "2021-03-10T22:31:38.302211Z",
     "iopub.status.idle": "2021-03-10T22:33:37.698016Z",
     "shell.execute_reply": "2021-03-10T22:33:37.697018Z",
     "shell.execute_reply.started": "2021-03-10T22:31:38.303213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:31:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:31:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.720\n"
     ]
    }
   ],
   "source": [
    "# glove feature에 xgboost fitting 1\n",
    "clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "print('logloss: %0.3f'%multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:33:37.699013Z",
     "iopub.status.busy": "2021-03-10T22:33:37.699013Z",
     "iopub.status.idle": "2021-03-10T22:37:09.955546Z",
     "shell.execute_reply": "2021-03-10T22:37:09.954549Z",
     "shell.execute_reply.started": "2021-03-10T22:33:37.699013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:33:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[07:33:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.679\n"
     ]
    }
   ],
   "source": [
    "# glove feature에 xgboost fitting 2\n",
    "clf = xgb.XGBClassifier(\n",
    "    max_depth=7, n_estimators=200, colsample_bytree=0.8,\n",
    "    subsample=0.8, nthread=10, learning_rate=0.1, silent=False\n",
    ")\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "print('logloss: %0.3f'%multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "여기서는 GloVe feature에 대해 LST과 간단한 Dense Network를 훈련시킬 것입니다. 먼저 dense network를 시작해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:37:09.956543Z",
     "iopub.status.busy": "2021-03-10T22:37:09.956543Z",
     "iopub.status.idle": "2021-03-10T22:37:10.140053Z",
     "shell.execute_reply": "2021-03-10T22:37:10.139055Z",
     "shell.execute_reply.started": "2021-03-10T22:37:09.956543Z"
    }
   },
   "outputs": [],
   "source": [
    "# 신경망 전에 데이터 스케일링\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:37:10.141050Z",
     "iopub.status.busy": "2021-03-10T22:37:10.141050Z",
     "iopub.status.idle": "2021-03-10T22:37:10.156011Z",
     "shell.execute_reply": "2021-03-10T22:37:10.155012Z",
     "shell.execute_reply.started": "2021-03-10T22:37:10.141050Z"
    }
   },
   "outputs": [],
   "source": [
    "# 신경망을 위해 레이블을 이분화\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:37:10.158005Z",
     "iopub.status.busy": "2021-03-10T22:37:10.157008Z",
     "iopub.status.idle": "2021-03-10T22:37:10.448229Z",
     "shell.execute_reply": "2021-03-10T22:37:10.447232Z",
     "shell.execute_reply.started": "2021-03-10T22:37:10.157008Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3 layer sequential neural net 생성\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:37:10.450224Z",
     "iopub.status.busy": "2021-03-10T22:37:10.450224Z",
     "iopub.status.idle": "2021-03-10T22:37:16.651769Z",
     "shell.execute_reply": "2021-03-10T22:37:16.650772Z",
     "shell.execute_reply.started": "2021-03-10T22:37:10.450224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "276/276 [==============================] - 1s 4ms/step - loss: 0.9002 - val_loss: 0.7023\n",
      "Epoch 2/5\n",
      "276/276 [==============================] - 1s 3ms/step - loss: 0.6944 - val_loss: 0.6712\n",
      "Epoch 3/5\n",
      "276/276 [==============================] - 1s 3ms/step - loss: 0.6311 - val_loss: 0.6545\n",
      "Epoch 4/5\n",
      "276/276 [==============================] - 1s 3ms/step - loss: 0.5881 - val_loss: 0.6472\n",
      "Epoch 5/5\n",
      "276/276 [==============================] - 1s 3ms/step - loss: 0.5534 - val_loss: 0.6432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bc7634cc88>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    xtrain_glove_scl, y=ytrain_enc, batch_size=64, epochs=5, verbose=1,\n",
    "    validation_data=(xvalid_glove_scl, yvalid_enc)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 나은 결과를 위해 계속 파라미터를 튜닝하고, 레이어를 추가하고, dropout을 높여야 합니다. 여기서는 단지 최적화 없이 xgboost보다 구현과 실행이 빠르고 더 나은 결과를 얻을 수 있다는 것을 보여드리고자 합니다.  \n",
    "\n",
    "LSTM과 같은 방법을 사용하기 위해서는 텍스트 데이터를 tokenize해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:37:16.653762Z",
     "iopub.status.busy": "2021-03-10T22:37:16.652765Z",
     "iopub.status.idle": "2021-03-10T22:37:17.827200Z",
     "shell.execute_reply": "2021-03-10T22:37:17.826238Z",
     "shell.execute_reply.started": "2021-03-10T22:37:16.653762Z"
    }
   },
   "outputs": [],
   "source": [
    "# keras의 tokenizer 사용\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:37:17.828197Z",
     "iopub.status.busy": "2021-03-10T22:37:17.828197Z",
     "iopub.status.idle": "2021-03-10T22:37:17.964959Z",
     "shell.execute_reply": "2021-03-10T22:37:17.963946Z",
     "shell.execute_reply.started": "2021-03-10T22:37:17.828197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360b883d4eab49e5bc06f6619cde610e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 셋에 있는 단어들에 대한 임베딩 매트릭스 생성\n",
    "embedding_matrix = np.zeros((len(word_index)+1, 300))\n",
    "for word, i in tqdm_notebook(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T22:37:17.965941Z",
     "iopub.status.busy": "2021-03-10T22:37:17.965941Z",
     "iopub.status.idle": "2021-03-10T23:50:36.978528Z",
     "shell.execute_reply": "2021-03-10T23:50:36.975535Z",
     "shell.execute_reply.started": "2021-03-10T22:37:17.965941Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 38s 1s/step - loss: 1.0558 - val_loss: 0.9187\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 39s 1s/step - loss: 0.8937 - val_loss: 0.7625\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 39s 1s/step - loss: 0.8178 - val_loss: 0.7174\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.7844 - val_loss: 0.6882\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.7616 - val_loss: 0.6590\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.7386 - val_loss: 0.6475\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.7265 - val_loss: 0.6353\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.7081 - val_loss: 0.6305\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.6846 - val_loss: 0.6112\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.6625 - val_loss: 0.5894\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.6413 - val_loss: 0.5844\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 39s 1s/step - loss: 0.6475 - val_loss: 0.6126\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 39s 1s/step - loss: 0.6179 - val_loss: 0.5781\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.5930 - val_loss: 0.5538\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.5798 - val_loss: 0.5612\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.5651 - val_loss: 0.5371\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.5518 - val_loss: 0.5669\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.5342 - val_loss: 0.5347\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 40s 1s/step - loss: 0.5292 - val_loss: 0.5476\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.5142 - val_loss: 0.5274\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.4941 - val_loss: 0.5060\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 40s 1s/step - loss: 0.4913 - val_loss: 0.5103\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 40s 1s/step - loss: 0.4708 - val_loss: 0.5147\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.4812 - val_loss: 0.5115\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.4604 - val_loss: 0.5176\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.4566 - val_loss: 0.5010\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.4438 - val_loss: 0.4981\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.4379 - val_loss: 0.5140\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.4196 - val_loss: 0.5013\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.4115 - val_loss: 0.5126\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.4137 - val_loss: 0.5263\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.4019 - val_loss: 0.5181\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.4025 - val_loss: 0.5079\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.3859 - val_loss: 0.5299\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.3891 - val_loss: 0.5275\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.3853 - val_loss: 0.5065\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.3757 - val_loss: 0.4934\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.3758 - val_loss: 0.4966\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.3601 - val_loss: 0.4894\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.3658 - val_loss: 0.4955\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.3522 - val_loss: 0.5025\n",
      "Epoch 42/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.3455 - val_loss: 0.5014\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.3388 - val_loss: 0.5035\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.3365 - val_loss: 0.4951\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - 52s 1s/step - loss: 0.3289 - val_loss: 0.5129\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - 62s 2s/step - loss: 0.3327 - val_loss: 0.5011\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.3297 - val_loss: 0.5078\n",
      "Epoch 48/100\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.3146 - val_loss: 0.5339\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.3136 - val_loss: 0.5438\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.3214 - val_loss: 0.5123\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.3199 - val_loss: 0.5006\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.3025 - val_loss: 0.4964\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - 49s 1s/step - loss: 0.3006 - val_loss: 0.5350\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.3035 - val_loss: 0.5230\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.2952 - val_loss: 0.5296\n",
      "Epoch 56/100\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.2950 - val_loss: 0.5221\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - 51s 1s/step - loss: 0.2900 - val_loss: 0.5128\n",
      "Epoch 58/100\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.2854 - val_loss: 0.5259\n",
      "Epoch 59/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.2814 - val_loss: 0.5208\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - 40s 1s/step - loss: 0.2773 - val_loss: 0.5185\n",
      "Epoch 61/100\n",
      "35/35 [==============================] - 40s 1s/step - loss: 0.2758 - val_loss: 0.5514\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.2811 - val_loss: 0.5314\n",
      "Epoch 63/100\n",
      "35/35 [==============================] - 40s 1s/step - loss: 0.2698 - val_loss: 0.5449\n",
      "Epoch 64/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.2736 - val_loss: 0.5269\n",
      "Epoch 65/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.2648 - val_loss: 0.5471\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.2634 - val_loss: 0.5214\n",
      "Epoch 67/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.2650 - val_loss: 0.5352\n",
      "Epoch 68/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.2563 - val_loss: 0.5497\n",
      "Epoch 69/100\n",
      "35/35 [==============================] - 39s 1s/step - loss: 0.2548 - val_loss: 0.5373\n",
      "Epoch 70/100\n",
      "35/35 [==============================] - 38s 1s/step - loss: 0.2658 - val_loss: 0.5415\n",
      "Epoch 71/100\n",
      "35/35 [==============================] - 39s 1s/step - loss: 0.2514 - val_loss: 0.5375\n",
      "Epoch 72/100\n",
      "35/35 [==============================] - 40s 1s/step - loss: 0.2562 - val_loss: 0.5645\n",
      "Epoch 73/100\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.2442 - val_loss: 0.5771\n",
      "Epoch 74/100\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.2584 - val_loss: 0.5380\n",
      "Epoch 75/100\n",
      "35/35 [==============================] - 49s 1s/step - loss: 0.2409 - val_loss: 0.5392\n",
      "Epoch 76/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.2445 - val_loss: 0.5507\n",
      "Epoch 77/100\n",
      "35/35 [==============================] - 39s 1s/step - loss: 0.2403 - val_loss: 0.5612\n",
      "Epoch 78/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.2478 - val_loss: 0.5580\n",
      "Epoch 79/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.2313 - val_loss: 0.5715\n",
      "Epoch 80/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.2327 - val_loss: 0.5651\n",
      "Epoch 81/100\n",
      "35/35 [==============================] - 47s 1s/step - loss: 0.2353 - val_loss: 0.5769\n",
      "Epoch 82/100\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.2239 - val_loss: 0.5824\n",
      "Epoch 83/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.2262 - val_loss: 0.5681\n",
      "Epoch 84/100\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.2230 - val_loss: 0.5784\n",
      "Epoch 85/100\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.2173 - val_loss: 0.5865\n",
      "Epoch 86/100\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.2156 - val_loss: 0.5996\n",
      "Epoch 87/100\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.2172 - val_loss: 0.5646\n",
      "Epoch 88/100\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.2213 - val_loss: 0.5800\n",
      "Epoch 89/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.2204 - val_loss: 0.5707\n",
      "Epoch 90/100\n",
      "35/35 [==============================] - 40s 1s/step - loss: 0.2119 - val_loss: 0.6165\n",
      "Epoch 91/100\n",
      "35/35 [==============================] - 39s 1s/step - loss: 0.2312 - val_loss: 0.5621\n",
      "Epoch 92/100\n",
      "35/35 [==============================] - 38s 1s/step - loss: 0.2203 - val_loss: 0.5539\n",
      "Epoch 93/100\n",
      "35/35 [==============================] - 38s 1s/step - loss: 0.2058 - val_loss: 0.6042\n",
      "Epoch 94/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.2148 - val_loss: 0.5658\n",
      "Epoch 95/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.2087 - val_loss: 0.5788\n",
      "Epoch 96/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.2085 - val_loss: 0.5771\n",
      "Epoch 97/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.2058 - val_loss: 0.5698\n",
      "Epoch 98/100\n",
      "35/35 [==============================] - 40s 1s/step - loss: 0.2037 - val_loss: 0.6036\n",
      "Epoch 99/100\n",
      "35/35 [==============================] - 38s 1s/step - loss: 0.2023 - val_loss: 0.5552\n",
      "Epoch 100/100\n",
      "35/35 [==============================] - 39s 1s/step - loss: 0.1936 - val_loss: 0.6043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bc76fc5708>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glove embedding과 2 dense layer로 이루어진 간단한 LSTM\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Embedding(len(word_index)+1, 300, weights=[embedding_matrix],\n",
    "              input_length=max_len, trainable=False)\n",
    ")\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# fitting\n",
    "model.fit(\n",
    "    xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1,\n",
    "    validation_data=(xvalid_pad, yvalid_enc)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'왜 dropout을 이렇게나 많이 할까?'라는 질문을 할 수 있습니다. dropout을 하지 않거나 적은 상태로 모델을 fitting하면 과적합이 발생합니다.  \n",
    "\n",
    "양방향 LSTM이 더 나은 결과를 줄 수 있는지 확인해보겠습니다. keras와 함께하면 식은 죽 먹기입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T23:50:36.984511Z",
     "iopub.status.busy": "2021-03-10T23:50:36.984511Z",
     "iopub.status.idle": "2021-03-11T00:38:28.197950Z",
     "shell.execute_reply": "2021-03-11T00:38:28.195955Z",
     "shell.execute_reply.started": "2021-03-10T23:50:36.984511Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 96s 3s/step - loss: 1.0678 - val_loss: 0.9329\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 111s 3s/step - loss: 0.8961 - val_loss: 0.7783\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 120s 3s/step - loss: 0.8174 - val_loss: 0.7162\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 115s 3s/step - loss: 0.7844 - val_loss: 0.7023\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 115s 3s/step - loss: 0.7586 - val_loss: 0.6725\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 119s 3s/step - loss: 0.7329 - val_loss: 0.6476\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 121s 3s/step - loss: 0.7159 - val_loss: 0.6414\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 121s 3s/step - loss: 0.7008 - val_loss: 0.6350\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 115s 3s/step - loss: 0.6811 - val_loss: 0.6087\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 116s 3s/step - loss: 0.6597 - val_loss: 0.5997\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 115s 3s/step - loss: 0.6451 - val_loss: 0.5913\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 116s 3s/step - loss: 0.6218 - val_loss: 0.5885\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 114s 3s/step - loss: 0.6035 - val_loss: 0.5776\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 111s 3s/step - loss: 0.5906 - val_loss: 0.5506\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 111s 3s/step - loss: 0.5743 - val_loss: 0.5457\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 112s 3s/step - loss: 0.5525 - val_loss: 0.5612\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 114s 3s/step - loss: 0.5503 - val_loss: 0.5300\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 120s 3s/step - loss: 0.5317 - val_loss: 0.5302\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 120s 3s/step - loss: 0.5159 - val_loss: 0.5488\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 121s 3s/step - loss: 0.5039 - val_loss: 0.5073\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 120s 3s/step - loss: 0.4975 - val_loss: 0.5007\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 121s 3s/step - loss: 0.4812 - val_loss: 0.5049\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 121s 3s/step - loss: 0.4659 - val_loss: 0.5713\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 121s 3s/step - loss: 0.4630 - val_loss: 0.5189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bc7f68a308>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glove embedding과 2 dense layer로 이루어진 간단한 양방향 LSTM\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Embedding(len(word_index)+1, 300, weights=[embedding_matrix],\n",
    "              input_length=max_len, trainable=False)\n",
    ")\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# fitting\n",
    "earlystop = EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto'\n",
    ")\n",
    "model.fit(\n",
    "    xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1,\n",
    "    validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 두 레이어로 이루어진 GRU도 사용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-11T00:38:28.202937Z",
     "iopub.status.busy": "2021-03-11T00:38:28.202937Z",
     "iopub.status.idle": "2021-03-11T02:47:11.772995Z",
     "shell.execute_reply": "2021-03-11T02:47:11.770025Z",
     "shell.execute_reply.started": "2021-03-11T00:38:28.202937Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 174s 5s/step - loss: 1.0532 - val_loss: 0.8919\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 187s 5s/step - loss: 0.9127 - val_loss: 0.7864\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 197s 6s/step - loss: 0.8333 - val_loss: 0.7636\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 201s 6s/step - loss: 0.7948 - val_loss: 0.7236\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 204s 6s/step - loss: 0.7673 - val_loss: 0.7076\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 207s 6s/step - loss: 0.7306 - val_loss: 0.6539\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 217s 6s/step - loss: 0.7199 - val_loss: 0.6437\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 210s 6s/step - loss: 0.6918 - val_loss: 0.6240\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 209s 6s/step - loss: 0.6661 - val_loss: 0.5934\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 228s 7s/step - loss: 0.6461 - val_loss: 0.6072\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 215s 6s/step - loss: 0.6387 - val_loss: 0.5753\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 221s 6s/step - loss: 0.6022 - val_loss: 0.5800\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 224s 6s/step - loss: 0.5880 - val_loss: 0.5401\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 230s 7s/step - loss: 0.5644 - val_loss: 0.5547\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 220s 6s/step - loss: 0.5493 - val_loss: 0.5419\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 223s 6s/step - loss: 0.5301 - val_loss: 0.5227\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 216s 6s/step - loss: 0.5286 - val_loss: 0.5502\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 216s 6s/step - loss: 0.5060 - val_loss: 0.5158\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 215s 6s/step - loss: 0.4963 - val_loss: 0.5032\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 213s 6s/step - loss: 0.4797 - val_loss: 0.4909\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 216s 6s/step - loss: 0.4672 - val_loss: 0.4971\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 211s 6s/step - loss: 0.4535 - val_loss: 0.4887\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 230s 7s/step - loss: 0.4529 - val_loss: 0.5018\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 227s 6s/step - loss: 0.4343 - val_loss: 0.5253\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 206s 6s/step - loss: 0.4295 - val_loss: 0.4710\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 216s 6s/step - loss: 0.4016 - val_loss: 0.4986\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 217s 6s/step - loss: 0.3967 - val_loss: 0.4706\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 236s 7s/step - loss: 0.3790 - val_loss: 0.4690\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 225s 6s/step - loss: 0.3823 - val_loss: 0.4727\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 210s 6s/step - loss: 0.3735 - val_loss: 0.4916\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 215s 6s/step - loss: 0.3583 - val_loss: 0.4668\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 215s 6s/step - loss: 0.3566 - val_loss: 0.4637\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 216s 6s/step - loss: 0.3388 - val_loss: 0.4745\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 214s 6s/step - loss: 0.3339 - val_loss: 0.4761\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 213s 6s/step - loss: 0.3263 - val_loss: 0.4669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bcc7ef5a08>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glove embedding과 2 dense layer로 이루어진 간단한 GRU\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Embedding(len(word_index)+1, 300, weights=[embedding_matrix],\n",
    "              input_length=max_len, trainable=False)\n",
    ")\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# fitting\n",
    "earlystop = EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto'\n",
    ")\n",
    "model.fit(\n",
    "    xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1,\n",
    "    validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-11T03:51:10.839007Z",
     "iopub.status.busy": "2021-03-11T03:51:10.839007Z",
     "iopub.status.idle": "2021-03-11T03:51:10.859950Z",
     "shell.execute_reply": "2021-03-11T03:51:10.858975Z",
     "shell.execute_reply.started": "2021-03-11T03:51:10.839007Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='[%(asctime)s] %(levelname)s %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    stream=sys.stdout\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-11T03:56:33.786812Z",
     "iopub.status.busy": "2021-03-11T03:56:33.786812Z",
     "iopub.status.idle": "2021-03-11T03:56:33.825707Z",
     "shell.execute_reply": "2021-03-11T03:56:33.824709Z",
     "shell.execute_reply.started": "2021-03-11T03:56:33.786812Z"
    }
   },
   "outputs": [],
   "source": [
    "class Ensembler(object):\n",
    "    def __init__(self, model_dict, num_folds=3, task_type='classification',\n",
    "                 optimize=roc_auc_score, lower_is_better=False, save_path=None):\n",
    "        '''\n",
    "        Ensebler init function\n",
    "        :param model_dict: 모델 딕셔너리\n",
    "        :param num_folds: 앙상블링을 위한 fold 수\n",
    "        :param task_type: classification/regression\n",
    "        :param optimize: AUC, logloss 등 최적화를 위한 함수. y_test와 y_pred가 있어야 함\n",
    "        :param lower_is_better: 최적화 함수의 값이 낮은 것이 좋은지 높은 것이 좋은지\n",
    "        :param save_path: 생성된 예\n",
    "        \n",
    "        측 결과와 모델 pickle 저장 경로\n",
    "        '''\n",
    "        self.model_dict = model_dict\n",
    "        self.levels = len(self.model_dict)\n",
    "        self.num_folds = num_folds\n",
    "        self.task_type = task_type\n",
    "        self.optimize = optimize\n",
    "        self.lower_is_better = lower_is_better\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        self.training_data = None\n",
    "        self.test_data = None\n",
    "        self.y = None\n",
    "        self.lbl_enc = None\n",
    "        self.y_enc = None\n",
    "        self.train_prediction_dict = None\n",
    "        self.test_prediction_dict = None\n",
    "        self.num_classes = None\n",
    "    \n",
    "    def fit(self, training_data, y, lentrain):\n",
    "        '''\n",
    "        :param training_data: 표 형식의 트레이닝 데이터\n",
    "        :param y: 이진값, multi-class/regression\n",
    "        :return: 예측에 사용될 모델들\n",
    "        '''\n",
    "        self.training_data = training_data\n",
    "        self.y = y\n",
    "        \n",
    "        if self.task_type == 'classification':\n",
    "            self.num_classes = len(np.unique(self.y))\n",
    "            logger.info('Found %d classes', self.num_classes)\n",
    "            self.lbl_enc = LabelEncoder()\n",
    "            self.y_enc = self.lbl_enc.fit_transform(self.y)\n",
    "            kf = StratifiedKFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, self.num_classes)\n",
    "        else:\n",
    "            self.num_classes = -1\n",
    "            self.y_enc = self.y\n",
    "            kf = KFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, 1)\n",
    "        \n",
    "        self.train_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.train_prediction_dict[level] = np.zeros((\n",
    "                train_prediction_shape[0], train_prediction_shape[1]*len(self.model_dict[level])\n",
    "            ))\n",
    "        \n",
    "        for level in range(self.levels):\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level-1]\n",
    "                \n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "                validation_scores = []\n",
    "                foldnum = 1\n",
    "                \n",
    "                for train_index, valid_index in kf.split(self.train_prediction_dict[0], self.y_enc):\n",
    "                    logger.info('Training Level %d Fold #%d. Model #%d', level, foldnum, model_num)\n",
    "                    if level != 0:\n",
    "                        l_training_data = temp_train[train_index]\n",
    "                        l_validation_data = temp_train[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "                    else:\n",
    "                        l0_training_data = temp_train[0][model_num]\n",
    "                        if type(l0_training_data) == list:\n",
    "                            l_training_data = [x[train_index] for x in l0_training_data]\n",
    "                            l_validation_data = [x[valid_index] for x in l0_training_data]\n",
    "                        else:\n",
    "                            l_training_data = l0_training_data[train_index]\n",
    "                            l_validation_data = l0_training_data[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "                    \n",
    "                    if self.task_type == 'classification':\n",
    "                        temp_train_predictions = model.predict_proba(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index, (model_num*self.num_classes):(model_num*self.num_classes)+self.num_classes] = temp_train_predictions\n",
    "                    else:\n",
    "                        temp_train_predictions = model.predict(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index, model_num] = temp_train_predictions\n",
    "                    validation_score = self.optimize(self.y_enc[valid_index], temp_train_predictions)\n",
    "                    validation_scores.append(validation_score)\n",
    "                    logger.info('Level %d. Fold #%d. Model #%d. Validation Score = %f', level, foldnum, model_num, validation_score)\n",
    "                    foldnum += 1\n",
    "                \n",
    "                avg_score = np.mean(validation_scores)\n",
    "                std_score = np.std(validation_scores)\n",
    "            logger.info('Level %d. Model #%d. Mean Score = %f. Std Dev = %f', level, model_num, avg_score, std_score)\n",
    "            train_predictions_df = pd.DataFrame(self.train_prediction_dict[level])\n",
    "            train_predictions_df.to_csv(os.path.join(self.save_path, 'train_predictions_level_' + str(level) + '.csv'), index=False, header=None)\n",
    "            \n",
    "        return self.train_prediction_dict\n",
    "        \n",
    "    def predict(self, test_data, lentest):\n",
    "        self.test_data = test_data\n",
    "        if self.task_type == 'classification':\n",
    "            test_prediction_shape = (lentest, self.num_classes)\n",
    "        else:\n",
    "            test_prediction_shape = (lentest, 1)\n",
    "\n",
    "        self.test_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.test_prediction_dict[level] = np.zeros((\n",
    "                test_prediction_shape[0], test_prediction_shape[1]*len(self.model_dict[level])\n",
    "            ))\n",
    "        self.test_data = test_data\n",
    "\n",
    "        for level in range(self.levels):\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "                temp_test = self.test_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level-1]\n",
    "                temp_test = self.test_prediction_dict[level-1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "                logger.info('Training Fulldata Level %d. Model #%d', level, model_num)\n",
    "                if level == 0:\n",
    "                    model.fit(temp_train[0][model_num], self.y_enc)\n",
    "                else:\n",
    "                    model.fit(temp_train, self.y_enc)\n",
    "                logger.info('Predicting Test Level %d. Model #%d', level, model_num)\n",
    "\n",
    "                if self.task_type == 'classification':\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test)\n",
    "                    self.test_prediction_dict[level][:, (model_num*self.num_classes):(model_num*self.num_classes)+self.num_classes] = temp_test_predictions\n",
    "                else:\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict(temp_test)\n",
    "                    self.test_prediction_dict[level][:, model_num] = temp_test_predictions\n",
    "\n",
    "            test_predictions_df = pd.DataFrame(self.test_prediction_dict[level])\n",
    "            test_predictions_df.to_csv(os.path.join(self.save_path, 'test_predictions_level_' + str(level) + '.csv'), index=False, header=None)\n",
    "\n",
    "        return self.test_prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-11T03:56:34.278982Z",
     "iopub.status.busy": "2021-03-11T03:56:34.277962Z",
     "iopub.status.idle": "2021-03-11T03:58:30.932904Z",
     "shell.execute_reply": "2021-03-11T03:58:30.931906Z",
     "shell.execute_reply.started": "2021-03-11T03:56:34.278982Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:56:34] INFO Found 3 classes\n",
      "[12:56:34] INFO Training Level 0 Fold #1. Model #0\n",
      "[12:56:35] INFO Level 0. Fold #1. Model #0. Validation Score = 0.626621\n",
      "[12:56:35] INFO Training Level 0 Fold #2. Model #0\n",
      "[12:56:36] INFO Level 0. Fold #2. Model #0. Validation Score = 0.616457\n",
      "[12:56:36] INFO Training Level 0 Fold #3. Model #0\n",
      "[12:56:37] INFO Level 0. Fold #3. Model #0. Validation Score = 0.619626\n",
      "[12:56:37] INFO Training Level 0 Fold #1. Model #1\n",
      "[12:57:04] INFO Level 0. Fold #1. Model #1. Validation Score = 0.573485\n",
      "[12:57:04] INFO Training Level 0 Fold #2. Model #1\n",
      "[12:57:23] INFO Level 0. Fold #2. Model #1. Validation Score = 0.563451\n",
      "[12:57:23] INFO Training Level 0 Fold #3. Model #1\n",
      "[12:57:43] INFO Level 0. Fold #3. Model #1. Validation Score = 0.567765\n",
      "[12:57:43] INFO Training Level 0 Fold #1. Model #2\n",
      "[12:57:43] INFO Level 0. Fold #1. Model #2. Validation Score = 0.463292\n",
      "[12:57:43] INFO Training Level 0 Fold #2. Model #2\n",
      "[12:57:43] INFO Level 0. Fold #2. Model #2. Validation Score = 0.456477\n",
      "[12:57:43] INFO Training Level 0 Fold #3. Model #2\n",
      "[12:57:43] INFO Level 0. Fold #3. Model #2. Validation Score = 0.461664\n",
      "[12:57:43] INFO Training Level 0 Fold #1. Model #3\n",
      "[12:57:43] INFO Level 0. Fold #1. Model #3. Validation Score = 0.472378\n",
      "[12:57:43] INFO Training Level 0 Fold #2. Model #3\n",
      "[12:57:43] INFO Level 0. Fold #2. Model #3. Validation Score = 0.473229\n",
      "[12:57:43] INFO Training Level 0 Fold #3. Model #3\n",
      "[12:57:43] INFO Level 0. Fold #3. Model #3. Validation Score = 0.479033\n",
      "[12:57:43] INFO Level 0. Model #3. Mean Score = 0.474880. Std Dev = 0.002957\n",
      "[12:57:44] INFO Training Level 1 Fold #1. Model #0\n",
      "[12:57:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:57:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:57:51] INFO Level 1. Fold #1. Model #0. Validation Score = 0.491459\n",
      "[12:57:51] INFO Training Level 1 Fold #2. Model #0\n",
      "[12:57:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:57:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:57:56] INFO Level 1. Fold #2. Model #0. Validation Score = 0.476751\n",
      "[12:57:56] INFO Training Level 1 Fold #3. Model #0\n",
      "[12:57:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:57:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:58:01] INFO Level 1. Fold #3. Model #0. Validation Score = 0.490793\n",
      "[12:58:01] INFO Level 1. Model #0. Mean Score = 0.486334. Std Dev = 0.006782\n",
      "[12:58:01] INFO Training Fulldata Level 0. Model #0\n",
      "[12:58:03] INFO Predicting Test Level 0. Model #0\n",
      "[12:58:03] INFO Training Fulldata Level 0. Model #1\n",
      "[12:58:22] INFO Predicting Test Level 0. Model #1\n",
      "[12:58:22] INFO Training Fulldata Level 0. Model #2\n",
      "[12:58:22] INFO Predicting Test Level 0. Model #2\n",
      "[12:58:22] INFO Training Fulldata Level 0. Model #3\n",
      "[12:58:22] INFO Predicting Test Level 0. Model #3\n",
      "[12:58:22] INFO Training Fulldata Level 1. Model #0\n",
      "[12:58:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:58:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:58:30] INFO Predicting Test Level 1. Model #0\n"
     ]
    }
   ],
   "source": [
    "train_data_dict = {0: [xtrain_tfv, xtrain_ctv, xtrain_tfv, xtrain_ctv],\n",
    "                   1: [xtrain_glove]}\n",
    "test_data_dict = {0: [xvalid_tfv, xvalid_ctv, xvalid_tfv, xvalid_ctv],\n",
    "                  1: [xvalid_glove]}\n",
    "model_dict = {0: [LogisticRegression(), LogisticRegression(), MultinomialNB(alpha=0.1), MultinomialNB()],\n",
    "              1: [xgb.XGBClassifier(silent=True, n_estimators=120, max_depth=7)]}\n",
    "\n",
    "ens = Ensembler(model_dict=model_dict, num_folds=3, task_type='classification',\n",
    "                optimize=multiclass_logloss, lower_is_better=True, save_path='data/')\n",
    "ens.fit(train_data_dict, ytrain, lentrain=xtrain_glove.shape[0])\n",
    "preds = ens.predict(test_data_dict, lentest=xvalid_glove.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-11T04:06:29.363198Z",
     "iopub.status.busy": "2021-03-11T04:06:29.362200Z",
     "iopub.status.idle": "2021-03-11T04:06:29.377160Z",
     "shell.execute_reply": "2021-03-11T04:06:29.376185Z",
     "shell.execute_reply.started": "2021-03-11T04:06:29.363198Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4659197712814561"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check error\n",
    "multiclass_logloss(yvalid, preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
