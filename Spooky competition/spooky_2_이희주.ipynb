{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- competition/dataset : [https://www.kaggle.com/c/spooky-author-identification](https://www.kaggle.com/c/spooky-author-identification)\n",
    "- date : 2021/03/10\n",
    "- original : [https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaching (Almost) Any NLP Problem on Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-19T23:23:23.804833Z",
     "iopub.status.busy": "2021-01-19T23:23:23.803834Z",
     "iopub.status.idle": "2021-01-19T23:23:23.811814Z",
     "shell.execute_reply": "2021-01-19T23:23:23.810817Z",
     "shell.execute_reply.started": "2021-01-19T23:23:23.804833Z"
    }
   },
   "source": [
    "**✏ 필사 1회** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 포스팅에서는 Kaggle의 NLP 문제에 대해 이야기하고자 합니다. 예제로 우리는 이 대회에서 주어진 데이터를 사용합니다. 먼저 기본 모델을 만들고, 다른 feature들을 이용하여 개선합니다. 또한 DNN이 어떻게 사용되는지 확인하고, 일반적인 앙상블에 대한 몇 가지 아이디어로 포스팅을 마무리할 예정입니다.  \n",
    "\n",
    "**This covers:**  \n",
    "* tfidf\n",
    "* count features\n",
    "* logistic regression\n",
    "* naive bayes\n",
    "* svm\n",
    "* xgboost\n",
    "* grid search\n",
    "* word vectors\n",
    "* LSTM\n",
    "* GRU\n",
    "* Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T23:04:43.044615Z",
     "iopub.status.busy": "2021-03-09T23:04:43.044615Z",
     "iopub.status.idle": "2021-03-09T23:04:43.068562Z",
     "shell.execute_reply": "2021-03-09T23:04:43.067524Z",
     "shell.execute_reply.started": "2021-03-09T23:04:43.044615Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T22:44:21.080279Z",
     "iopub.status.busy": "2021-03-09T22:44:21.079283Z",
     "iopub.status.idle": "2021-03-09T22:44:21.333747Z",
     "shell.execute_reply": "2021-03-09T22:44:21.332739Z",
     "shell.execute_reply.started": "2021-03-09T22:44:21.080279Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "sample = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T22:44:23.782299Z",
     "iopub.status.busy": "2021-03-09T22:44:23.781271Z",
     "iopub.status.idle": "2021-03-09T22:44:23.815182Z",
     "shell.execute_reply": "2021-03-09T22:44:23.814212Z",
     "shell.execute_reply.started": "2021-03-09T22:44:23.782299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T22:44:27.717887Z",
     "iopub.status.busy": "2021-03-09T22:44:27.717887Z",
     "iopub.status.idle": "2021-03-09T22:44:27.732847Z",
     "shell.execute_reply": "2021-03-09T22:44:27.731849Z",
     "shell.execute_reply.started": "2021-03-09T22:44:27.717887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T22:44:31.181147Z",
     "iopub.status.busy": "2021-03-09T22:44:31.181147Z",
     "iopub.status.idle": "2021-03-09T22:44:31.206081Z",
     "shell.execute_reply": "2021-03-09T22:44:31.205082Z",
     "shell.execute_reply.started": "2021-03-09T22:44:31.181147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.403494  0.287808  0.308698\n",
       "1  id24541  0.403494  0.287808  0.308698\n",
       "2  id00134  0.403494  0.287808  0.308698\n",
       "3  id27757  0.403494  0.287808  0.308698\n",
       "4  id04081  0.403494  0.287808  0.308698"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 문제는 텍스트에 주어진 EAP, HPL, MWS 저자를 예측해야 합니다. 간단히 말해서, 텍스트는 세 가지의 클래스로 분류되어 있습니다.  \n",
    "\n",
    "이 문제에 대해 Kaggle은 multi-class log-loss를 평가 메트릭으로 지정했습니다. 이는 다음과 같은 방법으로 구현됩니다: [https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/util.py](https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/util.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T22:54:19.556318Z",
     "iopub.status.busy": "2021-03-09T22:54:19.555343Z",
     "iopub.status.idle": "2021-03-09T22:54:19.573273Z",
     "shell.execute_reply": "2021-03-09T22:54:19.572305Z",
     "shell.execute_reply.started": "2021-03-09T22:54:19.556318Z"
    }
   },
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    '''\n",
    "    log loss 메트릭의 멀티 클래스 버전\n",
    "    :param actual: 실제 타겟 클래스를 포함한 array\n",
    "    :param predicted: 클래스 예측 값이 포함된 matrix, 한 클래스당 하나의 확률\n",
    "    '''\n",
    "    # actual이 binary array가 아니면 변환\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "    clip = np.clip(predicted, eps, 1-eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text label을 정수형 0, 1, 2로 변환하기 위해 scikit-learn의 `LabelEncoder`를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T22:55:43.613500Z",
     "iopub.status.busy": "2021-03-09T22:55:43.613500Z",
     "iopub.status.idle": "2021-03-09T22:55:43.632449Z",
     "shell.execute_reply": "2021-03-09T22:55:43.631461Z",
     "shell.execute_reply.started": "2021-03-09T22:55:43.613500Z"
    }
   },
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train['author'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 진행하기 앞서 훈련 데이터 셋과 검증 데이터 셋을 분리하는 것이 중요합니다. scikit-learn 모듈의 `model_selection`의 `train_test_split`을 사용하여 실행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T22:57:39.060412Z",
     "iopub.status.busy": "2021-03-09T22:57:39.060412Z",
     "iopub.status.idle": "2021-03-09T22:57:39.078363Z",
     "shell.execute_reply": "2021-03-09T22:57:39.077365Z",
     "shell.execute_reply.started": "2021-03-09T22:57:39.060412Z"
    }
   },
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(\n",
    "    train['text'].values, y, stratify=y, random_state=42,\n",
    "    test_size=0.1, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T22:57:58.396053Z",
     "iopub.status.busy": "2021-03-09T22:57:58.396053Z",
     "iopub.status.idle": "2021-03-09T22:57:58.408990Z",
     "shell.execute_reply": "2021-03-09T22:57:58.407991Z",
     "shell.execute_reply.started": "2021-03-09T22:57:58.396053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Basic Models\n",
    "첫 번째 모델을 생성해봅시다.  \n",
    "\n",
    "우리의 첫 모델은 TF-IDF(Tearm Frequency - Inverse Document Frequency)와 간단한 로지스틱 회귀입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T23:03:29.051328Z",
     "iopub.status.busy": "2021-03-09T23:03:29.051328Z",
     "iopub.status.idle": "2021-03-09T23:03:31.354954Z",
     "shell.execute_reply": "2021-03-09T23:03:31.353962Z",
     "shell.execute_reply.started": "2021-03-09T23:03:29.051328Z"
    }
   },
   "outputs": [],
   "source": [
    "# 항상 이 feature들로 시작\n",
    "tfv = TfidfVectorizer(\n",
    "    min_df=3, max_features=None, strip_accents='unicode',\n",
    "    analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 3),\n",
    "    use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words='english'\n",
    ")\n",
    "\n",
    "# training과 test 셋에 TF-IDF fitting (준지도학습)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv = tfv.transform(xtrain)\n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T23:04:46.343714Z",
     "iopub.status.busy": "2021-03-09T23:04:46.343714Z",
     "iopub.status.idle": "2021-03-09T23:04:47.484172Z",
     "shell.execute_reply": "2021-03-09T23:04:47.483175Z",
     "shell.execute_reply.started": "2021-03-09T23:04:46.343714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.572\n"
     ]
    }
   ],
   "source": [
    "# TFIDF에 로지스틱 회귀 적용\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "print('logloss: %0.3f'%multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 모델의 multiclass logloss는 0.572입니다.  \n",
    "\n",
    "동일한 모델을 다른 데이터로 살펴보겠습니다.  \n",
    "\n",
    "TF-DIF 대신에 단어 수를 feature로 사용할 수 있습니다. scikit-learn의 `CountVectorizer`을 사용하면 간단하게 해결할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T23:33:03.907954Z",
     "iopub.status.busy": "2021-03-09T23:33:03.906955Z",
     "iopub.status.idle": "2021-03-09T23:33:06.875820Z",
     "shell.execute_reply": "2021-03-09T23:33:06.874916Z",
     "shell.execute_reply.started": "2021-03-09T23:33:03.907954Z"
    }
   },
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(\n",
    "    analyzer='word', token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 3), stop_words='english'\n",
    ")\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv = ctv.transform(xtrain)\n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T23:35:04.139548Z",
     "iopub.status.busy": "2021-03-09T23:35:04.139548Z",
     "iopub.status.idle": "2021-03-09T23:35:22.225218Z",
     "shell.execute_reply": "2021-03-09T23:35:22.224220Z",
     "shell.execute_reply.started": "2021-03-09T23:35:04.139548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.527\n"
     ]
    }
   ],
   "source": [
    "# Counts에 로지스틱 회귀 적용\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "print('logloss: %0.3f'%multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.048 개선되었습니다.  \n",
    "\n",
    "다음으로, 예전에 꽤 유명했던 Naive Bayes 모델을 시도해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T23:39:13.076153Z",
     "iopub.status.busy": "2021-03-09T23:39:13.076153Z",
     "iopub.status.idle": "2021-03-09T23:39:13.101085Z",
     "shell.execute_reply": "2021-03-09T23:39:13.100110Z",
     "shell.execute_reply.started": "2021-03-09T23:39:13.076153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.578\n"
     ]
    }
   ],
   "source": [
    "# TFIDF에 Naive Bayes 적용\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "print('logloss: %0.3f'%multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts에 로지스틱 회귀를 적용한 것이 더 성능이 좋습니다. 위 모델을 counts data에 사용하면 어떻게 될까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T23:40:58.826062Z",
     "iopub.status.busy": "2021-03-09T23:40:58.826062Z",
     "iopub.status.idle": "2021-03-09T23:40:58.900371Z",
     "shell.execute_reply": "2021-03-09T23:40:58.897379Z",
     "shell.execute_reply.started": "2021-03-09T23:40:58.826062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.485\n"
     ]
    }
   ],
   "source": [
    "# Counts에 Naive Bayes 적용\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "print('logloss: %0.3f'%multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오래된 것이 더 잘 작동합니다!  \n",
    "\n",
    "오래된 알고리즘 중에 SVM이 있으며, 일부 사람들은 SVM을 매우 사랑합니다. 따라서, 데이터 셋에 SVM을 시도해보겠습니다.  \n",
    "\n",
    "SVM은 시간이 오래 걸리기 때문에 Singular Value Decomposition을 사용하여 TF-IDF의 feature의 수를 줄이겠습니다.  \n",
    "\n",
    "SVM을 적용하기 전에 반드시 표준화를 해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-09T23:47:22.042342Z",
     "iopub.status.busy": "2021-03-09T23:47:22.041344Z",
     "iopub.status.idle": "2021-03-09T23:47:23.803528Z",
     "shell.execute_reply": "2021-03-09T23:47:23.802531Z",
     "shell.execute_reply.started": "2021-03-09T23:47:22.042342Z"
    }
   },
   "outputs": [],
   "source": [
    "# SVD를 적용하여 120개의 컴포넌트 선택\n",
    "# SVM 모델에는 120 - 200개의 컴포넌트가 적절함\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "# SVD로 얻은 데이터 스케일링\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 SVM을 적용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T00:23:52.604010Z",
     "iopub.status.busy": "2021-03-10T00:23:52.604010Z",
     "iopub.status.idle": "2021-03-10T00:29:12.500970Z",
     "shell.execute_reply": "2021-03-10T00:29:12.499974Z",
     "shell.execute_reply.started": "2021-03-10T00:23:52.604010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.725\n"
     ]
    }
   ],
   "source": [
    "# SVM fitting\n",
    "# 오!래!걸!림!\n",
    "clf = SVC(C=1.0, probability=True) # 확률 값을 구하는 것이기 때문\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "print('logloss: %0.3f'%multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능이 좋지 않습니다. \n",
    "\n",
    "또 인기있는 알고리즘 중 하나인 xgboost를 적용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T00:29:12.504960Z",
     "iopub.status.busy": "2021-03-10T00:29:12.503963Z",
     "iopub.status.idle": "2021-03-10T00:29:32.226011Z",
     "shell.execute_reply": "2021-03-10T00:29:32.225014Z",
     "shell.execute_reply.started": "2021-03-10T00:29:12.504960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:29:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.781\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF에 xgboost 적용\n",
    "clf = xgb.XGBClassifier(\n",
    "    max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8,\n",
    "    nthread=10, learning_rate=0.1\n",
    ")\n",
    "clf.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "\n",
    "print ('logloss: %0.3f'% multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T00:29:32.227008Z",
     "iopub.status.busy": "2021-03-10T00:29:32.227008Z",
     "iopub.status.idle": "2021-03-10T00:31:14.478016Z",
     "shell.execute_reply": "2021-03-10T00:31:14.477018Z",
     "shell.execute_reply.started": "2021-03-10T00:29:32.227008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:29:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.772\n"
     ]
    }
   ],
   "source": [
    "# Counts에 xgboost 적용\n",
    "clf = xgb.XGBClassifier(\n",
    "    max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8,\n",
    "    nthread=10, learning_rate=0.1\n",
    ")\n",
    "clf.fit(xtrain_ctv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv.tocsc())\n",
    "\n",
    "print ('logloss: %0.3f'% multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T00:31:14.480009Z",
     "iopub.status.busy": "2021-03-10T00:31:14.479012Z",
     "iopub.status.idle": "2021-03-10T00:32:41.482549Z",
     "shell.execute_reply": "2021-03-10T00:32:41.481551Z",
     "shell.execute_reply.started": "2021-03-10T00:31:14.480009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:31:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.764\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF svd feature에 xgboost 적용\n",
    "clf = xgb.XGBClassifier(\n",
    "    max_depth=7, n_estimators=200, colsample_bytree=0.8, subsample=0.8,\n",
    "    nthread=10, learning_rate=0.1\n",
    ")\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print ('logloss: %0.3f'% multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T00:32:41.484543Z",
     "iopub.status.busy": "2021-03-10T00:32:41.484543Z",
     "iopub.status.idle": "2021-03-10T00:33:37.679214Z",
     "shell.execute_reply": "2021-03-10T00:33:37.678217Z",
     "shell.execute_reply.started": "2021-03-10T00:32:41.484543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:32:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.790\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF svd feature에 xgboost 적용\n",
    "clf = xgb.XGBClassifier(nthread=10)\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print ('logloss: %0.3f'% multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "이것은 하이퍼파라미터 최적화를 위한 기술입니다. 효과적이지는 않지만 사용하고자 하는 그리드를 알고 있다면 좋은 결과를 얻을 수 있습니다. 다음 포스트에 일반적으로 사용해야 할 파라미터에 대해 정리해 두었습니다: [http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/](http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/)  \n",
    "\n",
    "이 섹션에서는 로지스틱 회귀를 사용한 grid search에 대해 이야기하겠습니다.  \n",
    "\n",
    "grid search를 시작하기 전에 먼저 스코어링 함수를 만들어야 합니다. scikit-learn의 `make_scorer` 함수를 사용하여 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T00:33:37.681210Z",
     "iopub.status.busy": "2021-03-10T00:33:37.680213Z",
     "iopub.status.idle": "2021-03-10T00:33:37.695172Z",
     "shell.execute_reply": "2021-03-10T00:33:37.694175Z",
     "shell.execute_reply.started": "2021-03-10T00:33:37.681210Z"
    }
   },
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(\n",
    "    multiclass_logloss, greater_is_better=False, needs_proba=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 파이프라인이 필요합니다. 시연을 위해 SVD, 스케일링, 로지스틱 회귀로 구성된 파이프라인을 사용합니다. 많은 모듈들을 파이프라인에 적용하여 이해하는 것이 하나만 사용하는 것보다 더 낫습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T02:38:49.472937Z",
     "iopub.status.busy": "2021-03-10T02:38:49.472937Z",
     "iopub.status.idle": "2021-03-10T02:38:49.489892Z",
     "shell.execute_reply": "2021-03-10T02:38:49.488894Z",
     "shell.execute_reply.started": "2021-03-10T02:38:49.472937Z"
    }
   },
   "outputs": [],
   "source": [
    "# SVD 초기화\n",
    "svd = TruncatedSVD()\n",
    "\n",
    "# Standard Scaler 초기화\n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "# 로지스틱 회귀 초기화\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# 파이프라인 생성\n",
    "clf = pipeline.Pipeline([('svd', svd), ('scl', scl), ('lr', lr_model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파라미터 그리드가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T02:39:39.983417Z",
     "iopub.status.busy": "2021-03-10T02:39:39.983417Z",
     "iopub.status.idle": "2021-03-10T02:39:40.002367Z",
     "shell.execute_reply": "2021-03-10T02:39:40.001397Z",
     "shell.execute_reply.started": "2021-03-10T02:39:39.983417Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components':[120, 180],\n",
    "              'lr__C':[0.1, 1.0, 10],\n",
    "              'lr__penalty':['l1', 'l2']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD는 120개와 180개의 컴포넌트를 평가하고, 로지스틱 회귀의 경우 l1과 l2 패널티로 세 가지 다른 C의 값들을 평가합니다. 이제 이 파라미터들로 grid search를 시작할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T02:54:44.224771Z",
     "iopub.status.busy": "2021-03-10T02:54:44.223775Z",
     "iopub.status.idle": "2021-03-10T02:55:20.161267Z",
     "shell.execute_reply": "2021-03-10T02:55:20.160267Z",
     "shell.execute_reply.started": "2021-03-10T02:54:44.224771Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
      "[CV 1/2; 1/12] START lr__C=0.1, lr__penalty=l1, svd__n_components=120...........\n",
      "[CV 1/2; 1/12] END lr__C=0.1, lr__penalty=l1, svd__n_components=120; total time=   1.2s\n",
      "[CV 2/2; 1/12] START lr__C=0.1, lr__penalty=l1, svd__n_components=120...........\n",
      "[CV 2/2; 1/12] END lr__C=0.1, lr__penalty=l1, svd__n_components=120; total time=   1.0s\n",
      "[CV 1/2; 2/12] START lr__C=0.1, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 1/2; 2/12] END lr__C=0.1, lr__penalty=l1, svd__n_components=180; total time=   1.7s\n",
      "[CV 2/2; 2/12] START lr__C=0.1, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 2/2; 2/12] END lr__C=0.1, lr__penalty=l1, svd__n_components=180; total time=   1.5s\n",
      "[CV 1/2; 3/12] START lr__C=0.1, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 1/2; 3/12] END lr__C=0.1, lr__penalty=l2, svd__n_components=120; total time=   0.9s\n",
      "[CV 2/2; 3/12] START lr__C=0.1, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 2/2; 3/12] END lr__C=0.1, lr__penalty=l2, svd__n_components=120; total time=   1.0s\n",
      "[CV 1/2; 4/12] START lr__C=0.1, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 1/2; 4/12] END lr__C=0.1, lr__penalty=l2, svd__n_components=180; total time=   1.5s\n",
      "[CV 2/2; 4/12] START lr__C=0.1, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 2/2; 4/12] END lr__C=0.1, lr__penalty=l2, svd__n_components=180; total time=   2.0s\n",
      "[CV 1/2; 5/12] START lr__C=1.0, lr__penalty=l1, svd__n_components=120...........\n",
      "[CV 1/2; 5/12] END lr__C=1.0, lr__penalty=l1, svd__n_components=120; total time=   0.8s\n",
      "[CV 2/2; 5/12] START lr__C=1.0, lr__penalty=l1, svd__n_components=120...........\n",
      "[CV 2/2; 5/12] END lr__C=1.0, lr__penalty=l1, svd__n_components=120; total time=   1.1s\n",
      "[CV 1/2; 6/12] START lr__C=1.0, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 1/2; 6/12] END lr__C=1.0, lr__penalty=l1, svd__n_components=180; total time=   1.4s\n",
      "[CV 2/2; 6/12] START lr__C=1.0, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 2/2; 6/12] END lr__C=1.0, lr__penalty=l1, svd__n_components=180; total time=   1.3s\n",
      "[CV 1/2; 7/12] START lr__C=1.0, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 1/2; 7/12] END lr__C=1.0, lr__penalty=l2, svd__n_components=120; total time=   1.1s\n",
      "[CV 2/2; 7/12] START lr__C=1.0, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 2/2; 7/12] END lr__C=1.0, lr__penalty=l2, svd__n_components=120; total time=   1.2s\n",
      "[CV 1/2; 8/12] START lr__C=1.0, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 1/2; 8/12] END lr__C=1.0, lr__penalty=l2, svd__n_components=180; total time=   1.9s\n",
      "[CV 2/2; 8/12] START lr__C=1.0, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 2/2; 8/12] END lr__C=1.0, lr__penalty=l2, svd__n_components=180; total time=   1.8s\n",
      "[CV 1/2; 9/12] START lr__C=10, lr__penalty=l1, svd__n_components=120............\n",
      "[CV 1/2; 9/12] END lr__C=10, lr__penalty=l1, svd__n_components=120; total time=   0.8s\n",
      "[CV 2/2; 9/12] START lr__C=10, lr__penalty=l1, svd__n_components=120............\n",
      "[CV 2/2; 9/12] END lr__C=10, lr__penalty=l1, svd__n_components=120; total time=   0.8s\n",
      "[CV 1/2; 10/12] START lr__C=10, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 1/2; 10/12] END lr__C=10, lr__penalty=l1, svd__n_components=180; total time=   1.2s\n",
      "[CV 2/2; 10/12] START lr__C=10, lr__penalty=l1, svd__n_components=180...........\n",
      "[CV 2/2; 10/12] END lr__C=10, lr__penalty=l1, svd__n_components=180; total time=   1.1s\n",
      "[CV 1/2; 11/12] START lr__C=10, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 1/2; 11/12] END lr__C=10, lr__penalty=l2, svd__n_components=120; total time=   1.0s\n",
      "[CV 2/2; 11/12] START lr__C=10, lr__penalty=l2, svd__n_components=120...........\n",
      "[CV 2/2; 11/12] END lr__C=10, lr__penalty=l2, svd__n_components=120; total time=   0.9s\n",
      "[CV 1/2; 12/12] START lr__C=10, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 1/2; 12/12] END lr__C=10, lr__penalty=l2, svd__n_components=180; total time=   1.7s\n",
      "[CV 2/2; 12/12] START lr__C=10, lr__penalty=l2, svd__n_components=180...........\n",
      "[CV 2/2; 12/12] END lr__C=10, lr__penalty=l2, svd__n_components=180; total time=   1.6s\n",
      "Best score: -0.742\n",
      "Best parameters set:\n",
      "\tlr__C: 0.1\n",
      "\tlr__penalty: 'l2'\n",
      "\tsvd__n_components: 180\n"
     ]
    }
   ],
   "source": [
    "# Grid Search 모델 초기화\n",
    "model = GridSearchCV(\n",
    "    estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "    verbose=10, n_jobs=1, refit=True, cv=2\n",
    ")\n",
    "\n",
    "# fitting\n",
    "model.fit(xtrain_tfv, ytrain) # 전체 데이터를 사용해도 됨\n",
    "print('Best score: %0.3f'%model.best_score_)\n",
    "print('Best parameters set:')\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print('\\t%s: %r'%(param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "점수는 SVM과 비슷합니다. 이 기술은 다음과 같이 xgboost나 다항 naive bayes를 정교하게 튜닝하는데 사용될 수 있습니다. 여기서는 tfidf 데이터를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T03:01:48.187617Z",
     "iopub.status.busy": "2021-03-10T03:01:48.187617Z",
     "iopub.status.idle": "2021-03-10T03:01:48.530348Z",
     "shell.execute_reply": "2021-03-10T03:01:48.529380Z",
     "shell.execute_reply.started": "2021-03-10T03:01:48.187617Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "Best score: -0.492\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "# 파이프라인 생성\n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha':[0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Grid Search 모델 초기화\n",
    "model = GridSearchCV(\n",
    "    estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "    verbose=10, n_jobs=-1, refit=True, cv=2\n",
    ")\n",
    "\n",
    "# fitting\n",
    "model.fit(xtrain_tfv, ytrain) # 전체 데이터를 사용해도 됨\n",
    "print('Best score: %0.3f'%model.best_score_)\n",
    "print('Best parameters set:')\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print('\\t%s: %r'%(param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP 문제에서는 일반적으로 단어 벡터를 살펴봅니다. 단어 벡터는 데이터에 대한 많은 인사이트를 제공합니다. 살펴봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word Vectors\n",
    "깊게 들어가지 않고, 어떻게 문장 벡터를 만들고 이것을 머신러닝 모델에 어떻게 사용하는지에 대해 설명해보겠습니다. 저는 GloVe vertors, word2vec, fasttext를 굉장히 좋아합니다. 이 포스트에서는 GloVe vectors를 사용할 것입니다. 이것은 [http://www-nlp.stanford.edu/data/glove.840B.300d.zip](http://www-nlp.stanford.edu/data/glove.840B.300d.zip)에서 다운받을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T04:00:00.119598Z",
     "iopub.status.busy": "2021-03-10T04:00:00.119598Z",
     "iopub.status.idle": "2021-03-10T04:04:32.181771Z",
     "shell.execute_reply": "2021-03-10T04:04:32.180775Z",
     "shell.execute_reply.started": "2021-03-10T04:00:00.119598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747a8d18997d4ef88eb811e6f1bb1a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195864 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# 딕셔너리에 GloVe vector 로드\n",
    "embeddings_index = {}\n",
    "f = open('data/glove.840B.300d.txt', encoding='utf-8')\n",
    "for line in tqdm_notebook(f):\n",
    "    try:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        f.__next__()\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.'%len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T04:06:38.072624Z",
     "iopub.status.busy": "2021-03-10T04:06:38.072624Z",
     "iopub.status.idle": "2021-03-10T04:06:38.092603Z",
     "shell.execute_reply": "2021-03-10T04:06:38.091605Z",
     "shell.execute_reply.started": "2021-03-10T04:06:38.072624Z"
    }
   },
   "outputs": [],
   "source": [
    "# 전체 문장에 대해 표준화된 벡터를 생성하는 함수\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt(v**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T04:06:39.074411Z",
     "iopub.status.busy": "2021-03-10T04:06:39.073414Z",
     "iopub.status.idle": "2021-03-10T04:06:46.997453Z",
     "shell.execute_reply": "2021-03-10T04:06:46.996456Z",
     "shell.execute_reply.started": "2021-03-10T04:06:39.074411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e717010d983843e88e15eeb6ca304443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159220a64a37429194134d410929fe42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training과 validation 데이터 셋에 대해 위 함수를 적용하여 문장 벡터 생성\n",
    "xtrain_glove = np.array([sent2vec(x) for x in tqdm_notebook(xtrain)])\n",
    "xvalid_glove = np.array([sent2vec(x) for x in tqdm_notebook(xvalid)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove feature에 대해 xgboost의 성능을 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T04:07:07.786520Z",
     "iopub.status.busy": "2021-03-10T04:07:07.786520Z",
     "iopub.status.idle": "2021-03-10T04:09:24.152062Z",
     "shell.execute_reply": "2021-03-10T04:09:24.151065Z",
     "shell.execute_reply.started": "2021-03-10T04:07:07.786520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:07:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:07:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.720\n"
     ]
    }
   ],
   "source": [
    "# glove feature에 xgboost fitting 1\n",
    "clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "print('logloss: %0.3f'%multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T04:09:32.618970Z",
     "iopub.status.busy": "2021-03-10T04:09:32.618970Z",
     "iopub.status.idle": "2021-03-10T04:13:10.258800Z",
     "shell.execute_reply": "2021-03-10T04:13:10.257801Z",
     "shell.execute_reply.started": "2021-03-10T04:09:32.618970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:09:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:09:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.679\n"
     ]
    }
   ],
   "source": [
    "# glove feature에 xgboost fitting 2\n",
    "clf = xgb.XGBClassifier(\n",
    "    max_depth=7, n_estimators=200, colsample_bytree=0.8,\n",
    "    subsample=0.8, nthread=10, learning_rate=0.1, silent=False\n",
    ")\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "print('logloss: %0.3f'%multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "여기서는 GloVe feature에 대해 LST과 간단한 Dense Network를 훈련시킬 것입니다. 먼저 dense network를 시작해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T04:13:21.208402Z",
     "iopub.status.busy": "2021-03-10T04:13:21.208402Z",
     "iopub.status.idle": "2021-03-10T04:13:21.391911Z",
     "shell.execute_reply": "2021-03-10T04:13:21.390915Z",
     "shell.execute_reply.started": "2021-03-10T04:13:21.208402Z"
    }
   },
   "outputs": [],
   "source": [
    "# 신경망 전에 데이터 스케일링\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T04:13:22.017643Z",
     "iopub.status.busy": "2021-03-10T04:13:22.017643Z",
     "iopub.status.idle": "2021-03-10T04:13:22.031605Z",
     "shell.execute_reply": "2021-03-10T04:13:22.030608Z",
     "shell.execute_reply.started": "2021-03-10T04:13:22.017643Z"
    }
   },
   "outputs": [],
   "source": [
    "# 신경망을 위해 레이블을 이분화\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T04:13:22.689257Z",
     "iopub.status.busy": "2021-03-10T04:13:22.688247Z",
     "iopub.status.idle": "2021-03-10T04:13:23.064335Z",
     "shell.execute_reply": "2021-03-10T04:13:23.063335Z",
     "shell.execute_reply.started": "2021-03-10T04:13:22.689257Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3 layer sequential neural net 생성\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T04:13:24.107134Z",
     "iopub.status.busy": "2021-03-10T04:13:24.107134Z",
     "iopub.status.idle": "2021-03-10T04:13:31.065978Z",
     "shell.execute_reply": "2021-03-10T04:13:31.064983Z",
     "shell.execute_reply.started": "2021-03-10T04:13:24.107134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "276/276 [==============================] - 1s 5ms/step - loss: 0.9170 - val_loss: 0.7175\n",
      "Epoch 2/5\n",
      "276/276 [==============================] - 1s 4ms/step - loss: 0.7066 - val_loss: 0.6817\n",
      "Epoch 3/5\n",
      "276/276 [==============================] - 1s 4ms/step - loss: 0.6323 - val_loss: 0.6678\n",
      "Epoch 4/5\n",
      "276/276 [==============================] - 1s 4ms/step - loss: 0.5910 - val_loss: 0.6729\n",
      "Epoch 5/5\n",
      "276/276 [==============================] - 1s 4ms/step - loss: 0.5594 - val_loss: 0.6652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a8f6cb1608>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    xtrain_glove_scl, y=ytrain_enc, batch_size=64, epochs=5, verbose=1,\n",
    "    validation_data=(xvalid_glove_scl, yvalid_enc)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 나은 결과를 위해 계속 파라미터를 튜닝하고, 레이어를 추가하고, dropout을 높여야 합니다. 여기서는 단지 최적화 없이 xgboost보다 구현과 실행이 빠르고 더 나은 결과를 얻을 수 있다는 것을 보여드리고자 합니다.  \n",
    "\n",
    "LSTM과 같은 방법을 사용하기 위해서는 텍스트 데이터를 tokenize해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T04:16:42.917659Z",
     "iopub.status.busy": "2021-03-10T04:16:42.916662Z",
     "iopub.status.idle": "2021-03-10T04:16:44.119398Z",
     "shell.execute_reply": "2021-03-10T04:16:44.118434Z",
     "shell.execute_reply.started": "2021-03-10T04:16:42.917659Z"
    }
   },
   "outputs": [],
   "source": [
    "# keras의 tokenizer 사용\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T04:16:45.083042Z",
     "iopub.status.busy": "2021-03-10T04:16:45.082044Z",
     "iopub.status.idle": "2021-03-10T04:16:45.245118Z",
     "shell.execute_reply": "2021-03-10T04:16:45.244121Z",
     "shell.execute_reply.started": "2021-03-10T04:16:45.082044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd28b8aaf08742c898465e7b6128704f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 셋에 있는 단어들에 대한 임베딩 매트릭스 생성\n",
    "embedding_matrix = np.zeros((len(word_index)+1, 300))\n",
    "for word, i in tqdm_notebook(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T04:16:56.825632Z",
     "iopub.status.busy": "2021-03-10T04:16:56.825632Z",
     "iopub.status.idle": "2021-03-10T05:32:52.091956Z",
     "shell.execute_reply": "2021-03-10T05:32:52.089962Z",
     "shell.execute_reply.started": "2021-03-10T04:16:56.825632Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 40s 1s/step - loss: 1.0676 - val_loss: 0.9156\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 40s 1s/step - loss: 0.8932 - val_loss: 0.7366\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.8076 - val_loss: 0.6992\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.7796 - val_loss: 0.6754\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.7553 - val_loss: 0.6582\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.7339 - val_loss: 0.6946\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.7249 - val_loss: 0.6500\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.7046 - val_loss: 0.6225\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.6760 - val_loss: 0.6068\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.6666 - val_loss: 0.5963\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.6471 - val_loss: 0.5891\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.6322 - val_loss: 0.5781\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.6065 - val_loss: 0.5678\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.5947 - val_loss: 0.5672\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.5728 - val_loss: 0.5588\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 48s 1s/step - loss: 0.5662 - val_loss: 0.5647\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.5463 - val_loss: 0.5336\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 39s 1s/step - loss: 0.5344 - val_loss: 0.5347\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.5179 - val_loss: 0.5324\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 49s 1s/step - loss: 0.5097 - val_loss: 0.5296\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 40s 1s/step - loss: 0.5030 - val_loss: 0.5218\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 47s 1s/step - loss: 0.4891 - val_loss: 0.5071\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 53s 2s/step - loss: 0.4756 - val_loss: 0.5247\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.4653 - val_loss: 0.5120\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 40s 1s/step - loss: 0.4564 - val_loss: 0.5104\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 55s 2s/step - loss: 0.4529 - val_loss: 0.5277\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.4417 - val_loss: 0.5212\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.4433 - val_loss: 0.5046\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.4236 - val_loss: 0.5137\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.4121 - val_loss: 0.5258\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.4041 - val_loss: 0.5085\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.4025 - val_loss: 0.5203\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 40s 1s/step - loss: 0.3947 - val_loss: 0.4911\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.3963 - val_loss: 0.5229\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 56s 2s/step - loss: 0.3968 - val_loss: 0.5148\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - 51s 1s/step - loss: 0.3819 - val_loss: 0.4916\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - 52s 1s/step - loss: 0.3710 - val_loss: 0.5358\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - 56s 2s/step - loss: 0.3668 - val_loss: 0.5117\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - 51s 1s/step - loss: 0.3592 - val_loss: 0.5114\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - 53s 2s/step - loss: 0.3541 - val_loss: 0.5297\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.3453 - val_loss: 0.5307\n",
      "Epoch 42/100\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.3492 - val_loss: 0.5335\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.3561 - val_loss: 0.5225\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - 49s 1s/step - loss: 0.3355 - val_loss: 0.5439\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - 47s 1s/step - loss: 0.3412 - val_loss: 0.5096\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.3274 - val_loss: 0.5083\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.3210 - val_loss: 0.5417\n",
      "Epoch 48/100\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.3142 - val_loss: 0.5342\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - 47s 1s/step - loss: 0.3182 - val_loss: 0.5321\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.3173 - val_loss: 0.5515\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.3148 - val_loss: 0.5275\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - 48s 1s/step - loss: 0.3031 - val_loss: 0.5241\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - 48s 1s/step - loss: 0.3199 - val_loss: 0.5323\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.2951 - val_loss: 0.5372\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.2959 - val_loss: 0.5321\n",
      "Epoch 56/100\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.2947 - val_loss: 0.5256\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.2945 - val_loss: 0.5191\n",
      "Epoch 58/100\n",
      "35/35 [==============================] - 49s 1s/step - loss: 0.2812 - val_loss: 0.5307\n",
      "Epoch 59/100\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.2987 - val_loss: 0.5301\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - 54s 2s/step - loss: 0.2854 - val_loss: 0.5315\n",
      "Epoch 61/100\n",
      "35/35 [==============================] - 58s 2s/step - loss: 0.2872 - val_loss: 0.5422\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - 47s 1s/step - loss: 0.2735 - val_loss: 0.5326\n",
      "Epoch 63/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.2708 - val_loss: 0.5510\n",
      "Epoch 64/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.2740 - val_loss: 0.5678\n",
      "Epoch 65/100\n",
      "35/35 [==============================] - 52s 1s/step - loss: 0.2726 - val_loss: 0.5715\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.2698 - val_loss: 0.5479\n",
      "Epoch 67/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.2591 - val_loss: 0.6050\n",
      "Epoch 68/100\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.2624 - val_loss: 0.5548\n",
      "Epoch 69/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.2542 - val_loss: 0.5546\n",
      "Epoch 70/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.2485 - val_loss: 0.5619\n",
      "Epoch 71/100\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.2554 - val_loss: 0.5617\n",
      "Epoch 72/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.2541 - val_loss: 0.5480\n",
      "Epoch 73/100\n",
      "35/35 [==============================] - 43s 1s/step - loss: 0.2480 - val_loss: 0.5542\n",
      "Epoch 74/100\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.2454 - val_loss: 0.5328\n",
      "Epoch 75/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.2489 - val_loss: 0.5401\n",
      "Epoch 76/100\n",
      "35/35 [==============================] - 46s 1s/step - loss: 0.2537 - val_loss: 0.5418\n",
      "Epoch 77/100\n",
      "35/35 [==============================] - 45s 1s/step - loss: 0.2413 - val_loss: 0.5687\n",
      "Epoch 78/100\n",
      "35/35 [==============================] - 39s 1s/step - loss: 0.2413 - val_loss: 0.5470\n",
      "Epoch 79/100\n",
      "35/35 [==============================] - 37s 1s/step - loss: 0.2368 - val_loss: 0.5599\n",
      "Epoch 80/100\n",
      "35/35 [==============================] - 36s 1s/step - loss: 0.2319 - val_loss: 0.5707\n",
      "Epoch 81/100\n",
      "35/35 [==============================] - 37s 1s/step - loss: 0.2374 - val_loss: 0.5527\n",
      "Epoch 82/100\n",
      "35/35 [==============================] - 36s 1s/step - loss: 0.2371 - val_loss: 0.5639\n",
      "Epoch 83/100\n",
      "35/35 [==============================] - 36s 1s/step - loss: 0.2301 - val_loss: 0.5738\n",
      "Epoch 84/100\n",
      "35/35 [==============================] - 38s 1s/step - loss: 0.2307 - val_loss: 0.5712\n",
      "Epoch 85/100\n",
      "35/35 [==============================] - 40s 1s/step - loss: 0.2236 - val_loss: 0.5769\n",
      "Epoch 86/100\n",
      "35/35 [==============================] - 38s 1s/step - loss: 0.2336 - val_loss: 0.5609\n",
      "Epoch 87/100\n",
      "35/35 [==============================] - 38s 1s/step - loss: 0.2264 - val_loss: 0.6010\n",
      "Epoch 88/100\n",
      "35/35 [==============================] - 37s 1s/step - loss: 0.2206 - val_loss: 0.5599\n",
      "Epoch 89/100\n",
      "35/35 [==============================] - 38s 1s/step - loss: 0.2144 - val_loss: 0.5897\n",
      "Epoch 90/100\n",
      "35/35 [==============================] - 38s 1s/step - loss: 0.2225 - val_loss: 0.5632\n",
      "Epoch 91/100\n",
      "35/35 [==============================] - 38s 1s/step - loss: 0.2222 - val_loss: 0.5604\n",
      "Epoch 92/100\n",
      "35/35 [==============================] - 37s 1s/step - loss: 0.2150 - val_loss: 0.5802\n",
      "Epoch 93/100\n",
      "35/35 [==============================] - 37s 1s/step - loss: 0.2105 - val_loss: 0.5991\n",
      "Epoch 94/100\n",
      "35/35 [==============================] - 38s 1s/step - loss: 0.2058 - val_loss: 0.5905\n",
      "Epoch 95/100\n",
      "35/35 [==============================] - 41s 1s/step - loss: 0.2033 - val_loss: 0.6100\n",
      "Epoch 96/100\n",
      "35/35 [==============================] - 39s 1s/step - loss: 0.2077 - val_loss: 0.5718\n",
      "Epoch 97/100\n",
      "35/35 [==============================] - 44s 1s/step - loss: 0.2101 - val_loss: 0.5972\n",
      "Epoch 98/100\n",
      "35/35 [==============================] - 38s 1s/step - loss: 0.2125 - val_loss: 0.5901\n",
      "Epoch 99/100\n",
      "35/35 [==============================] - 39s 1s/step - loss: 0.2086 - val_loss: 0.5734\n",
      "Epoch 100/100\n",
      "35/35 [==============================] - 42s 1s/step - loss: 0.1994 - val_loss: 0.6076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a8ff977788>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glove embedding과 2 dense layer로 이루어진 간단한 LSTM\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Embedding(len(word_index)+1, 300, weights=[embedding_matrix],\n",
    "              input_length=max_len, trainable=False)\n",
    ")\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# fitting\n",
    "model.fit(\n",
    "    xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1,\n",
    "    validation_data=(xvalid_pad, yvalid_enc)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'왜 dropout을 이렇게나 많이 할까?'라는 질문을 할 수 있습니다. dropout을 하지 않거나 적은 상태로 모델을 fitting하면 과적합이 발생합니다.  \n",
    "\n",
    "양방향 LSTM이 더 나은 결과를 줄 수 있는지 확인해보겠습니다. keras와 함께하면 식은 죽 먹기입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove embedding과 2 dense layer로 이루어진 간단한 양방향 LSTM\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Embedding(len(word_index)+1, 300, weights=[embedding_matrix],\n",
    "              input_length=max_len, trainable=False)\n",
    ")\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# fitting\n",
    "earlystop = EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto'\n",
    ")\n",
    "model.fit(\n",
    "    xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1,\n",
    "    validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 두 레이어로 이루어진 GRU도 사용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove embedding과 2 dense layer로 이루어진 간단한 GRU\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Embedding(len(word_index)+1, 300, weights=[embedding_matrix],\n",
    "              input_length=max_len, trainable=False)\n",
    ")\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# fitting\n",
    "earlystop = EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto'\n",
    ")\n",
    "model.fit(\n",
    "    xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1,\n",
    "    validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
