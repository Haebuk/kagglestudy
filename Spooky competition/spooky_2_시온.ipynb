{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opening-effect",
   "metadata": {},
   "source": [
    "Day 36 : 21.03.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-reynolds",
   "metadata": {},
   "source": [
    "# Approaching (Almost) Any NLP Problem on Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-trainer",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-joint",
   "metadata": {},
   "source": [
    "여기선 자연어 처리 문제에 대해 이야기하고자 한다. 우선 기본적인 첫 번째 모델을 만든 후 다른 변수를 사용해 개선한다. 또한 신경망이 얼마나 심층적으로 사용될 수 있는지 보고 일반적으로 앙상블에 대한 아이디어로 끝낼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-yeast",
   "metadata": {},
   "source": [
    "여기서 다룰 문제:\n",
    "- tf-idf\n",
    "- count features\n",
    "- logistic regression\n",
    "- naive bayes\n",
    "- svm\n",
    "- xgboost\n",
    "- grid search\n",
    "- word vectors\n",
    "- LSTM\n",
    "- GRU\n",
    "- Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "looking-bones",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T13:27:45.605148Z",
     "start_time": "2021-03-10T13:27:44.887598Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "liquid-password",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T13:28:24.376731Z",
     "start_time": "2021-03-10T13:28:24.238104Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "sample = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cardiac-washington",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T13:28:28.431181Z",
     "start_time": "2021-03-10T13:28:28.408244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dangerous-context",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T13:28:33.907126Z",
     "start_time": "2021-03-10T13:28:33.895158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "arbitrary-australian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T13:28:40.387260Z",
     "start_time": "2021-03-10T13:28:40.358338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.403494  0.287808  0.308698\n",
       "1  id24541  0.403494  0.287808  0.308698\n",
       "2  id00134  0.403494  0.287808  0.308698\n",
       "3  id27757  0.403494  0.287808  0.308698\n",
       "4  id04081  0.403494  0.287808  0.308698"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-coordinate",
   "metadata": {},
   "source": [
    "이 문제는 저자를 예측하는 문제다. 즉, text에 주어진 EAP, HPL, MWS를 예측해야한다. 간단히 말해서 text 분류는 3가지 클래스로 이루어진다.\n",
    "\n",
    "이 특정 문제에 대해 Kaggle은 multi-class log-loss(다중 클래스 로그 손실)을 평가 지표로 지정했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "occupational-retail",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T13:35:19.611692Z",
     "start_time": "2021-03-10T13:35:19.593739Z"
    }
   },
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\" 로그 손실 메트릭의 다중 클래스 버전 \"\"\"\n",
    "    # actual : 실제 target 클래스를 포함하는 배열\n",
    "    # predicted : 클래스 예측이 있는 행렬, 클래스당 하나의 확률\n",
    "    \n",
    "    # 이진 배열이 아닌 actual을 이진 배열로 변환\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i,val] = 1\n",
    "        actual = actual2\n",
    "        \n",
    "    clip = np.clip(predicted, eps, 1-eps)  ### np.clip(array, min, max) : array의 min, max 범위 넘어가는 요소를 작으면 min, 크면 max로 변환\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual*np.log(clip))\n",
    "    \n",
    "    return -1.0 / rows*vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-district",
   "metadata": {},
   "source": [
    "scikit-learn의 LabelEncoder를 사용해 text 레이블을 정수로 변환 : 0,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "genetic-football",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T13:36:10.525737Z",
     "start_time": "2021-03-10T13:36:10.513730Z"
    }
   },
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train.author.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-exchange",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T13:36:15.780484Z",
     "start_time": "2021-03-10T13:36:15.764524Z"
    }
   },
   "source": [
    "더 나아가기 전에 데이터를 training, validation 셋으로 나누는 것이 중요하다. scikit-learn의 model_selection 모듈에서 train_test_split을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "filled-hungarian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T13:37:46.695756Z",
     "start_time": "2021-03-10T13:37:46.675771Z"
    }
   },
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values,y,\n",
    "                                                 stratify=y, random_state=42, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "heard-committee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T13:37:54.944562Z",
     "start_time": "2021-03-10T13:37:54.940573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-persian",
   "metadata": {},
   "source": [
    "## Building Basic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-accessory",
   "metadata": {},
   "source": [
    "첫 번재 모델을 만들어보자.\n",
    "\n",
    "첫 모델을 단순 TF-IDF(Term Frequency - Inverse Document Frequency) 후 단순 로지스틱 회귀분석이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hindu-battlefield",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:01:36.212165Z",
     "start_time": "2021-03-10T14:01:33.920842Z"
    }
   },
   "outputs": [],
   "source": [
    "# 항상 이 매개변수들로 해라. 거의 항상 이걸로 작동된다.\n",
    "tfv = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word',\n",
    "                     token_pattern=r'\\w{1,}', ngram_range=(1,3), use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words='english')\n",
    "# training, test 셋에 TF-IDF 적합 (준지도학습)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv = tfv.transform(xtrain)\n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "loving-toddler",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T15:17:24.709226Z",
     "start_time": "2021-03-10T15:17:18.055004Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-66e7f208a786>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TF-IDF에 단순 로지스틱 회귀분석 적합\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain_tfv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxvalid_tfv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m             \u001b[0mprefer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'processes'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m   1408\u001b[0m                                \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m             path_func(X, y, pos_class=class_, Cs=[C_],\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    760\u001b[0m                 \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"iprint\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gtol\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"maxiter\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m             )\n\u001b[1;32m--> 762\u001b[1;33m             n_iter_i = _check_optimize_result(\n\u001b[0m\u001b[0;32m    763\u001b[0m                 \u001b[0msolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                 extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\sklearn\\utils\\optimize.py\u001b[0m in \u001b[0;36m_check_optimize_result\u001b[1;34m(solver, result, max_iter, extra_warning_msg)\u001b[0m\n\u001b[0;32m    241\u001b[0m                 \u001b[1;34m\"    https://scikit-learn.org/stable/modules/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m                 \u001b[1;34m\"preprocessing.html\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             ).format(solver, result.status, result.message.decode(\"latin1\"))\n\u001b[0m\u001b[0;32m    244\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_warning_msg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[0mwarning_msg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextra_warning_msg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "# TF-IDF에 단순 로지스틱 회귀분석 적합\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print('logloss : %0.3f' %multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-genre",
   "metadata": {},
   "source": [
    "0.626의 다중 클래스 로그 손실을 갖는 첫 모델을 생성했다. 하지만 더 좋은 점수를 위해 다른 데이터로 동일한 모델을 살펴보자.\n",
    "\n",
    "TF-IDF를 사용하는 대신 단어 수를 변수로 사용할 수도 있다. 이 작업은 scikit-learn에서 CountVectorizer를 사용해 쉽게 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "regulated-shark",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:09:50.573650Z",
     "start_time": "2021-03-10T14:09:47.279305Z"
    }
   },
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,3), stop_words='english')\n",
    "\n",
    "# training, test 셋에 Count Vectorizer 적합 (준지도학습)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv = ctv.transform(xtrain)\n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "classified-interface",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:11:00.673560Z",
     "start_time": "2021-03-10T14:10:37.717897Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-6101a131cb56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Counts에 단순 로지스틱 회귀 적합\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain_ctv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxvalid_ctv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m             \u001b[0mprefer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'processes'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m   1408\u001b[0m                                \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m             path_func(X, y, pos_class=class_, Cs=[C_],\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    760\u001b[0m                 \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"iprint\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gtol\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"maxiter\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m             )\n\u001b[1;32m--> 762\u001b[1;33m             n_iter_i = _check_optimize_result(\n\u001b[0m\u001b[0;32m    763\u001b[0m                 \u001b[0msolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                 extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\sklearn\\utils\\optimize.py\u001b[0m in \u001b[0;36m_check_optimize_result\u001b[1;34m(solver, result, max_iter, extra_warning_msg)\u001b[0m\n\u001b[0;32m    241\u001b[0m                 \u001b[1;34m\"    https://scikit-learn.org/stable/modules/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m                 \u001b[1;34m\"preprocessing.html\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             ).format(solver, result.status, result.message.decode(\"latin1\"))\n\u001b[0m\u001b[0;32m    244\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_warning_msg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[0mwarning_msg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextra_warning_msg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "# Counts에 단순 로지스틱 회귀 적합\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print('logloss : %0.3f' %multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-place",
   "metadata": {},
   "source": [
    "첫 모델을 0.1점 향상시켰다!\n",
    "\n",
    "다음으로, Naive Bayes 모델을 시도해보자. 다음 두 데이터셋에서 Naive bayes를 사용할 때 어떤 일이 일어나는지 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "functioning-norway",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:12:52.081719Z",
     "start_time": "2021-03-10T14:12:52.058743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss : 0.578\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF에 단순 Naive Bayes 적합\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print('logloss : %0.3f' %multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-minnesota",
   "metadata": {},
   "source": [
    "좋은 성능이지만 count에 대한 로지스틱회귀분석이 더 좋다. 대신 count 데이터에 이 모델을 사용하면 어떻게 될까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "premier-ladder",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:14:38.916991Z",
     "start_time": "2021-03-10T14:14:38.848176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss : 0.485\n"
     ]
    }
   ],
   "source": [
    "# Counts에 단순 Naive Bayes 적합\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print('logloss : %0.3f' %multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-seeker",
   "metadata": {},
   "source": [
    "다음은 SVM을 적용해보자. SVM은 시간이 많이 걸리므로 적용하기 전 Singlular Value Decomposition을 사용해 TF-IDF의 변수 개수를 줄인다. 또한 SVM을 적용하기 전 데이터를 표준화해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "timely-tuesday",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:18:33.257161Z",
     "start_time": "2021-03-10T14:18:31.367211Z"
    }
   },
   "outputs": [],
   "source": [
    "# 120개 요소를 선택해 SVD 적용. 120-200 요소가 SVM 모델에 충분하다\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "# SVD로 얻은 데이터를 스케일링. 스케일링 없이 재사용할 변수 이름 재서렂ㅇ\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "compact-introduction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:27:50.295919Z",
     "start_time": "2021-03-10T14:19:40.143475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss : 0.727\n"
     ]
    }
   ],
   "source": [
    "# 단순 SVM 적합\n",
    "clf = SVC(C=1.0, probability=True)  # 확률이 필요하기 때문\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "print('logloss : %0.3f' %multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-trout",
   "metadata": {},
   "source": [
    "SVM이 이 데이터에서 성능이 좋지 않은 것 같다. 캐글에서 가장 유명한 알고리즘인 xgboost를 적용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "respiratory-separate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:30:13.011158Z",
     "start_time": "2021-03-10T14:29:39.726093Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\study\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:29:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss : 0.781\n"
     ]
    }
   ],
   "source": [
    "# tf-idf에 xgboost 적합\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8,\n",
    "                       subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv.tocsc(), ytrain)  ### tocsc : 희소행렬로 만들기\n",
    "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "\n",
    "print('logloss : %0.3f' %multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "central-seeking",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:35:51.836783Z",
     "start_time": "2021-03-10T14:32:00.934162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:32:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss : 0.772\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# tf-idf에 xgboost 적합\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8,\n",
    "                       subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_ctv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv.tocsc())\n",
    "\n",
    "print('logloss : %0.3f' %multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "becoming-subsection",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:40:09.138763Z",
     "start_time": "2021-03-10T14:36:13.138334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:36:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss : 0.768\n"
     ]
    }
   ],
   "source": [
    "# tf-idf svd 변수에 xgboost 적용\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print('logloss : %0.3f' %multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "powerful-dating",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:44:13.554908Z",
     "start_time": "2021-03-10T14:42:14.824146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:42:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss : 0.786\n"
     ]
    }
   ],
   "source": [
    "# tf-idf svd 변수에 xgboost 적합\n",
    "clf = xgb.XGBClassifier(nthread=10)\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print('logloss : %0.3f' %multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-margin",
   "metadata": {},
   "source": [
    "XGBoost는 안 좋아보인다. 하지만 정확하지 않다. 아직 매개변수 최적화를 하지 않았다. 최적화 방법은 다음 섹션에서 설명한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-tobago",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-poetry",
   "metadata": {},
   "source": [
    "매개변수를 최적화하기 위한 기술이다. 그리 효과적이지 않지만 사용할 그리드를 알고 있으면 좋은 결과를 얻을 수 있다. \n",
    "\n",
    "이 섹션에서 로지스틱 회귀분석을 사용한 Grid Search에 대해 설명한다. Grid Search를 시작하기 전, 점수 계산 함수를 만들어야한다. 이는 scikit-learn의 make_scorer 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "missing-miracle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:44:22.202170Z",
     "start_time": "2021-03-10T14:44:22.195188Z"
    }
   },
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-thomas",
   "metadata": {},
   "source": [
    "다음은 파이프라인이 필요하다. 여기서 시연하기 위해, SVD, 스케일링, 로지스틱 회귀분석으로 구성된 파이프라인을 사용한다. 하나의 모듈보다 더 많은 모듈을 파이프라인에 배치하는 것이 이해하기 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "applicable-layout",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:44:24.026673Z",
     "start_time": "2021-03-10T14:44:24.009721Z"
    }
   },
   "outputs": [],
   "source": [
    "# SVD 초기화\n",
    "svd = TruncatedSVD()\n",
    "# standard scaler 초기화\n",
    "scl = preprocessing.StandardScaler()\n",
    "# 로지스틱 회귀분석\n",
    "lr_model = LogisticRegression()\n",
    "# 파이프라인 생성\n",
    "clf = pipeline.Pipeline([('svd',svd), ('scl',scl), ('lr',lr_model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-stephen",
   "metadata": {},
   "source": [
    "다음으로 매개변수의 그리드가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "educated-newton",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:44:28.393220Z",
     "start_time": "2021-03-10T14:44:28.378258Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components':[120,180],\n",
    "             'lr__C':[0.1,1.0,10],\n",
    "             'lr__penalty':['l1','l2']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-massachusetts",
   "metadata": {},
   "source": [
    "따라서 SVD의 경우 120개와 180개의 요소를 평가하고, 로지스틱회귀분석의 경우 lr과 l2 패널티로 C의 세 가지 다른 값을 평가한다. 이제 이러한 매개 변수에 대한 Grid Search를 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "increased-charleston",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:45:21.944006Z",
     "start_time": "2021-03-10T14:44:30.259103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
      "[CV] lr__C=0.1, lr__penalty=l1, svd__n_components=120 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=0.1, lr__penalty=l1, svd__n_components=120, score=nan, total=   1.8s\n",
      "[CV] lr__C=0.1, lr__penalty=l1, svd__n_components=120 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=0.1, lr__penalty=l1, svd__n_components=120, score=nan, total=   2.4s\n",
      "[CV] lr__C=0.1, lr__penalty=l1, svd__n_components=180 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=0.1, lr__penalty=l1, svd__n_components=180, score=nan, total=   4.4s\n",
      "[CV] lr__C=0.1, lr__penalty=l1, svd__n_components=180 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    8.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=0.1, lr__penalty=l1, svd__n_components=180, score=nan, total=   3.2s\n",
      "[CV] lr__C=0.1, lr__penalty=l2, svd__n_components=120 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   11.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=0.1, lr__penalty=l2, svd__n_components=120, score=-0.777, total=   2.2s\n",
      "[CV] lr__C=0.1, lr__penalty=l2, svd__n_components=120 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   13.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=0.1, lr__penalty=l2, svd__n_components=120, score=-0.772, total=   2.1s\n",
      "[CV] lr__C=0.1, lr__penalty=l2, svd__n_components=180 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   16.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=0.1, lr__penalty=l2, svd__n_components=180, score=-0.739, total=   2.9s\n",
      "[CV] lr__C=0.1, lr__penalty=l2, svd__n_components=180 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   19.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=0.1, lr__penalty=l2, svd__n_components=180, score=-0.734, total=   2.7s\n",
      "[CV] lr__C=1.0, lr__penalty=l1, svd__n_components=120 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   21.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=1.0, lr__penalty=l1, svd__n_components=120, score=nan, total=   1.5s\n",
      "[CV] lr__C=1.0, lr__penalty=l1, svd__n_components=120 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   23.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=1.0, lr__penalty=l1, svd__n_components=120, score=nan, total=   1.1s\n",
      "[CV] lr__C=1.0, lr__penalty=l1, svd__n_components=180 ................\n",
      "[CV]  lr__C=1.0, lr__penalty=l1, svd__n_components=180, score=nan, total=   2.0s\n",
      "[CV] lr__C=1.0, lr__penalty=l1, svd__n_components=180 ................\n",
      "[CV]  lr__C=1.0, lr__penalty=l1, svd__n_components=180, score=nan, total=   2.0s\n",
      "[CV] lr__C=1.0, lr__penalty=l2, svd__n_components=120 ................\n",
      "[CV]  lr__C=1.0, lr__penalty=l2, svd__n_components=120, score=-0.775, total=   1.7s\n",
      "[CV] lr__C=1.0, lr__penalty=l2, svd__n_components=120 ................\n",
      "[CV]  lr__C=1.0, lr__penalty=l2, svd__n_components=120, score=-0.766, total=   1.6s\n",
      "[CV] lr__C=1.0, lr__penalty=l2, svd__n_components=180 ................\n",
      "[CV]  lr__C=1.0, lr__penalty=l2, svd__n_components=180, score=-0.745, total=   2.2s\n",
      "[CV] lr__C=1.0, lr__penalty=l2, svd__n_components=180 ................\n",
      "[CV]  lr__C=1.0, lr__penalty=l2, svd__n_components=180, score=-0.743, total=   2.1s\n",
      "[CV] lr__C=10, lr__penalty=l1, svd__n_components=120 .................\n",
      "[CV]  lr__C=10, lr__penalty=l1, svd__n_components=120, score=nan, total=   1.2s\n",
      "[CV] lr__C=10, lr__penalty=l1, svd__n_components=120 .................\n",
      "[CV]  lr__C=10, lr__penalty=l1, svd__n_components=120, score=nan, total=   1.1s\n",
      "[CV] lr__C=10, lr__penalty=l1, svd__n_components=180 .................\n",
      "[CV]  lr__C=10, lr__penalty=l1, svd__n_components=180, score=nan, total=   1.8s\n",
      "[CV] lr__C=10, lr__penalty=l1, svd__n_components=180 .................\n",
      "[CV]  lr__C=10, lr__penalty=l1, svd__n_components=180, score=nan, total=   1.9s\n",
      "[CV] lr__C=10, lr__penalty=l2, svd__n_components=120 .................\n",
      "[CV]  lr__C=10, lr__penalty=l2, svd__n_components=120, score=-0.778, total=   1.3s\n",
      "[CV] lr__C=10, lr__penalty=l2, svd__n_components=120 .................\n",
      "[CV]  lr__C=10, lr__penalty=l2, svd__n_components=120, score=-0.769, total=   1.3s\n",
      "[CV] lr__C=10, lr__penalty=l2, svd__n_components=180 .................\n",
      "[CV]  lr__C=10, lr__penalty=l2, svd__n_components=180, score=-0.740, total=   1.8s\n",
      "[CV] lr__C=10, lr__penalty=l2, svd__n_components=180 .................\n",
      "[CV]  lr__C=10, lr__penalty=l2, svd__n_components=180, score=-0.735, total=   2.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:   48.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score : -0.737\n",
      "Best parameters set :\n",
      "\tlr__C: 0.1\n",
      "\tlr__penalty: 'l2'\n",
      "\tsvd__n_components: 180\n"
     ]
    }
   ],
   "source": [
    "# Grid Search 모델 초기화\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                    verbose=10, n_jobs=1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Grid Search 모델 적합\n",
    "model.fit(xtrain_tfv, ytrain)  # 여기선 전체 데이터를 사용할 수 있지만 xtrain만 사용한다\n",
    "print('Best score : %0.3f' %model.best_score_)\n",
    "print('Best parameters set :')\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print('\\t%s: %r' %(param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-hurricane",
   "metadata": {},
   "source": [
    "점수는 SVM 점수와 비슷하다. 이 기술은 아래와 같이 xgboost 또는 multinomial naive bayes를 finetuning하는 데 사용할 수 있다. 여기서 tf-idf 데이터를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "precious-involvement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T14:49:26.941702Z",
     "start_time": "2021-03-10T14:49:26.712316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "[CV] nb__alpha=0.001 .................................................\n",
      "[CV] .................... nb__alpha=0.001, score=-0.620, total=   0.0s\n",
      "[CV] nb__alpha=0.001 .................................................\n",
      "[CV] .................... nb__alpha=0.001, score=-0.641, total=   0.0s\n",
      "[CV] nb__alpha=0.01 ..................................................\n",
      "[CV] ..................... nb__alpha=0.01, score=-0.511, total=   0.0s\n",
      "[CV] nb__alpha=0.01 ..................................................\n",
      "[CV] ..................... nb__alpha=0.01, score=-0.523, total=   0.0s\n",
      "[CV] nb__alpha=0.1 ...................................................\n",
      "[CV] ...................... nb__alpha=0.1, score=-0.489, total=   0.0s\n",
      "[CV] nb__alpha=0.1 ...................................................\n",
      "[CV] ...................... nb__alpha=0.1, score=-0.495, total=   0.0s\n",
      "[CV] nb__alpha=1 .....................................................\n",
      "[CV] ........................ nb__alpha=1, score=-0.663, total=   0.0s\n",
      "[CV] nb__alpha=1 .....................................................\n",
      "[CV] ........................ nb__alpha=1, score=-0.666, total=   0.0s\n",
      "[CV] nb__alpha=10 ....................................................\n",
      "[CV] ....................... nb__alpha=10, score=-0.950, total=   0.0s\n",
      "[CV] nb__alpha=10 ....................................................\n",
      "[CV] ....................... nb__alpha=10, score=-0.951, total=   0.0s\n",
      "[CV] nb__alpha=100 ...................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... nb__alpha=100, score=-1.067, total=   0.0s\n",
      "[CV] nb__alpha=100 ...................................................\n",
      "[CV] ...................... nb__alpha=100, score=-1.067, total=   0.0s\n",
      "Best score : -0.492\n",
      "Best parameters set :\n",
      "\tnb__alpha : 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "clf = pipeline.Pipeline([('nb',nb_model)])  # pipleine 생성\n",
    "param_grid = {'nb__alpha':[0.001,0.01,0.1,1,10,100]}  # parameter grid\n",
    "\n",
    "# Grid Search 모델 초기화\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                    verbose=10, n_jobs=1, iid=True, refit=True, cv=2)\n",
    "# Grid Search 모델 적합\n",
    "model.fit(xtrain_tfv, ytrain)  # 전체 데이터 사용 가능하지만 xtrain만 사용\n",
    "print('Best score : %0.3f' %model.best_score_)\n",
    "print('Best parameters set :')\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print('\\t%s : %r' %(param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-strike",
   "metadata": {},
   "source": [
    "원래 naive bayes 점수보다 8% 향상되었다. \n",
    "\n",
    "NLP 문제에서는 일반적으로 단어 벡터를 살펴본다. 단어 벡터를 통해 데이터에 대한 통찰력을 얻을 수 있다. 자세히 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-addition",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-instruction",
   "metadata": {},
   "source": [
    "문장 벡터를 만드는 방법, 그것에 기계학습 모델을 만드는 방법을 간단히 설명하자면, GloVe 벡터를 사용한다. http://www-nlp.stanford.edu/data/glove.840B.300d.zip 여기서 GloVe 를 다운받을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "yellow-catch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T15:29:21.158264Z",
     "start_time": "2021-03-10T15:22:50.542796Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2195997it [06:30, 5622.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195864 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GloVe 벡터를 딕셔너리로 불러오기\n",
    "embeddings_index = {}\n",
    "f = open('../input/glove.840B.300d.txt', encoding='UTF8')\n",
    "for line in tqdm(f):\n",
    "    try:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        f.__next__()\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' %len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "august-implementation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-10T15:46:02.409907Z",
     "start_time": "2021-03-10T15:46:02.389963Z"
    }
   },
   "outputs": [],
   "source": [
    "# 이 함수는 전체 문장을 위한 정규화된 벡터를 만든다.\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words :\n",
    "        try :\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "presidential-acoustic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T00:19:57.443379Z",
     "start_time": "2021-03-11T00:19:48.106932Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 17621/17621 [00:08<00:00, 2076.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1958/1958 [00:00<00:00, 2367.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# trainig, validation 셋을 위해 위의 함수를 사용해 문장 벡터 생성\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "faced-figure",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T00:56:07.887059Z",
     "start_time": "2021-03-11T00:56:07.826224Z"
    }
   },
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-treaty",
   "metadata": {},
   "source": [
    "glove 변수에서 xgboost의 성능을 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "checked-vault",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T01:04:00.445150Z",
     "start_time": "2021-03-11T00:59:19.986509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:59:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:59:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss : 0.726\n"
     ]
    }
   ],
   "source": [
    "# glove 변수에 xgboost 적합\n",
    "clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print('logloss : %0.3f' %multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "reliable-arctic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T01:11:49.034069Z",
     "start_time": "2021-03-11T01:04:29.679981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:04:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:04:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss : 0.688\n"
     ]
    }
   ],
   "source": [
    "# glove 변수에 xgboost 적합\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8,\n",
    "                       subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print('logloss : %0.3f' %multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-living",
   "metadata": {},
   "source": [
    "간단한 매개변수 튜닝으로 GloVe 변수에서 xgboost 점수를 향상시킬 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-western",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-barcelona",
   "metadata": {},
   "source": [
    "하지만 지금은 딥러닝 시대다.  GloVe 변수에 LSTM와 단순 심층 신경망을 학습시킨다. 심층신경망을 먼저 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "outside-seventh",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T01:19:42.342383Z",
     "start_time": "2021-03-11T01:19:42.152888Z"
    }
   },
   "outputs": [],
   "source": [
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "scenic-trace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T01:19:49.339759Z",
     "start_time": "2021-03-11T01:19:49.325796Z"
    }
   },
   "outputs": [],
   "source": [
    "# 신경망의 레이블을 이진화해야한다.\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "alleged-porcelain",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T01:19:53.745469Z",
     "start_time": "2021-03-11T01:19:53.248280Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sequential 신경망에 레이어 3개 생성\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# 모델 compile\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "pressing-young",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T01:20:09.821009Z",
     "start_time": "2021-03-11T01:19:58.609458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "276/276 [==============================] - 5s 9ms/step - loss: 1.0475 - val_loss: 0.7178\n",
      "Epoch 2/5\n",
      "276/276 [==============================] - 2s 5ms/step - loss: 0.7135 - val_loss: 0.6808\n",
      "Epoch 3/5\n",
      "276/276 [==============================] - 2s 6ms/step - loss: 0.6231 - val_loss: 0.6687\n",
      "Epoch 4/5\n",
      "276/276 [==============================] - 1s 5ms/step - loss: 0.5892 - val_loss: 0.6589\n",
      "Epoch 5/5\n",
      "276/276 [==============================] - 2s 6ms/step - loss: 0.5455 - val_loss: 0.6512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24e1fec4430>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, epochs=5,\n",
    "         verbose=1, validation_data=(xvalid_glove_scl, yvalid_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-slide",
   "metadata": {},
   "source": [
    "더 나은 결과를 얻기 위해 신경망의 매개변수를 조정하고, 더 많은 레이어를 추가하고, dropout을 늘려야한다. 여기선 xgboost보다 구현과 실행이 빠르고 더 나은 결과를 얻을 수 있다는 것을 최적화 없이 보여주려고 한다. \n",
    "\n",
    "LSTM을 사용해 텍스트 데이터를 토큰화해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "forty-grace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T01:21:38.688102Z",
     "start_time": "2021-03-11T01:21:36.359945Z"
    }
   },
   "outputs": [],
   "source": [
    "# keras tokenizer 사용\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# sequences에 0 패딩\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "burning-recognition",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T03:51:31.373591Z",
     "start_time": "2021-03-11T03:51:30.775156Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 25943/25943 [00:00<00:00, 45243.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋에 있는 단어에 대한 embedding matrix 생성\n",
    "embedding_matrix = np.zeros((len(word_index)+1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "canadian-criticism",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T03:52:55.027940Z",
     "start_time": "2021-03-11T03:52:54.579250Z"
    }
   },
   "outputs": [],
   "source": [
    "# glove embeddings와 두 dense 레이어를 갖춘 LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, 300,\n",
    "                    weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "alike-tyler",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T06:14:10.277810Z",
     "start_time": "2021-03-11T03:53:13.222629Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 85s 2s/step - loss: 1.0917 - val_loss: 0.9267\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.9304 - val_loss: 0.7624\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 85s 2s/step - loss: 0.8252 - val_loss: 0.7007\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 91s 3s/step - loss: 0.7887 - val_loss: 0.6844\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.7584 - val_loss: 0.6644\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 98s 3s/step - loss: 0.7390 - val_loss: 0.6717\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.7239 - val_loss: 0.6322\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 97s 3s/step - loss: 0.6977 - val_loss: 0.6224\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.6742 - val_loss: 0.6026\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 95s 3s/step - loss: 0.6702 - val_loss: 0.5934\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.6464 - val_loss: 0.5991\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 95s 3s/step - loss: 0.6427 - val_loss: 0.5868\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 77s 2s/step - loss: 0.6192 - val_loss: 0.5607\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 96s 3s/step - loss: 0.5847 - val_loss: 0.5833\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 87s 3s/step - loss: 0.5930 - val_loss: 0.5373\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 85s 2s/step - loss: 0.5657 - val_loss: 0.5459\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 92s 3s/step - loss: 0.5490 - val_loss: 0.5336\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.5462 - val_loss: 0.5328\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 97s 3s/step - loss: 0.5212 - val_loss: 0.5304\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 69s 2s/step - loss: 0.5123 - val_loss: 0.5356\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 100s 3s/step - loss: 0.5152 - val_loss: 0.5177\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 78s 2s/step - loss: 0.4806 - val_loss: 0.5144\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 91s 3s/step - loss: 0.4836 - val_loss: 0.5108\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 89s 3s/step - loss: 0.4583 - val_loss: 0.5161\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 78s 2s/step - loss: 0.4570 - val_loss: 0.4975\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 96s 3s/step - loss: 0.4467 - val_loss: 0.5247\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 69s 2s/step - loss: 0.4594 - val_loss: 0.4957\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 97s 3s/step - loss: 0.4316 - val_loss: 0.4933\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.4225 - val_loss: 0.4861\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 94s 3s/step - loss: 0.4170 - val_loss: 0.5008\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 83s 2s/step - loss: 0.4072 - val_loss: 0.5033\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 85s 2s/step - loss: 0.4094 - val_loss: 0.5152\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 96s 3s/step - loss: 0.4010 - val_loss: 0.5003\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.3914 - val_loss: 0.5307\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 98s 3s/step - loss: 0.3810 - val_loss: 0.5093\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - 69s 2s/step - loss: 0.3932 - val_loss: 0.5135\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - 96s 3s/step - loss: 0.3797 - val_loss: 0.5031\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.3644 - val_loss: 0.5118\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - 91s 3s/step - loss: 0.3606 - val_loss: 0.4951\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - 84s 2s/step - loss: 0.3465 - val_loss: 0.5040\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.3505 - val_loss: 0.5043\n",
      "Epoch 42/100\n",
      "35/35 [==============================] - 94s 3s/step - loss: 0.3437 - val_loss: 0.5061\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.3462 - val_loss: 0.5243\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - 97s 3s/step - loss: 0.3429 - val_loss: 0.5096\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.3309 - val_loss: 0.5462\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - 91s 3s/step - loss: 0.3298 - val_loss: 0.5147\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - 82s 2s/step - loss: 0.3285 - val_loss: 0.5158\n",
      "Epoch 48/100\n",
      "35/35 [==============================] - 78s 2s/step - loss: 0.3227 - val_loss: 0.5134\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - 92s 3s/step - loss: 0.3112 - val_loss: 0.5168\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.3047 - val_loss: 0.5301\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - 97s 3s/step - loss: 0.3034 - val_loss: 0.5249\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.3137 - val_loss: 0.5099\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - 98s 3s/step - loss: 0.3039 - val_loss: 0.5256\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - 69s 2s/step - loss: 0.2980 - val_loss: 0.5152\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - 95s 3s/step - loss: 0.3016 - val_loss: 0.5165\n",
      "Epoch 56/100\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.2931 - val_loss: 0.5037\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - 88s 2s/step - loss: 0.2885 - val_loss: 0.5016\n",
      "Epoch 58/100\n",
      "35/35 [==============================] - 89s 3s/step - loss: 0.2728 - val_loss: 0.5165\n",
      "Epoch 59/100\n",
      "35/35 [==============================] - 82s 2s/step - loss: 0.2763 - val_loss: 0.5423\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - 98s 3s/step - loss: 0.2870 - val_loss: 0.5500\n",
      "Epoch 61/100\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.2755 - val_loss: 0.5410\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - 93s 3s/step - loss: 0.2874 - val_loss: 0.5546\n",
      "Epoch 63/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2582 - val_loss: 0.5608\n",
      "Epoch 64/100\n",
      "35/35 [==============================] - 85s 2s/step - loss: 0.2724 - val_loss: 0.5245\n",
      "Epoch 65/100\n",
      "35/35 [==============================] - 91s 3s/step - loss: 0.2608 - val_loss: 0.5199\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.2504 - val_loss: 0.5348\n",
      "Epoch 67/100\n",
      "35/35 [==============================] - 99s 3s/step - loss: 0.2491 - val_loss: 0.5489\n",
      "Epoch 68/100\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.2530 - val_loss: 0.5465\n",
      "Epoch 69/100\n",
      "35/35 [==============================] - 97s 3s/step - loss: 0.2637 - val_loss: 0.5537\n",
      "Epoch 70/100\n",
      "35/35 [==============================] - 100s 3s/step - loss: 0.2579 - val_loss: 0.5690\n",
      "Epoch 71/100\n",
      "35/35 [==============================] - 85s 2s/step - loss: 0.2589 - val_loss: 0.5501\n",
      "Epoch 72/100\n",
      "35/35 [==============================] - 116s 3s/step - loss: 0.2496 - val_loss: 0.5548\n",
      "Epoch 73/100\n",
      "35/35 [==============================] - 126s 4s/step - loss: 0.2447 - val_loss: 0.5586\n",
      "Epoch 74/100\n",
      "35/35 [==============================] - 91s 3s/step - loss: 0.2342 - val_loss: 0.5561\n",
      "Epoch 75/100\n",
      "35/35 [==============================] - 83s 2s/step - loss: 0.2339 - val_loss: 0.5370\n",
      "Epoch 76/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2463 - val_loss: 0.5253\n",
      "Epoch 77/100\n",
      "35/35 [==============================] - 79s 2s/step - loss: 0.2340 - val_loss: 0.5561\n",
      "Epoch 78/100\n",
      "35/35 [==============================] - 87s 2s/step - loss: 0.2424 - val_loss: 0.5564\n",
      "Epoch 79/100\n",
      "35/35 [==============================] - 83s 2s/step - loss: 0.2288 - val_loss: 0.5719\n",
      "Epoch 80/100\n",
      "35/35 [==============================] - 110s 3s/step - loss: 0.2350 - val_loss: 0.5472\n",
      "Epoch 81/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.2275 - val_loss: 0.5750\n",
      "Epoch 82/100\n",
      "35/35 [==============================] - 94s 3s/step - loss: 0.2270 - val_loss: 0.5668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "35/35 [==============================] - 93s 3s/step - loss: 0.2336 - val_loss: 0.5731\n",
      "Epoch 84/100\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.2209 - val_loss: 0.5622\n",
      "Epoch 85/100\n",
      "35/35 [==============================] - 98s 3s/step - loss: 0.2246 - val_loss: 0.5673\n",
      "Epoch 86/100\n",
      "35/35 [==============================] - 62s 2s/step - loss: 0.2249 - val_loss: 0.5631\n",
      "Epoch 87/100\n",
      "35/35 [==============================] - 88s 3s/step - loss: 0.2196 - val_loss: 0.5670\n",
      "Epoch 88/100\n",
      "35/35 [==============================] - 62s 2s/step - loss: 0.2243 - val_loss: 0.5737\n",
      "Epoch 89/100\n",
      "35/35 [==============================] - 87s 3s/step - loss: 0.2073 - val_loss: 0.5661\n",
      "Epoch 90/100\n",
      "35/35 [==============================] - 64s 2s/step - loss: 0.2212 - val_loss: 0.5431\n",
      "Epoch 91/100\n",
      "35/35 [==============================] - 87s 3s/step - loss: 0.2083 - val_loss: 0.5826\n",
      "Epoch 92/100\n",
      "35/35 [==============================] - 63s 2s/step - loss: 0.2042 - val_loss: 0.5942\n",
      "Epoch 93/100\n",
      "35/35 [==============================] - 86s 2s/step - loss: 0.2111 - val_loss: 0.5512\n",
      "Epoch 94/100\n",
      "35/35 [==============================] - 65s 2s/step - loss: 0.2207 - val_loss: 0.6083\n",
      "Epoch 95/100\n",
      "35/35 [==============================] - 82s 2s/step - loss: 0.2109 - val_loss: 0.5812\n",
      "Epoch 96/100\n",
      "35/35 [==============================] - 69s 2s/step - loss: 0.2171 - val_loss: 0.5981\n",
      "Epoch 97/100\n",
      "35/35 [==============================] - 78s 2s/step - loss: 0.2091 - val_loss: 0.5816\n",
      "Epoch 98/100\n",
      "35/35 [==============================] - 71s 2s/step - loss: 0.2004 - val_loss: 0.6075\n",
      "Epoch 99/100\n",
      "35/35 [==============================] - 80s 2s/step - loss: 0.2048 - val_loss: 0.5786\n",
      "Epoch 100/100\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.2133 - val_loss: 0.5884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24d1ccb74c0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100,\n",
    "         verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-address",
   "metadata": {},
   "source": [
    "이제 점수가 0.5점 미만이다. 최적에서 멈추지 않고 더 많은 epochs를 실행할 것이지만 최적 반복에서 멈추기 위해 조기멈춤을 사용할 수 있다. 조기중지는 모델을 다시 컴파일하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "representative-camping",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T07:52:14.367324Z",
     "start_time": "2021-03-11T07:18:13.262280Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 65s 2s/step - loss: 1.0892 - val_loss: 0.9289\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 84s 2s/step - loss: 0.9232 - val_loss: 0.7476\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 66s 2s/step - loss: 0.8208 - val_loss: 0.7206\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 91s 3s/step - loss: 0.7787 - val_loss: 0.6945\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 66s 2s/step - loss: 0.7743 - val_loss: 0.7361\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 94s 3s/step - loss: 0.7578 - val_loss: 0.6738\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 62s 2s/step - loss: 0.7233 - val_loss: 0.6473\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 89s 3s/step - loss: 0.7133 - val_loss: 0.6434\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 70s 2s/step - loss: 0.6827 - val_loss: 0.6079\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 91s 3s/step - loss: 0.6723 - val_loss: 0.6065\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 69s 2s/step - loss: 0.6428 - val_loss: 0.5839\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 90s 3s/step - loss: 0.6208 - val_loss: 0.5748\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 74s 2s/step - loss: 0.6162 - val_loss: 0.5581\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 84s 2s/step - loss: 0.5899 - val_loss: 0.5526\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 73s 2s/step - loss: 0.5680 - val_loss: 0.5385\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 511s 15s/step - loss: 0.5470 - val_loss: 0.5344\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.5469 - val_loss: 0.5444\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 68s 2s/step - loss: 0.5581 - val_loss: 0.5136\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 72s 2s/step - loss: 0.5201 - val_loss: 0.5183\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 68s 2s/step - loss: 0.5092 - val_loss: 0.5193\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 76s 2s/step - loss: 0.5092 - val_loss: 0.5269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24d2a936550>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glove embedding과 두 dense 레이어를 갖는 LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, 300,\n",
    "                    weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "# 조기중지 callback을 사용해 모델 적합\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100,\n",
    "         verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-problem",
   "metadata": {},
   "source": [
    "한 가지 의문은, 왜 이렇게 많은 dropout을 사용하는지이다. dropout이 없거나 거의 없는 모델을 적합하면 과적합하기 때문이다.\n",
    "\n",
    "양방향 LSTM이 더 나은 결과를 얻을 수 있는지 알아보자. keras로 쉽게 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "killing-athens",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T09:40:18.156833Z",
     "start_time": "2021-03-11T07:57:38.990573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 447s 13s/step - loss: 1.0818 - val_loss: 0.8282\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 467s 13s/step - loss: 0.8783 - val_loss: 0.7249\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 506s 15s/step - loss: 0.8042 - val_loss: 0.6955\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 531s 15s/step - loss: 0.7768 - val_loss: 0.6880\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 544s 15s/step - loss: 0.7551 - val_loss: 0.6776\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 559s 16s/step - loss: 0.7298 - val_loss: 0.6675\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 567s 16s/step - loss: 0.7132 - val_loss: 0.6319\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 479s 14s/step - loss: 0.6816 - val_loss: 0.6390\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 479s 14s/step - loss: 0.6681 - val_loss: 0.5889\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 520s 15s/step - loss: 0.6300 - val_loss: 0.5938\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 519s 15s/step - loss: 0.6155 - val_loss: 0.5925\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 539s 15s/step - loss: 0.5863 - val_loss: 0.6413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24e6004f940>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glove embeddings와 두 dense 레이어를 갖는 양방향 LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, 300,\n",
    "                   weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# 조기중기 callback으로 모델 적합\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100,\n",
    "         verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-generator",
   "metadata": {},
   "source": [
    "거의 다 왔다. GRU의 두 레이어를 시도해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "visible-aircraft",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T13:27:58.521477Z",
     "start_time": "2021-03-11T09:56:00.943083Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 479s 13s/step - loss: 1.0947 - val_loss: 0.8931\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 423s 12s/step - loss: 0.9144 - val_loss: 0.7906\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 427s 12s/step - loss: 0.8345 - val_loss: 0.7673\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 441s 13s/step - loss: 0.7951 - val_loss: 0.7119\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 361s 10s/step - loss: 0.7870 - val_loss: 0.6882\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 396s 11s/step - loss: 0.7410 - val_loss: 0.6759\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 354s 10s/step - loss: 0.7081 - val_loss: 0.6445\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 362s 10s/step - loss: 0.7113 - val_loss: 0.6367\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 376s 11s/step - loss: 0.6745 - val_loss: 0.6278\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 357s 10s/step - loss: 0.6504 - val_loss: 0.5729\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 368s 11s/step - loss: 0.6310 - val_loss: 0.5691\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 378s 11s/step - loss: 0.6062 - val_loss: 0.5644\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 357s 10s/step - loss: 0.5902 - val_loss: 0.5665\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 374s 11s/step - loss: 0.5838 - val_loss: 0.5514\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 380s 11s/step - loss: 0.5494 - val_loss: 0.5269\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 368s 11s/step - loss: 0.5237 - val_loss: 0.5620\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 381s 11s/step - loss: 0.5484 - val_loss: 0.5086\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 380s 11s/step - loss: 0.5043 - val_loss: 0.5382\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 434s 12s/step - loss: 0.4749 - val_loss: 0.4945\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 401s 12s/step - loss: 0.4804 - val_loss: 0.5053\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 401s 11s/step - loss: 0.4637 - val_loss: 0.4866\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 359s 10s/step - loss: 0.4448 - val_loss: 0.5028\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 363s 10s/step - loss: 0.4299 - val_loss: 0.4803\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 376s 11s/step - loss: 0.4433 - val_loss: 0.4852\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 396s 11s/step - loss: 0.4121 - val_loss: 0.4921\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 386s 11s/step - loss: 0.4043 - val_loss: 0.4771\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 371s 11s/step - loss: 0.3889 - val_loss: 0.4650\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 328s 9s/step - loss: 0.3743 - val_loss: 0.4679\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 418s 12s/step - loss: 0.3669 - val_loss: 0.4708\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 366s 10s/step - loss: 0.3506 - val_loss: 0.4632\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 380s 11s/step - loss: 0.3545 - val_loss: 0.4705\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 425s 12s/step - loss: 0.3549 - val_loss: 0.4711\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 348s 10s/step - loss: 0.3356 - val_loss: 0.4924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24e66f9bd90>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glove embeddings와 두 dense 레이어를 가진 GRU\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, 300,\n",
    "                     weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# 조기중기 callback으로 모델 적합\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-helena",
   "metadata": {},
   "source": [
    "최적화 작업을 계속하면 성능이 계속 향상된다. stemming과 lemmatization은 시도할만한 가치가 있지만 지금은 생략한다.\n",
    "\n",
    "이제 앙상블을 확인해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-parking",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "super-penguin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T14:12:22.533981Z",
     "start_time": "2021-03-11T14:12:22.485113Z"
    }
   },
   "outputs": [],
   "source": [
    "# 메인 앙상블 클래스다. 다음 셀에서 어떻게 사용하는지 확인해보자.\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                   format = '[%(asctime)s] %(levelname)s %(message)s',\n",
    "                   datefmt = '%H:%M:%S', stream=sys.stdout)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Ensembler(object):\n",
    "    def __init__(self, model_dict, num_folds=3, task_type='classification',\n",
    "                 optimize=roc_auc_score, lower_is_better=False, save_path=None):\n",
    "        # model_dict : 모델 딕셔너리\n",
    "        # num_folds : 앙상블 위한 fold 수\n",
    "        # task_type : 분류인지 회귀인지\n",
    "        # optimizer : AUC, logloss 등 최적화함수. y_test와 y_pred 두 개의 인수가 있어야함\n",
    "        # lower_is_better : 낮은 최적화 함수 값이 더 좋거나 높음\n",
    "        # save_path : 생성된 예측과 모델 pickle을 dump할 경로, 혹은 None\n",
    "        self.model_dict = model_dict\n",
    "        self.levels = len(self.model_dict)\n",
    "        self.num_folds = num_folds\n",
    "        self.task_type = task_type\n",
    "        self.optimize = optimize\n",
    "        self.lower_is_better = lower_is_better\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        self.training_data = None\n",
    "        self.test_data = None\n",
    "        self.y = None\n",
    "        self.lbl_enc = None\n",
    "        self.y_enc = None\n",
    "        self.train_prediction_dict = None\n",
    "        self.test_prediction_dict = None\n",
    "        self.num_classes = None\n",
    "        \n",
    "    def fit(self, training_data, y, lentrain):\n",
    "        # training_data : tabular 형식의 training 데이터\n",
    "        # y : 이진, 다항, 회귀\n",
    "        # reutrn : 예측에 사용될 모델\n",
    "        self.training_data = training_data\n",
    "        self.y = y\n",
    "        \n",
    "        if self.task_type == 'classification':\n",
    "            self.num_classes = len(np.unique(self.y))\n",
    "            logger.info('Found %d classes', self.num_classes)\n",
    "            self.lbl_enc = preprocessing.LabelEncoder()\n",
    "            self.y_enc = self.lbl_enc.fit_transform(self.y)\n",
    "            kf = StratifiedKFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, self.num_classes)\n",
    "        else:\n",
    "            self.num_classes = -1\n",
    "            self.y_enc = self.y\n",
    "            kf = KFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, 1)\n",
    "        \n",
    "        self.train_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.train_prediction_dict[level] = np.zeros((\n",
    "                train_prediction_shape[0], train_prediction_shape[1]*len(self.model_dict[level])\n",
    "            ))\n",
    "        \n",
    "        for level in range(self.levels):\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level-1]\n",
    "                \n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "                validation_scores = []\n",
    "                foldnum = 1\n",
    "                \n",
    "                for train_index, valid_index in kf.split(self.train_prediction_dict[0], self.y_enc):\n",
    "                    logger.info('Training Level %d Fold #%d. Model #%d', level, foldnum, model_num)\n",
    "                    if level != 0:\n",
    "                        l_training_data = temp_train[train_index]\n",
    "                        l_validation_data = temp_train[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "                    else:\n",
    "                        l0_training_data = temp_train[0][model_num]\n",
    "                        if type(l0_training_data) == list:\n",
    "                            l_training_data = [x[train_index] for x in l0_training_data]\n",
    "                            l_validation_data = [x[valid_index] for x in l0_training_data]\n",
    "                        else:\n",
    "                            l_training_data = l0_training_data[train_index]\n",
    "                            l_validation_data = l0_training_data[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "                    \n",
    "                    if self.task_type == 'classification':\n",
    "                        temp_train_predictions = model.predict_proba(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index, (model_num*self.num_classes):(model_num*self.num_classes)+self.num_classes] = temp_train_predictions\n",
    "                    else:\n",
    "                        temp_train_predictions = model.predict(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index, model_num] = temp_train_predictions\n",
    "                    validation_score = self.optimize(self.y_enc[valid_index], temp_train_predictions)\n",
    "                    validation_scores.append(validation_score)\n",
    "                    logger.info('Level %d. Fold #%d. Model #%d. Validation Score = %f', level, foldnum, model_num, validation_score)\n",
    "                    foldnum += 1\n",
    "                \n",
    "                avg_score = np.mean(validation_scores)\n",
    "                std_score = np.std(validation_scores)\n",
    "            logger.info('Level %d. Model #%d. Mean Score = %f. Std Dev = %f', level, model_num, avg_score, std_score)\n",
    "            train_predictions_df = pd.DataFrame(self.train_prediction_dict[level])\n",
    "            train_predictions_df.to_csv(os.path.join(self.save_path, 'train_predictions_level_' + str(level) + '.csv'), index=False, header=None)\n",
    "            \n",
    "        return self.train_prediction_dict\n",
    "        \n",
    "    def predict(self, test_data, lentest):\n",
    "        self.test_data = test_data\n",
    "        if self.task_type == 'classification':\n",
    "            test_prediction_shape = (lentest, self.num_classes)\n",
    "        else:\n",
    "            test_prediction_shape = (lentest, 1)\n",
    "\n",
    "        self.test_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.test_prediction_dict[level] = np.zeros((\n",
    "                test_prediction_shape[0], test_prediction_shape[1]*len(self.model_dict[level])\n",
    "            ))\n",
    "        self.test_data = test_data\n",
    "\n",
    "        for level in range(self.levels):\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "                temp_test = self.test_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level-1]\n",
    "                temp_test = self.test_prediction_dict[level-1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "                logger.info('Training Fulldata Level %d. Model #%d', level, model_num)\n",
    "                if level == 0:\n",
    "                    model.fit(temp_train[0][model_num], self.y_enc)\n",
    "                else:\n",
    "                    model.fit(temp_train, self.y_enc)\n",
    "                logger.info('Predicting Test Level %d. Model #%d', level, model_num)\n",
    "\n",
    "                if self.task_type == 'classification':\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test)\n",
    "                    self.test_prediction_dict[level][:, (model_num*self.num_classes):(model_num*self.num_classes)+self.num_classes] = temp_test_predictions\n",
    "                else:\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict(temp_test)\n",
    "                    self.test_prediction_dict[level][:, model_num] = temp_test_predictions\n",
    "\n",
    "            test_predictions_df = pd.DataFrame(self.test_prediction_dict[level])\n",
    "            test_predictions_df.to_csv(os.path.join(self.save_path, 'test_predictions_level_' + str(level) + '.csv'), index=False, header=None)\n",
    "\n",
    "        return self.test_prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "miniature-thomson",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T14:12:37.123135Z",
     "start_time": "2021-03-11T14:12:35.202267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:12:35] INFO Found 3 classes\n",
      "[23:12:35] INFO Training Level 0 Fold #1. Model #0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-d1df97b702de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 optimize=multiclass_logloss, lower_is_better=True, save_path='data/')\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlentrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxtrain_glove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlentest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxvalid_glove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-c94a86bad79b>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, training_data, y, lentrain)\u001b[0m\n\u001b[0;32m     88\u001b[0m                             \u001b[0ml_training_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml0_training_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                             \u001b[0ml_validation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml0_training_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m                         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml_training_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_enc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'classification'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m             \u001b[0mprefer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'processes'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m   1408\u001b[0m                                \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m             path_func(X, y, pos_class=class_, Cs=[C_],\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    760\u001b[0m                 \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"iprint\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gtol\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"maxiter\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m             )\n\u001b[1;32m--> 762\u001b[1;33m             n_iter_i = _check_optimize_result(\n\u001b[0m\u001b[0;32m    763\u001b[0m                 \u001b[0msolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                 extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\study\\lib\\site-packages\\sklearn\\utils\\optimize.py\u001b[0m in \u001b[0;36m_check_optimize_result\u001b[1;34m(solver, result, max_iter, extra_warning_msg)\u001b[0m\n\u001b[0;32m    241\u001b[0m                 \u001b[1;34m\"    https://scikit-learn.org/stable/modules/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m                 \u001b[1;34m\"preprocessing.html\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             ).format(solver, result.status, result.message.decode(\"latin1\"))\n\u001b[0m\u001b[0;32m    244\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_warning_msg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[0mwarning_msg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextra_warning_msg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "# 앙상블의 모든 단계에 사용될 데이터 특정\n",
    "train_data_dict = {0: [xtrain_tfv, xtrain_ctv, xtrain_tfv, xtrain_ctv], 1: [xtrain_glove]}\n",
    "test_data_dict = {0: [xvalid_tfv, xvalid_ctv, xvalid_tfv, xvalid_ctv], 1: [xvalid_glove]}\n",
    "\n",
    "model_dict = {0: [LogisticRegression(), LogisticRegression(), MultinomialNB(alpha=0.1), MultinomialNB()],\n",
    "              1: [xgb.XGBClassifier(silent=True, n_estimators=120, max_depth=7)]}\n",
    "\n",
    "ens = Ensembler(model_dict=model_dict, num_folds=3, task_type='classification',\n",
    "                optimize=multiclass_logloss, lower_is_better=True, save_path='data/')\n",
    "\n",
    "ens.fit(train_data_dict, ytrain, lentrain=xtrain_glove.shape[0])\n",
    "preds = ens.predict(test_data_dict, lentest=xvalid_glove.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오류 확인\n",
    "multiclass_logloss(yvalid, preds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-privilege",
   "metadata": {},
   "source": [
    "따라서 앙상블 점수가 크게 향상되는 것을 볼 수 있다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
