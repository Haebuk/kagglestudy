{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- competition/dataset : [https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/data](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/data)\n",
    "- date : 2021/02/22\n",
    "- original : [https://www.kaggle.com/ivallesp/wavception-v1-a-1-d-inception-approach-lb-0-76](https://www.kaggle.com/ivallesp/wavception-v1-a-1-d-inception-approach-lb-0-76)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WavCeption V1: a 1-D Inception approach (LB 0.76)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-19T23:23:23.804833Z",
     "iopub.status.busy": "2021-01-19T23:23:23.803834Z",
     "iopub.status.idle": "2021-01-19T23:23:23.811814Z",
     "shell.execute_reply": "2021-01-19T23:23:23.810817Z",
     "shell.execute_reply.started": "2021-01-19T23:23:23.804833Z"
    }
   },
   "source": [
    "**✏ 필사 1회** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WavCeption V1: just a 1-D Inception approach\n",
    "그저 가지고 놀던 작은 장난감을 공유하고 싶었으며, 놀라운 결과를 주었습니다. **WavCeption V1** 네트워크가 일반적인 컨볼루션 신경망에 비해 인상적인 결과를 주는 것 같지만 이 컴피티션에서는 전처리와 알려지지 않은 트랙 관리에 대한 어려움이 있어 보입니다.  \n",
    "\n",
    "몇 주 전에 많은 모듈들을 완전히 연결하여 1D-inception network를 쉽게 구축할 수 있도록 구현한 모듈을 작성했습니다.  \n",
    "\n",
    "불행히도 kaggle의 여러 제약들로 인해 커널에서는 잘 실행되지 않으므로 직접 다운받아 실행하는 것을 추천합니다.  \n",
    "\n",
    "너무 많이 고군분투하지 않고 12시간 동안 모델을 실행시킴으로써 리더보드에서 0.76이라는 점수를 달성했습니다. (0.84 in local test) 같은 라인의 다른 시도에서는 0.89점까지도 나왔으며, 따라서 알려지지 않은 영상을 다루는 방법이 매우 개선되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load modules and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:43:21.661542Z",
     "iopub.status.busy": "2021-02-22T04:43:21.661542Z",
     "iopub.status.idle": "2021-02-22T04:43:21.686474Z",
     "shell.execute_reply": "2021-02-22T04:43:21.685477Z",
     "shell.execute_reply.started": "2021-02-22T04:43:21.661542Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import IPython\n",
    "from numpy.fft import rfft, irfft\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise generation functions\n",
    "출처:\n",
    "[https://github.com/python-acoustics/python-acoustics/blob/master/acoustics/generator.py](https://github.com/python-acoustics/python-acoustics/blob/master/acoustics/generator.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T03:13:05.535994Z",
     "iopub.status.busy": "2021-02-22T03:13:05.534966Z",
     "iopub.status.idle": "2021-02-22T03:13:05.541946Z",
     "shell.execute_reply": "2021-02-22T03:13:05.540948Z",
     "shell.execute_reply.started": "2021-02-22T03:13:05.535994Z"
    }
   },
   "outputs": [],
   "source": [
    "def ms(x):\n",
    "    '''\n",
    "    신호 x의 제곱의 평균값\n",
    "    param x: 동적인 양\n",
    "    returns: x의 제곱의 평균\n",
    "    '''\n",
    "    return (np.abs(x)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T03:51:20.844200Z",
     "iopub.status.busy": "2021-02-22T03:51:20.843203Z",
     "iopub.status.idle": "2021-02-22T03:51:20.862152Z",
     "shell.execute_reply": "2021-02-22T03:51:20.861182Z",
     "shell.execute_reply.started": "2021-02-22T03:51:20.844200Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(y, x=None):\n",
    "    if x is not None:\n",
    "        x = ms(x)\n",
    "    else:\n",
    "        x = 1.0\n",
    "    return y * np.sqrt(x / ms(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T03:53:02.781279Z",
     "iopub.status.busy": "2021-02-22T03:53:02.781279Z",
     "iopub.status.idle": "2021-02-22T03:53:02.788231Z",
     "shell.execute_reply": "2021-02-22T03:53:02.787232Z",
     "shell.execute_reply.started": "2021-02-22T03:53:02.781279Z"
    }
   },
   "outputs": [],
   "source": [
    "def white_noise(N, state=None):\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    return state.randn(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T03:57:11.297377Z",
     "iopub.status.busy": "2021-02-22T03:57:11.297377Z",
     "iopub.status.idle": "2021-02-22T03:57:11.310343Z",
     "shell.execute_reply": "2021-02-22T03:57:11.309357Z",
     "shell.execute_reply.started": "2021-02-22T03:57:11.297377Z"
    }
   },
   "outputs": [],
   "source": [
    "def pink_noise(N, state=None):\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    uneven = N % 2\n",
    "    X = state.randn(N//2 + 1 + uneven) + 1j * state.randn(N//2 + 1 + uneven)\n",
    "    S = np.sqrt(np.arange(len(X)) + 1.)\n",
    "    y = (irfft(X / S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    return normalize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T03:57:46.016684Z",
     "iopub.status.busy": "2021-02-22T03:57:46.015687Z",
     "iopub.status.idle": "2021-02-22T03:57:46.031645Z",
     "shell.execute_reply": "2021-02-22T03:57:46.030647Z",
     "shell.execute_reply.started": "2021-02-22T03:57:46.016684Z"
    }
   },
   "outputs": [],
   "source": [
    "def blue_noise(N, state=None):\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    uneven = N%2\n",
    "    X = state.randn(N//2+1+uneven) + 1j * state.randn(N//2 + 1 + uneven)\n",
    "    S = np.sqrt(np.arange(len(X)))\n",
    "    y = (irfft(X * S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    return normalize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T03:58:14.859981Z",
     "iopub.status.busy": "2021-02-22T03:58:14.858985Z",
     "iopub.status.idle": "2021-02-22T03:58:14.876936Z",
     "shell.execute_reply": "2021-02-22T03:58:14.875939Z",
     "shell.execute_reply.started": "2021-02-22T03:58:14.858985Z"
    }
   },
   "outputs": [],
   "source": [
    "def brown_noise(N, state=None):\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    uneven = N % 2\n",
    "    X = state.randn(N//2 + 1 + uneven) + 1j * state.randn(N//2 + 1 + uneven)\n",
    "    S = (np.arange(len(X)) + 1)\n",
    "    y = (irfft(X / S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    return normalize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T03:58:47.172124Z",
     "iopub.status.busy": "2021-02-22T03:58:47.172124Z",
     "iopub.status.idle": "2021-02-22T03:58:47.191073Z",
     "shell.execute_reply": "2021-02-22T03:58:47.190097Z",
     "shell.execute_reply.started": "2021-02-22T03:58:47.172124Z"
    }
   },
   "outputs": [],
   "source": [
    "def violet_noise(N, state=None):\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    uneven = N % 2\n",
    "    X = state.randn(N//2 + 1 + uneven) + 1j * state.randn(N//2 + 1 + uneven)\n",
    "    S = (np.arange(len(X)))\n",
    "    y = (irfft(X * S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    return normalize(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:01:58.109661Z",
     "iopub.status.busy": "2021-02-22T04:01:58.108664Z",
     "iopub.status.idle": "2021-02-22T04:01:58.121629Z",
     "shell.execute_reply": "2021-02-22T04:01:58.120631Z",
     "shell.execute_reply.started": "2021-02-22T04:01:58.109661Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_tensorflow_configuration(device='0', memory_fraction=1):\n",
    "    '''\n",
    "    사용할 GPU와 프로세스에서 사용할 수 있는 메모리 양 선택\n",
    "    :param device: 사용할 디바이스 (str)\n",
    "    :param memory_fraction: 할당해야 하는 메모리 비율 (float)\n",
    "    :return: 세션에 전달할 구성 (tf object)\n",
    "    '''\n",
    "    device = str(device)\n",
    "    config = tf.ConfigProto()\n",
    "    config.allow_soft_placement = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = memory_fraction\n",
    "    config.gpu_options.visible_device_list = device\n",
    "    return(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:06:56.831265Z",
     "iopub.status.busy": "2021-02-22T04:06:56.830268Z",
     "iopub.status.idle": "2021-02-22T04:06:56.843233Z",
     "shell.execute_reply": "2021-02-22T04:06:56.842236Z",
     "shell.execute_reply.started": "2021-02-22T04:06:56.831265Z"
    }
   },
   "outputs": [],
   "source": [
    "def start_tensorflow_session(device=\"0\", memory_fraction=1):\n",
    "    '''\n",
    "    사용할 GPU 장치를 관리하는 tensorflow 세션 시작\n",
    "    :device: 장치 번호 (str)\n",
    "    :memory_fraction: 지정된 장치에서 미리 할당할 메모리 비율 (float [0, 1])\n",
    "    :return: 구성된 tf 세션\n",
    "    '''\n",
    "    return(tf.Session(config=get_tensorflow_configuration(device=device, memory_fraction=memory_fraction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:09:30.564626Z",
     "iopub.status.busy": "2021-02-22T04:09:30.563598Z",
     "iopub.status.idle": "2021-02-22T04:09:30.583545Z",
     "shell.execute_reply": "2021-02-22T04:09:30.582547Z",
     "shell.execute_reply.started": "2021-02-22T04:09:30.564626Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_summary_writer(session, logs_path, project_id, version_id):\n",
    "    '''\n",
    "    텐서보드 리포팅\n",
    "    :param session: 텐서 세션 오픈 (tf.Session)\n",
    "    :param logs_path: 텐서보드가 로그를 찾는 경로 (str)\n",
    "    :param project_id: 리포팅 목적의 프로젝트 이름 (str)\n",
    "    :param version_id: 리포팅 목적의 버전 이름 (str)\n",
    "    :return summary_writer: 텐서보드 writer\n",
    "    '''\n",
    "    path = os.path.join(logs_path,\"{}_{}\".format(project_id, version_id)) \n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    summary_writer = tf.summary.FileWriter(path, graph_def=session.graph_def)\n",
    "    return(summary_writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths management module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:12:24.855111Z",
     "iopub.status.busy": "2021-02-22T04:12:24.854113Z",
     "iopub.status.idle": "2021-02-22T04:12:24.867079Z",
     "shell.execute_reply": "2021-02-22T04:12:24.866081Z",
     "shell.execute_reply.started": "2021-02-22T04:12:24.855111Z"
    }
   },
   "outputs": [],
   "source": [
    "def _norm_path(path):\n",
    "    '''\n",
    "    경로 검색 기능의 출력을 정규화하는데 사용하기 위한 장식 함수\n",
    "    슬래시/백슬래시 창을 고정하는데 유용함\n",
    "    '''\n",
    "    def normalize_path(*args, **kwargs):\n",
    "        return os.path.normpath(path(*args, **kwargs))\n",
    "    return normalize_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:19:02.407153Z",
     "iopub.status.busy": "2021-02-22T04:19:02.407153Z",
     "iopub.status.idle": "2021-02-22T04:19:02.423110Z",
     "shell.execute_reply": "2021-02-22T04:19:02.422142Z",
     "shell.execute_reply.started": "2021-02-22T04:19:02.407153Z"
    }
   },
   "outputs": [],
   "source": [
    "def _assure_path_exists(path):\n",
    "    '''\n",
    "    경로 검색 기능의 출력의 존재 여부를 확인하기 위한 장식 함수\n",
    "    슬래시/백슬래시 창을 고정하는데 유용함\n",
    "    '''\n",
    "    def assure_exists(*args, **kwargs):\n",
    "        p=path(*args, **kwargs)\n",
    "        assert os.path.exists(p), \"the following path does not exist: '{}'\".format(p)\n",
    "        return p\n",
    "    return assure_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:21:37.956725Z",
     "iopub.status.busy": "2021-02-22T04:21:37.956725Z",
     "iopub.status.idle": "2021-02-22T04:21:37.968694Z",
     "shell.execute_reply": "2021-02-22T04:21:37.967696Z",
     "shell.execute_reply.started": "2021-02-22T04:21:37.956725Z"
    }
   },
   "outputs": [],
   "source": [
    "def _is_output_path(path):\n",
    "    '''\n",
    "    경로 검색 기능의 출력에 적용되는 함수를 그룹화하기 위한 장식 함수\n",
    "    슬래시/백슬래시 창을 고정하는데 유용함\n",
    "    '''\n",
    "    @_norm_path\n",
    "    @_assure_path_exists\n",
    "    def check_existence_or_create_it(*args, **kwargs):\n",
    "        if not os.path.exists(path(*args, **kwargs)):\n",
    "            \"Path does not exist... creating it: {}\".format(path(*args, **kwargs))\n",
    "            os.makedirs(path(*args, **kwargs))\n",
    "        return path(*args, **kwargs)\n",
    "    return check_existence_or_create_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:22:29.924325Z",
     "iopub.status.busy": "2021-02-22T04:22:29.924325Z",
     "iopub.status.idle": "2021-02-22T04:22:29.938288Z",
     "shell.execute_reply": "2021-02-22T04:22:29.937312Z",
     "shell.execute_reply.started": "2021-02-22T04:22:29.924325Z"
    }
   },
   "outputs": [],
   "source": [
    "def _is_input_path(path):\n",
    "    '''\n",
    "    경로 검색 기능의 입력에 적용되는 함수를 그룹화하기 위한 장식 함수\n",
    "    '''\n",
    "    @_norm_path\n",
    "    @_assure_path_exists\n",
    "    def check_existence(*args, **kwargs):\n",
    "        return path(*args, **kwargs)\n",
    "    return check_existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:23:35.185154Z",
     "iopub.status.busy": "2021-02-22T04:23:35.185154Z",
     "iopub.status.idle": "2021-02-22T04:23:35.197123Z",
     "shell.execute_reply": "2021-02-22T04:23:35.196125Z",
     "shell.execute_reply.started": "2021-02-22T04:23:35.185154Z"
    }
   },
   "outputs": [],
   "source": [
    "@_is_input_path\n",
    "def get_train_path():\n",
    "    path = \"data/train\"\n",
    "    return path\n",
    "\n",
    "@_is_input_path\n",
    "def get_test_path():\n",
    "    path = \"data/test\"\n",
    "    return path\n",
    "\n",
    "@_is_input_path\n",
    "def get_train_audio_path():\n",
    "    path = os.path.join(get_train_path(), \"audio\")\n",
    "    return path\n",
    "\n",
    "@_is_input_path\n",
    "def get_scoring_audio_path():\n",
    "    path = os.path.join(get_test_path(), \"audio\")\n",
    "    return path\n",
    "\n",
    "@_is_output_path\n",
    "def get_submissions_path():\n",
    "    path = \"data\"\n",
    "    return path\n",
    "\n",
    "@_is_output_path\n",
    "def get_silence_path():\n",
    "    path = \"data\"\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:23:50.576235Z",
     "iopub.status.busy": "2021-02-22T04:23:50.576235Z",
     "iopub.status.idle": "2021-02-22T04:23:50.588202Z",
     "shell.execute_reply": "2021-02-22T04:23:50.587205Z",
     "shell.execute_reply.started": "2021-02-22T04:23:50.576235Z"
    }
   },
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def batching(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:26:15.460322Z",
     "iopub.status.busy": "2021-02-22T04:26:15.459295Z",
     "iopub.status.idle": "2021-02-22T04:26:15.470266Z",
     "shell.execute_reply": "2021-02-22T04:26:15.469302Z",
     "shell.execute_reply.started": "2021-02-22T04:26:15.460322Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_wav(filepath, pad=True):\n",
    "    '''\n",
    "    wav 파일의 경로를 입력받아 정규화하고 16k개의 샘플을 가지고 있도록 패딩하는 함수\n",
    "    :param filepath: wav 파일의 경로 (str)\n",
    "    :param pad: 패딩 필요 여부 (bool)\n",
    "    :returns: 샘플과 타겟 변수 (tuple of (np.array, str))\n",
    "    '''\n",
    "    sample_rate, x = wavfile.read(filepath)\n",
    "    target = os.path.split(os.path.split(filepath)[0])[1]\n",
    "    assert sample_rate == 16000\n",
    "    if pad:\n",
    "        return np.pad(x, (0, 16000-len(x)), mode=\"constant\")/32768, target\n",
    "    else:\n",
    "        return x/32768, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:29:10.552893Z",
     "iopub.status.busy": "2021-02-22T04:29:10.551898Z",
     "iopub.status.idle": "2021-02-22T04:29:10.563865Z",
     "shell.execute_reply": "2021-02-22T04:29:10.562867Z",
     "shell.execute_reply.started": "2021-02-22T04:29:10.552893Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batcher(list_of_paths, batch_size, label_encoder=None, scoring=False):\n",
    "    '''\n",
    "    배치 리스트에 대한 배치 generator 구축\n",
    "    :param list_of_paths: 형식적인 요소가 있는 튜플 리스트 (filepath, target) (list)\n",
    "    :param batch_size: 배치 사이즈 (int)\n",
    "    :param label_encoder: fitted LabelEncoder (sklearn.LabelEncoder|optional)\n",
    "    :param scoring: 타겟 고려 여부 (bool)\n",
    "    :returns: 배치 generator\n",
    "    '''\n",
    "    for filepaths in batching(list_of_paths, batch_size):\n",
    "        wavs, targets = zip(*list(map(read_wav, filepaths)))\n",
    "        if scoring:\n",
    "            yield np.expand_dims(np.row_stack(wavs), 2), filepaths\n",
    "        else:\n",
    "            if label_encoder is None:\n",
    "                yield np.expand_dims(np.row_stack(wavs), 2), np.row_stack(targets)\n",
    "            else:\n",
    "                yield np.expand_dims(np.row_stack(wavs), 2), np.expand_dims(label_encoder.transform(np.squeeze(targets)),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:49:21.510197Z",
     "iopub.status.busy": "2021-02-22T04:49:21.509199Z",
     "iopub.status.idle": "2021-02-22T04:49:21.520171Z",
     "shell.execute_reply": "2021-02-22T04:49:21.519194Z",
     "shell.execute_reply.started": "2021-02-22T04:49:21.510197Z"
    }
   },
   "outputs": [],
   "source": [
    "class BatchNorm(object):\n",
    "    def __init__(self, epsilon=1e-5, momentum=0.999, name=\"batch_norm\"):\n",
    "        with tf.compat.v1.variable_scope(name):\n",
    "            self.epsilon = epsilon\n",
    "            self.momentum = momentum\n",
    "            self.name = name\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        return tf.contrib.layers.batch_norm(x,\n",
    "                                            decay=self.momentum,\n",
    "                                            updates_collections=None,\n",
    "                                            epsilon=self.epsilon,\n",
    "                                            scale=True,\n",
    "                                            is_training=train,\n",
    "                                            scope=self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:49:21.734186Z",
     "iopub.status.busy": "2021-02-22T04:49:21.733188Z",
     "iopub.status.idle": "2021-02-22T04:49:21.767098Z",
     "shell.execute_reply": "2021-02-22T04:49:21.766100Z",
     "shell.execute_reply.started": "2021-02-22T04:49:21.734186Z"
    }
   },
   "outputs": [],
   "source": [
    "def inception_1d(x, is_train, depth, norm_function, activ_function, name):\n",
    "    \"\"\"\n",
    "    Inception 1D 모듈 구현\n",
    "    :param x: 현재 모듈 입력 (4D tensor with channels-last)\n",
    "    :param is_train: it is intented to be a boolean placeholder for controling the BatchNormalization behavior (0D tensor)\n",
    "    :param depth: linearly controls the depth of the network (int)\n",
    "    :param norm_function: normalization class (same format as the BatchNorm class above)\n",
    "    :param activ_function: tensorflow activation function (e.g. tf.nn.relu) \n",
    "    :param name: name of the variable scope (str)\n",
    "    \"\"\"\n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "        x_norm = norm_function(name=\"norm_input\")(x, train=is_train)\n",
    "\n",
    "        # Branch 1: 64 x conv 1x1 \n",
    "        branch_conv_1_1 = tf.layers.conv1d(inputs=x_norm, filters=16*depth, kernel_size=1,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_1_1\")\n",
    "        branch_conv_1_1 = norm_function(name=\"norm_conv_1_1\")(branch_conv_1_1, train=is_train)\n",
    "        branch_conv_1_1 = activ_function(branch_conv_1_1, \"activation_1_1\")\n",
    "\n",
    "        # Branch 2: 128 x conv 3x3 \n",
    "        branch_conv_3_3 = tf.layers.conv1d(inputs=x_norm, filters=16, kernel_size=1, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_3_3_1\")\n",
    "        branch_conv_3_3 = norm_function(name=\"norm_conv_3_3_1\")(branch_conv_3_3, train=is_train)\n",
    "        branch_conv_3_3 = activ_function(branch_conv_3_3, \"activation_3_3_1\")\n",
    "\n",
    "        branch_conv_3_3 = tf.layers.conv1d(inputs=branch_conv_3_3, filters=32*depth, kernel_size=3, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_3_3_2\")\n",
    "        branch_conv_3_3 = norm_function(name=\"norm_conv_3_3_2\")(branch_conv_3_3, train=is_train)\n",
    "        branch_conv_3_3 = activ_function(branch_conv_3_3, \"activation_3_3_2\")\n",
    "\n",
    "        # Branch 3: 128 x conv 5x5 \n",
    "        branch_conv_5_5 = tf.layers.conv1d(inputs=x_norm, filters=16, kernel_size=1, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_5_5_1\")\n",
    "        branch_conv_5_5 = norm_function(name=\"norm_conv_5_5_1\")(branch_conv_5_5, train=is_train)\n",
    "        branch_conv_5_5 = activ_function(branch_conv_5_5, \"activation_5_5_1\")\n",
    "\n",
    "        branch_conv_5_5 = tf.layers.conv1d(inputs=branch_conv_5_5, filters=32*depth, kernel_size=5, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_5_5_2\")\n",
    "        branch_conv_5_5 = norm_function(name=\"norm_conv_5_5_2\")(branch_conv_5_5, train=is_train)\n",
    "        branch_conv_5_5 = activ_function(branch_conv_5_5, \"activation_5_5_2\")\n",
    "\n",
    "        # Branch 4: 128 x conv 7x7\n",
    "        branch_conv_7_7 = tf.layers.conv1d(inputs=x_norm, filters=16, kernel_size=1, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_7_7_1\")\n",
    "        branch_conv_7_7 = norm_function(name=\"norm_conv_7_7_1\")(branch_conv_7_7, train=is_train)\n",
    "        branch_conv_7_7 = activ_function(branch_conv_7_7, \"activation_7_7_1\")\n",
    "\n",
    "        branch_conv_7_7 = tf.layers.conv1d(inputs=branch_conv_7_7, filters=32*depth, kernel_size=5, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_7_7_2\")\n",
    "        branch_conv_7_7 = norm_function(name=\"norm_conv_7_7_2\")(branch_conv_7_7, train=is_train)\n",
    "        branch_conv_7_7 = activ_function(branch_conv_7_7, \"activation_7_7_2\")\n",
    "\n",
    "        # Branch 5: 16 x (max_pool 3x3 + conv 1x1)\n",
    "        branch_maxpool_3_3 = tf.layers.max_pooling1d(inputs=x_norm, pool_size=3, strides=1, padding=\"same\", name=\"maxpool_3\")\n",
    "        branch_maxpool_3_3 = norm_function(name=\"norm_maxpool_3_3\")(branch_maxpool_3_3, train=is_train)\n",
    "        branch_maxpool_3_3 = tf.layers.conv1d(inputs=branch_maxpool_3_3, filters=16, kernel_size=1, \n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              padding=\"same\", name=\"conv_maxpool_3\")\n",
    "\n",
    "        # Branch 6: 16 x (max_pool 5x5 + conv 1x1)\n",
    "        branch_maxpool_5_5 = tf.layers.max_pooling1d(inputs=x_norm, pool_size=5, strides=1, padding=\"same\", name=\"maxpool_5\")\n",
    "        branch_maxpool_5_5 = norm_function(name=\"norm_maxpool_5_5\")(branch_maxpool_5_5, train=is_train)\n",
    "        branch_maxpool_5_5 = tf.layers.conv1d(inputs=branch_maxpool_5_5, filters=16, kernel_size=1, \n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              padding=\"same\", name=\"conv_maxpool_5\")\n",
    "\n",
    "        # Branch 7: 16 x (avg_pool 3x3 + conv 1x1)\n",
    "        branch_avgpool_3_3 = tf.layers.average_pooling1d(inputs=x_norm, pool_size=3, strides=1, padding=\"same\", name=\"avgpool_3\")\n",
    "        branch_avgpool_3_3 = norm_function(name=\"norm_avgpool_3_3\")(branch_avgpool_3_3, train=is_train)\n",
    "        branch_avgpool_3_3 = tf.layers.conv1d(inputs=branch_avgpool_3_3, filters=16, kernel_size=1,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              padding=\"same\", name=\"conv_avgpool_3\")\n",
    "\n",
    "        # Branch 8: 16 x (avg_pool 5x5 + conv 1x1)\n",
    "        branch_avgpool_5_5 = tf.layers.average_pooling1d(inputs=x_norm, pool_size=5, strides=1, padding=\"same\", name=\"avgpool_5\")\n",
    "        branch_avgpool_5_5 = norm_function(name=\"norm_avgpool_5_5\")(branch_avgpool_5_5, train=is_train)\n",
    "        branch_avgpool_5_5 = tf.layers.conv1d(inputs=branch_avgpool_5_5, filters=16, kernel_size=1, \n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              padding=\"same\", name=\"conv_avgpool_5\")\n",
    "\n",
    "        # Concatenate\n",
    "        output = tf.concat([branch_conv_1_1, branch_conv_3_3, branch_conv_5_5, branch_conv_7_7, branch_maxpool_3_3, \n",
    "                           branch_maxpool_5_5, branch_avgpool_3_3, branch_avgpool_5_5], axis=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:49:22.102042Z",
     "iopub.status.busy": "2021-02-22T04:49:22.101044Z",
     "iopub.status.idle": "2021-02-22T04:49:22.737384Z",
     "shell.execute_reply": "2021-02-22T04:49:22.736386Z",
     "shell.execute_reply.started": "2021-02-22T04:49:22.102042Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-dd808212f793>:8: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sample_rate, x = wavfile.read(filepath)\n"
     ]
    }
   ],
   "source": [
    "filepaths_noise = glob.glob(os.path.join(get_train_audio_path(), \"_background_noise_\", \"*.wav\"))\n",
    "\n",
    "noise = np.concatenate(list(map(lambda x: read_wav(x, False)[0], filepaths_noise)))\n",
    "noise = np.concatenate([noise, noise[::-1]])\n",
    "synthetic_noise = np.concatenate([white_noise( N=16000*30, state=np.random.RandomState(655321)), \n",
    "                                  blue_noise(  N=16000*30, state=np.random.RandomState(655321)),\n",
    "                                  pink_noise(  N=16000*30, state=np.random.RandomState(655321)),\n",
    "                                  brown_noise( N=16000*30, state=np.random.RandomState(655321)),\n",
    "                                  violet_noise(N=16000*30, state=np.random.RandomState(655321)),\n",
    "                                  np.zeros(16000*60)])\n",
    "synthetic_noise /= np.max(np.abs(synthetic_noise))\n",
    "synthetic_noise = np.concatenate([synthetic_noise, (synthetic_noise+synthetic_noise[::-1])/2])\n",
    "all_noise = np.concatenate([noise, synthetic_noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:49:22.739378Z",
     "iopub.status.busy": "2021-02-22T04:49:22.738381Z",
     "iopub.status.idle": "2021-02-22T04:49:57.190658Z",
     "shell.execute_reply": "2021-02-22T04:49:57.189659Z",
     "shell.execute_reply.started": "2021-02-22T04:49:22.739378Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████▎                                      | 3983/8000 [00:18<00:15, 265.13it/s]<ipython-input-53-fa034f2a4b63>:17: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ((32767*clip/np.max(np.abs(clip))).astype(np.int16)))\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:34<00:00, 232.33it/s]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(655321)\n",
    "random.seed(655321)\n",
    "\n",
    "path = get_silence_path()\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path) # It fails in kaggle kernel due to the read-only filesystem\n",
    "\n",
    "for noise_clip_no in tqdm(range(8000)):\n",
    "    if noise_clip_no<=4000:\n",
    "        idx = np.random.randint(0, len(noise)-16000)\n",
    "        clip = noise[idx:(idx+16000)]\n",
    "    else:\n",
    "        idx = np.random.randint(0, len(synthetic_noise)-16000)\n",
    "        clip = synthetic_noise[idx:(idx+16000)]\n",
    "    wavfile.write(os.path.join(path, \"{0:04d}.wav\".format(noise_clip_no)), 16000, \n",
    "                               ((32767*clip/np.max(np.abs(clip))).astype(np.int16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:49:57.193649Z",
     "iopub.status.busy": "2021-02-22T04:49:57.192653Z",
     "iopub.status.idle": "2021-02-22T04:50:04.133331Z",
     "shell.execute_reply": "2021-02-22T04:50:04.132334Z",
     "shell.execute_reply.started": "2021-02-22T04:49:57.193649Z"
    }
   },
   "outputs": [],
   "source": [
    "filepaths = glob.glob(os.path.join(get_train_audio_path(), \"**/*.wav\"), recursive=True)\n",
    "filepaths += glob.glob(os.path.join(get_silence_path(), \"**/*.wav\"), recursive=True)\n",
    "filepaths = list(filter(lambda fp: \"_background_noise_\" not in fp, filepaths))\n",
    "validation_list = open(os.path.join(get_train_path(), \"validation_list.txt\")).readlines()\n",
    "test_list = open(os.path.join(get_train_path(), \"testing_list.txt\")).readlines()\n",
    "validation_list = list(map(lambda fn: os.path.join(get_train_audio_path(), fn.strip()), validation_list))\n",
    "testing_list = list(map(lambda fn: os.path.join(get_train_audio_path(), fn.strip()), test_list))\n",
    "training_list = np.setdiff1d(filepaths, validation_list+testing_list).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:50:04.135324Z",
     "iopub.status.busy": "2021-02-22T04:50:04.135324Z",
     "iopub.status.idle": "2021-02-22T04:50:04.706339Z",
     "shell.execute_reply": "2021-02-22T04:50:04.705340Z",
     "shell.execute_reply.started": "2021-02-22T04:50:04.135324Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(655321)\n",
    "random.shuffle(filepaths)\n",
    "random.shuffle(validation_list)\n",
    "random.shuffle(testing_list)\n",
    "random.shuffle(training_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:50:04.707336Z",
     "iopub.status.busy": "2021-02-22T04:50:04.707336Z",
     "iopub.status.idle": "2021-02-22T04:50:05.470296Z",
     "shell.execute_reply": "2021-02-22T04:50:05.469297Z",
     "shell.execute_reply.started": "2021-02-22T04:50:04.707336Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-07d55d41c003>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Test number of files and their consistencies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\".wav\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepaths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m64727\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m6\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m8000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepaths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m6798\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m6835\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m6798\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Quick Unit-Tests\n",
    "# Test number of files and their consistencies\n",
    "assert all(map(lambda fp: os.path.splitext(fp)[1]==\".wav\", filepaths))\n",
    "assert len(filepaths) == 64727 - 6 + 8000\n",
    "assert len(training_list) == len(filepaths) - 6798 - 6835 \n",
    "assert len(validation_list) == 6798\n",
    "assert len(testing_list) == 6835\n",
    "\n",
    "# Test file existence\n",
    "assert all(map(lambda fn: os.path.exists(os.path.join(fn)), validation_list))\n",
    "assert all(map(lambda fn: os.path.exists(os.path.join(fn)), testing_list))\n",
    "assert all(map(lambda fn: os.path.exists(os.path.join(fn)), training_list))\n",
    "assert set(validation_list + testing_list + training_list) == set(filepaths)\n",
    "\n",
    "# Test non-overlap among sets\n",
    "assert len(np.intersect1d(validation_list, testing_list))==0\n",
    "assert len(np.intersect1d(training_list, testing_list))==0\n",
    "assert len(np.intersect1d(training_list, validation_list))==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:50:10.218228Z",
     "iopub.status.busy": "2021-02-22T04:50:10.217232Z",
     "iopub.status.idle": "2021-02-22T04:50:14.731030Z",
     "shell.execute_reply": "2021-02-22T04:50:14.730063Z",
     "shell.execute_reply.started": "2021-02-22T04:50:10.218228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'audio': 158538,\n",
       "         'one': 4740,\n",
       "         'zero': 4752,\n",
       "         'four': 4744,\n",
       "         'nine': 4728,\n",
       "         'house': 3500,\n",
       "         'off': 4714,\n",
       "         'bed': 3426,\n",
       "         'eight': 4704,\n",
       "         'down': 4718,\n",
       "         'go': 4744,\n",
       "         'up': 4750,\n",
       "         'left': 4706,\n",
       "         'five': 4714,\n",
       "         'data': 8000,\n",
       "         'bird': 3462,\n",
       "         'two': 4746,\n",
       "         'no': 4750,\n",
       "         'wow': 3490,\n",
       "         'yes': 4754,\n",
       "         'tree': 3466,\n",
       "         'dog': 3492,\n",
       "         'three': 4712,\n",
       "         'on': 4734,\n",
       "         'six': 4738,\n",
       "         'sheila': 3468,\n",
       "         'happy': 3484,\n",
       "         'seven': 4754,\n",
       "         'cat': 3466,\n",
       "         'stop': 4760,\n",
       "         'right': 4734,\n",
       "         'marvin': 3492})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classes processing\n",
    "cardinal_classes = list(set(map(lambda fp:os.path.split(os.path.split(fp)[0])[1], filepaths)))\n",
    "le_classes = LabelEncoder().fit(cardinal_classes)\n",
    "Counter(map(lambda fp:os.path.split(os.path.split(fp)[0])[1], filepaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:50:14.732027Z",
     "iopub.status.busy": "2021-02-22T04:50:14.732027Z",
     "iopub.status.idle": "2021-02-22T04:50:21.222863Z",
     "shell.execute_reply": "2021-02-22T04:50:21.221852Z",
     "shell.execute_reply.started": "2021-02-22T04:50:14.732027Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quick Unit-Tests\n",
    "# Test data preparation\n",
    "_gen_test = get_batcher(filepaths, 1000)\n",
    "batch_a_wav, batch_a_target = next(_gen_test)\n",
    "batch_b_wav, batch_b_target = next(_gen_test)\n",
    "_gen_test_le = get_batcher(filepaths, 1000, label_encoder=le_classes)\n",
    "batch_le_wav, batch_le_target = next(_gen_test_le)\n",
    "\n",
    "# Test batch matrix shape coherences\n",
    "assert batch_a_wav.shape == (1000, 16000, 1)\n",
    "assert batch_le_wav.shape == (1000, 16000, 1)\n",
    "assert batch_a_wav.shape == batch_b_wav.shape == batch_le_wav.shape\n",
    "\n",
    "# Test batch reproducibility\n",
    "assert np.sum(np.abs(batch_a_wav-batch_b_wav)) != 0\n",
    "assert len(batch_a_target) == len(batch_b_target) == len(batch_le_target)\n",
    "assert any(batch_a_target != batch_b_target)\n",
    "\n",
    "# Test classes label encoder\n",
    "assert all(batch_le_target == np.expand_dims(le_classes.transform(np.squeeze(batch_a_target)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:50:21.224844Z",
     "iopub.status.busy": "2021-02-22T04:50:21.223846Z",
     "iopub.status.idle": "2021-02-22T04:50:23.163500Z",
     "shell.execute_reply": "2021-02-22T04:50:23.162501Z",
     "shell.execute_reply.started": "2021-02-22T04:50:21.224844Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quick Unit-Tests\n",
    "# Test data preparation\n",
    "_gen_test = get_batcher(filepaths, 1000)\n",
    "batch_a_wav, batch_a_target = next(_gen_test)\n",
    "batch_b_wav, batch_b_target = next(_gen_test)\n",
    "_gen_test_le = get_batcher(filepaths, 1000, label_encoder=le_classes)\n",
    "batch_le_wav, batch_le_target = next(_gen_test_le)\n",
    "\n",
    "# Test batch matrix shape coherences\n",
    "assert batch_a_wav.shape == (1000, 16000, 1)\n",
    "assert batch_le_wav.shape == (1000, 16000, 1)\n",
    "assert batch_a_wav.shape == batch_b_wav.shape == batch_le_wav.shape\n",
    "\n",
    "# Test batch reproducibility\n",
    "assert np.sum(np.abs(batch_a_wav-batch_b_wav)) != 0\n",
    "assert len(batch_a_target) == len(batch_b_target) == len(batch_le_target)\n",
    "assert any(batch_a_target != batch_b_target)\n",
    "\n",
    "# Test classes label encoder\n",
    "assert all(batch_le_target == np.expand_dims(le_classes.transform(np.squeeze(batch_a_target)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T04:59:18.937923Z",
     "iopub.status.busy": "2021-02-22T04:59:18.937923Z",
     "iopub.status.idle": "2021-02-22T04:59:18.969836Z",
     "shell.execute_reply": "2021-02-22T04:59:18.968862Z",
     "shell.execute_reply.started": "2021-02-22T04:59:18.937923Z"
    }
   },
   "outputs": [],
   "source": [
    "class NameSpacer:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "class Architecture:\n",
    "    def __init__(self, class_cardinality, seq_len=16000, name=\"architecture\"):\n",
    "        self.seq_len = seq_len\n",
    "        self.class_cardinality = class_cardinality\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        self.name=name\n",
    "        self.define_computation_graph()\n",
    "        \n",
    "        #Aliases\n",
    "        self.ph = self.placeholders\n",
    "        self.op = self.optimizers\n",
    "        self.summ = self.summaries\n",
    "\n",
    "    def define_computation_graph(self):\n",
    "        # Reset graph\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        self.placeholders = NameSpacer(**self.define_placeholders())\n",
    "        self.core_model = NameSpacer(**self.define_core_model())\n",
    "        self.losses = NameSpacer(**self.define_losses())\n",
    "        self.optimizers = NameSpacer(**self.define_optimizers())\n",
    "        self.summaries = NameSpacer(**self.define_summaries())\n",
    "\n",
    "    def define_placeholders(self):\n",
    "        with tf.compat.v1.variable_scope(\"Placeholders\"):\n",
    "            wav_in = tf.Variable(tf.ones(shape=[None, self.seq_len, 1]), dtype=tf.float32, name=\"wav_in\")\n",
    "            is_train = tf.Variable(tf.ones(shape=None), dtype=tf.bool, name='is_train')\n",
    "            target = tf.Variable(tf.ones(shape=[None, 1], dtype=tf.int32, name='target'))\n",
    "            acc_dev = tf.Variable(tf.ones(shape=None), dtype=tf.float32, name='acc_dev')\n",
    "            loss_dev = tf.Variable(tf.ones(shape=None), dtype=tf.float32, name='loss_dev')\n",
    "            return({\"wav_in\": wav_in, \"target\": target, \"is_train\": is_train, \"acc_dev\": \n",
    "                    acc_dev, \"loss_dev\": loss_dev})\n",
    "        \n",
    "    def define_core_model(self):\n",
    "        with tf.compat.v1.variable_scope(\"Core_Model\"):\n",
    "            x = inception_1d(x=self.placeholders.wav_in, is_train=self.placeholders.is_train, \n",
    "                             norm_function=BatchNorm, activ_function=tf.nn.relu, depth=1,\n",
    "                             name=\"Inception_1_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=1, name=\"Inception_1_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=1, name=\"Inception_2_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=1, name=\"Inception_2_3\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_2\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=2, name=\"Inception_3_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=2, name=\"Inception_3_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_3\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=2, name=\"Inception_4_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=2, name=\"Inception_4_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_4\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=3, name=\"Inception_5_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=3, name=\"Inception_5_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_5\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=3, name=\"Inception_6_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=3, name=\"Inception_6_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_6\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_7_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_7_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_7\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_8_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_8_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_8\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_9_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm, \n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_9_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_9\")\n",
    "            x = tf.contrib.layers.flatten(x)\n",
    "            x = tf.layers.dense(BatchNorm(name=\"bn_dense_1\")(x,train=self.placeholders.is_train),\n",
    "                                128, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"dense_1\")\n",
    "            output = tf.layers.dense(BatchNorm(name=\"bn_dense_2\")(x,train=self.placeholders.is_train),\n",
    "                                self.class_cardinality, activation=None, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"output\")\n",
    "            return({\"output\": output})\n",
    "        \n",
    "    def define_losses(self):\n",
    "        with tf.compat.v1.variable_scope(\"Losses\"):\n",
    "            softmax_ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.squeeze(self.placeholders.target), \n",
    "                                                                        logits=self.core_model.output,\n",
    "                                                                        name=\"softmax\")\n",
    "            return({\"softmax\": softmax_ce})\n",
    "\n",
    "    def define_optimizers(self):\n",
    "        with tf.compat.v1.variable_scope(\"Optimization\"):\n",
    "            op = self.optimizer.minimize(self.losses.softmax)\n",
    "            return({\"op\": op})\n",
    "\n",
    "    def define_summaries(self):\n",
    "        with tf.compat.v1.variable_scope(\"Summaries\"):\n",
    "            ind_max = tf.squeeze(tf.cast(tf.argmax(self.core_model.output, axis=1), tf.int32))\n",
    "            target = tf.squeeze(self.placeholders.target)\n",
    "            acc= tf.reduce_mean(tf.cast(tf.equal(ind_max, target), tf.float32))\n",
    "            loss = tf.reduce_mean(self.losses.softmax)\n",
    "            train_scalar_probes = {\"accuracy\": acc, \n",
    "                                   \"loss\": loss}\n",
    "            train_performance_scalar = [tf.summary.scalar(k, tf.reduce_mean(v), family=self.name) \n",
    "                                        for k, v in train_scalar_probes.items()]\n",
    "            train_performance_scalar = tf.summary.merge(train_performance_scalar)\n",
    "\n",
    "            dev_scalar_probes = {\"acc_dev\": self.placeholders.acc_dev, \n",
    "                                 \"loss_dev\": self.placeholders.loss_dev}\n",
    "            dev_performance_scalar = [tf.summary.scalar(k, v, family=self.name) for k, v in dev_scalar_probes.items()]\n",
    "            dev_performance_scalar = tf.summary.merge(dev_performance_scalar)\n",
    "            return({\"accuracy\": acc, \"loss\": loss, \"s_tr\": train_performance_scalar, \"s_de\": dev_performance_scalar})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-22T07:12:50.797174Z",
     "iopub.status.busy": "2021-02-22T07:12:50.795180Z",
     "iopub.status.idle": "2021-02-22T07:12:50.960737Z",
     "shell.execute_reply": "2021-02-22T07:12:50.958742Z",
     "shell.execute_reply.started": "2021-02-22T07:12:50.797174Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mones\u001b[1;34m(shape, dtype, name)\u001b[0m\n\u001b[0;32m   3124\u001b[0m         \u001b[1;31m# Go through tensor shapes to get int64-if-needed semantics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3125\u001b[1;33m         shape = constant_op._tensor_shape_tensor_conversion_function(\n\u001b[0m\u001b[0;32m   3126\u001b[0m             tensor_shape.TensorShape(shape))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_tensor_shape_tensor_conversion_function\u001b[1;34m(s, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    354\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m     raise ValueError(\n\u001b[0m\u001b[0;32m    356\u001b[0m         \"Cannot convert a partially known TensorShape to a Tensor: %s\" % s)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot convert a partially known TensorShape to a Tensor: (None, 16000, 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-44d55d5cd39b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mArchitecture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcardinal_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"wavception\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart_tensorflow_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_summary_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"~/.logs_tensorboard/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wavception\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"V1\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Adjust your tensorboard logs path here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-e1c553eb1fdf>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, class_cardinality, seq_len, name)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefine_computation_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#Aliases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-e1c553eb1fdf>\u001b[0m in \u001b[0;36mdefine_computation_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Reset graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNameSpacer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefine_placeholders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNameSpacer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefine_core_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNameSpacer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefine_losses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-e1c553eb1fdf>\u001b[0m in \u001b[0;36mdefine_placeholders\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdefine_placeholders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Placeholders\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mwav_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"wav_in\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mis_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'is_train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mones\u001b[1;34m(shape, dtype, name)\u001b[0m\n\u001b[0;32m   3127\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3128\u001b[0m         \u001b[1;31m# Happens when shape is a list with tensor elements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3129\u001b[1;33m         \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3131\u001b[0m       \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Ensure it's a vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1540\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    337\u001b[0m                                          as_ref=False):\n\u001b[0;32m    338\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m--> 264\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    265\u001b[0m                         allow_broadcast=True)\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    274\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m   \u001b[1;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m   \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "net = Architecture(class_cardinality=len(cardinal_classes), name=\"wavception\")\n",
    "\n",
    "sess = start_tensorflow_session(device=\"1\")\n",
    "sw = get_summary_writer(sess, \"~/.logs_tensorboard/\", \"wavception\", \"V1\") # Adjust your tensorboard logs path here\n",
    "c = 0\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-02-22T04:37:26.343737Z",
     "iopub.status.idle": "2021-02-22T04:37:26.344737Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(655321)\n",
    "random.seed(655321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-02-22T04:37:26.345732Z",
     "iopub.status.idle": "2021-02-22T04:37:26.346731Z"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(50000):\n",
    "    random.shuffle(training_list)\n",
    "    batcher = get_batcher(training_list, 16, le_classes)\n",
    "    for i, (batch_x, batch_y) in enumerate(batcher):\n",
    "        _, loss, acc, s = sess.run([net.op.op, net.losses.softmax, net.summ.accuracy, net.summ.s_tr],\n",
    "                                 feed_dict={net.ph.wav_in: batch_x, net.ph.target: batch_y, \n",
    "                                            net.ph.is_train: True})\n",
    "        print(\"[{0:04d}|{1:04d}] Accuracy train: {2:.2f}%\".format(epoch, i, acc*100))\n",
    "        sw.add_summary(s, c)\n",
    "        \n",
    "        if c%1000==0: # Validation\n",
    "            accuracies_dev=[]\n",
    "            losses_dev=[]\n",
    "            batcher = get_batcher(validation_list, 16, le_classes)\n",
    "            for i, (batch_x, batch_y) in enumerate(batcher):\n",
    "                acc, loss= sess.run([net.summ.accuracy, net.summ.loss], \n",
    "                               feed_dict={net.ph.wav_in: batch_x, net.ph.target: batch_y, \n",
    "                                          net.ph.is_train: False})\n",
    "                accuracies_dev.append(acc)\n",
    "                losses_dev.append(loss)\n",
    "            s = sess.run(net.summ.s_de, feed_dict={net.ph.acc_dev: np.mean(accuracies_dev),\n",
    "                                                        net.ph.loss_dev: np.mean(losses_dev)})\n",
    "            sw.add_summary(s, c)\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-02-22T04:37:26.347727Z",
     "iopub.status.idle": "2021-02-22T04:37:26.347727Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracies=[]\n",
    "batcher = get_batcher(testing_list, 64, le_classes)\n",
    "for i, (batch_x, batch_y) in tqdm(enumerate(batcher)):\n",
    "    acc= sess.run(net.summ.accuracy, feed_dict={net.ph.wav_in: batch_x, net.ph.target: batch_y, \n",
    "                                                     net.ph.is_train: False})\n",
    "    accuracies.append(acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and submission building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-02-22T04:37:26.348725Z",
     "iopub.status.idle": "2021-02-22T04:37:26.349722Z"
    }
   },
   "outputs": [],
   "source": [
    "scoring_list = glob.glob(os.path.join(get_scoring_audio_path(), \"*.wav\"), recursive=True)\n",
    "batcher = get_batcher(scoring_list, 80, le_classes, scoring=True)\n",
    "fns = []\n",
    "prds = []\n",
    "for i, (batch_x, filepaths) in tqdm(enumerate(batcher)):\n",
    "    pred = sess.run(net.core_model.output, feed_dict={net.ph.wav_in: batch_x, net.ph.is_train: False})\n",
    "    fns.extend(map(lambda f:os.path.split(f)[1], filepaths))\n",
    "    prds.extend(map(lambda f:np.argmax(pred, axis=1).tolist(), pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
