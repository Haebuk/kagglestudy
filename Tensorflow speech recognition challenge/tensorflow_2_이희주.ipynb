{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- competition/dataset : [https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/data](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/data)\n",
    "- date : 2021/02/19\n",
    "- original : [https://www.kaggle.com/alphasis/light-weight-cnn-lb-0-74](https://www.kaggle.com/alphasis/light-weight-cnn-lb-0-74)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light-Weight CNN LB 0.74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-19T23:23:23.804833Z",
     "iopub.status.busy": "2021-01-19T23:23:23.803834Z",
     "iopub.status.idle": "2021-01-19T23:23:23.811814Z",
     "shell.execute_reply": "2021-01-19T23:23:23.810817Z",
     "shell.execute_reply.started": "2021-01-19T23:23:23.804833Z"
    }
   },
   "source": [
    "**✏ 필사 1회** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preface\n",
    "이 노트북은 가벼운 CNN을 구축하는데 초점을 두었습니다. 이것은 입력으로 리샘플링된 wav 파일의 spectrogram을 사용합니다. kaggle 클라우드의 하드웨어 제한 때문에 원래 버전에 비해 이 스크립트는 모자란 버전입니다.  \n",
    "\n",
    "LB 0.74를 기록하기 위해서는 epoch를 5로 설정하고 chop_audio(num=1000)도 설정해야 하며, 모든 Conv layer 파라미터를 두배로 해야합니다.  \n",
    "\n",
    "이 스크립트가 비록 Alex Ozerin의 베이스라인보다 조금 우수한 정도이지만, 원래의 wav 파일(16000 sample rate)을 사용하면 훨씬 더 높은 점수를 얻을 수 있을 것이라고 확신합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve This Script\n",
    "오로지 경량화된 CNN만 사용하기 때문에 성능에 제한이 있습니다. 다음과 같은 방법들로 성능을 개선할 수 있습니다:  \n",
    "1. 리샘플된 데이터 대신 원래의 wav 파일 사용\n",
    "2. chop_audio를 사용하여 더욱 silence한 wav 파일 생성\n",
    "3. 더 깊은 CNN을 구축하거나 RNN 사용\n",
    "4. epochs 횟수를 늘려서 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Words\n",
    "LB 0.88을 도달하려면 아직도 멀었습니다. 사실, CNN이 그렇게 높은 곳에 도달할 수 있을지도 의심스럽습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T12:29:20.345124Z",
     "iopub.status.busy": "2021-02-20T12:29:20.342132Z",
     "iopub.status.idle": "2021-02-20T12:29:20.547888Z",
     "shell.execute_reply": "2021-02-20T12:29:20.493035Z",
     "shell.execute_reply.started": "2021-02-20T12:29:20.345124Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import gc\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래의 sample rate는 16000이며, 데이터 크기를 줄이기 위해 8000으로 resampling할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T12:29:20.577808Z",
     "iopub.status.busy": "2021-02-20T12:29:20.577808Z",
     "iopub.status.idle": "2021-02-20T12:29:20.767303Z",
     "shell.execute_reply": "2021-02-20T12:29:20.729404Z",
     "shell.execute_reply.started": "2021-02-20T12:29:20.577808Z"
    }
   },
   "outputs": [],
   "source": [
    "L = 16000\n",
    "legal_labels = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "\n",
    "# src folders\n",
    "root_path = 'data'\n",
    "out_path = 'data'\n",
    "model_path = 'data'\n",
    "train_data_path = os.path.join(root_path, 'train', 'audio')\n",
    "test_data_path = os.path.join(root_path, 'test', 'audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T12:29:20.930864Z",
     "iopub.status.busy": "2021-02-20T12:29:20.923883Z",
     "iopub.status.idle": "2021-02-20T12:29:21.206412Z",
     "shell.execute_reply": "2021-02-20T12:29:21.157543Z",
     "shell.execute_reply.started": "2021-02-20T12:29:20.930864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, array([  16,    9,   52, ..., -170, -152, -176], dtype=int16))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wavfile.read(os.path.join(train_data_path, 'on', '00b01445_nohash_0.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T12:29:21.243337Z",
     "iopub.status.busy": "2021-02-20T12:29:21.243337Z",
     "iopub.status.idle": "2021-02-20T12:29:21.377990Z",
     "shell.execute_reply": "2021-02-20T12:29:21.338060Z",
     "shell.execute_reply.started": "2021-02-20T12:29:21.243337Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_fft(y, fs):\n",
    "    T = 1.0 / fs\n",
    "    N = y.shape[0]\n",
    "    yf = fft(y)\n",
    "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "    # FFT는 대칭형태이기 때문에 절반만 사용\n",
    "    \n",
    "    vals = 2.0/N * np.abs(yf[0:N//2])\n",
    "    return xf, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T12:29:21.399896Z",
     "iopub.status.busy": "2021-02-20T12:29:21.390920Z",
     "iopub.status.idle": "2021-02-20T12:29:21.582142Z",
     "shell.execute_reply": "2021-02-20T12:29:21.540255Z",
     "shell.execute_reply.started": "2021-02-20T12:29:21.399896Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_specgram(audio, sample_rate, window_size=20, step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(\n",
    "        audio, fs=sample_rate, window='hann', nperseg=nperseg,\n",
    "        noverlap=noverlap, detrend=False\n",
    "    )\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T12:29:21.600094Z",
     "iopub.status.busy": "2021-02-20T12:29:21.600094Z",
     "iopub.status.idle": "2021-02-20T12:29:21.844439Z",
     "shell.execute_reply": "2021-02-20T12:29:21.787592Z",
     "shell.execute_reply.started": "2021-02-20T12:29:21.600094Z"
    }
   },
   "outputs": [],
   "source": [
    "def list_wavs_fname(dirpath, ext='wav'):\n",
    "    print(dirpath)\n",
    "    fpaths = glob(os.path.join(dirpath, r'*/*' + ext))\n",
    "    pat = r'.+\\\\(\\w+)\\\\\\w+\\.' + ext + '$'\n",
    "    labels = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            labels.append(r.group(1))\n",
    "    pat = r'.+\\\\(\\w+\\.' + ext + ')$'\n",
    "    fnames = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            fnames.append(r.group(1))\n",
    "    return labels, fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pad_audio**: 16000 이하인 오디오들에 대해 같은 길이를 갖도록 0으로 padding.  \n",
    "**chop_audio**: 16000 이상인 오디오들을 16000으로 잘라냄. 주어진 매개변수 'num'에 따라 하나의 큰 wav 파일로부터 여러 개의 청크를 생성.\n",
    "**label_transform**: 더미 값들의 레이블 변경. 레이블을 예측하기 위해 softmax와 조합할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T12:29:21.872367Z",
     "iopub.status.busy": "2021-02-20T12:29:21.862392Z",
     "iopub.status.idle": "2021-02-20T12:29:22.039161Z",
     "shell.execute_reply": "2021-02-20T12:29:21.988331Z",
     "shell.execute_reply.started": "2021-02-20T12:29:21.872367Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_audio(samples):\n",
    "    if len(samples) >= L:\n",
    "        return samples\n",
    "    else:\n",
    "        return np.pad(samples, pad_width=(L - len(samples), 0),\n",
    "                      mode='constant', constant_values=(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T12:29:22.066090Z",
     "iopub.status.busy": "2021-02-20T12:29:22.056116Z",
     "iopub.status.idle": "2021-02-20T12:29:22.179785Z",
     "shell.execute_reply": "2021-02-20T12:29:22.144878Z",
     "shell.execute_reply.started": "2021-02-20T12:29:22.066090Z"
    }
   },
   "outputs": [],
   "source": [
    "def chop_audio(samples, L=16000, num=20):\n",
    "    for i in range(num):\n",
    "        beg = np.random.randint(0, len(samples) - L)\n",
    "        yield samples[beg:beg+L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T12:29:22.261565Z",
     "iopub.status.busy": "2021-02-20T12:29:22.254584Z",
     "iopub.status.idle": "2021-02-20T12:29:22.388227Z",
     "shell.execute_reply": "2021-02-20T12:29:22.344344Z",
     "shell.execute_reply.started": "2021-02-20T12:29:22.261565Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_transform(labels):\n",
    "    nlabels = []\n",
    "    for label in labels:\n",
    "        if label == '__background_noise_':\n",
    "            nlabels.append('silence')\n",
    "        elif label not in legal_labels:\n",
    "            nlabels.append('unknown')\n",
    "        else:\n",
    "            nlabels.append(label)\n",
    "    return pd.get_dummies(pd.Series(nlabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train, y_train을 생성하기 위해 위에서 만든 함수들을 사용합니다. label_index는 pandas로 더미 값을 생성하기 위한 인덱스이므로 나중에 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T12:29:22.431112Z",
     "iopub.status.busy": "2021-02-20T12:29:22.423134Z",
     "iopub.status.idle": "2021-02-20T12:29:34.354500Z",
     "shell.execute_reply": "2021-02-20T12:29:34.337517Z",
     "shell.execute_reply.started": "2021-02-20T12:29:22.431112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\train\\audio\n"
     ]
    }
   ],
   "source": [
    "labels, fnames = list_wavs_fname(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T12:29:34.363445Z",
     "iopub.status.busy": "2021-02-20T12:29:34.362471Z",
     "iopub.status.idle": "2021-02-20T12:57:25.008185Z",
     "shell.execute_reply": "2021-02-20T12:57:25.007187Z",
     "shell.execute_reply.started": "2021-02-20T12:29:34.363445Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-d820abf2dd3b>:6: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sample_rate, samples = wavfile.read(os.path.join(train_data_path, label, fname))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sample_rate = L/2\n",
    "y_train = []\n",
    "x_train = []\n",
    "\n",
    "for label, fname in zip(labels, fnames):\n",
    "    sample_rate, samples = wavfile.read(os.path.join(train_data_path, label, fname))\n",
    "    samples = pad_audio(samples)\n",
    "    if len(samples) > L:\n",
    "        n_samples = chop_audio(samples)\n",
    "    else:\n",
    "        n_samples = [samples]\n",
    "    for samples in n_samples:\n",
    "        resampled = signal.resample(\n",
    "            samples, int(new_sample_rate/sample_rate*samples.shape[0])\n",
    "        )\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        y_train.append(label)\n",
    "        x_train.append(specgram)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_train = x_train.reshape(tuple(list(x_train.shape) + [1]))\n",
    "y_train = label_transform(y_train)\n",
    "label_index = y_train.columns.values\n",
    "y_train = y_train.values\n",
    "y_train = np.array(y_train)\n",
    "del labels, fnames\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 CNN을 선언합니다. 생성된 specgram은 형태가 (99, 81)이지만, Conv2D layer에 fitting하기 위해서는 변형시켜야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T12:57:25.011178Z",
     "iopub.status.busy": "2021-02-20T12:57:25.010180Z",
     "iopub.status.idle": "2021-02-20T13:12:26.327420Z",
     "shell.execute_reply": "2021-02-20T13:12:26.324475Z",
     "shell.execute_reply.started": "2021-02-20T12:57:25.011178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 99, 81, 1)]       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 99, 81, 1)         4         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 98, 80, 8)         40        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 97, 79, 8)         264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 39, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 46, 37, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 44, 35, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 22, 17, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 22, 17, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 20, 15, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 7, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 7, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2240)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               286848    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11)                1419      \n",
      "=================================================================\n",
      "Total params: 314,239\n",
      "Trainable params: 313,725\n",
      "Non-trainable params: 514\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "3648/3648 - 296s - loss: 0.1733 - val_loss: 0.0970\n",
      "Epoch 2/3\n",
      "3648/3648 - 294s - loss: 0.0854 - val_loss: 0.0545\n",
      "Epoch 3/3\n",
      "3648/3648 - 293s - loss: 0.0671 - val_loss: 0.0535\n",
      "INFO:tensorflow:Assets written to: data\\cnn.model\\assets\n"
     ]
    }
   ],
   "source": [
    "input_shape = (99, 81, 1)\n",
    "nclass = 11\n",
    "inp = Input(shape=input_shape)\n",
    "norm_inp = BatchNormalization()(inp)\n",
    "img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(norm_inp)\n",
    "img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Convolution2D(32, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Flatten()(img_1)\n",
    "\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(img_1))\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(dense_1))\n",
    "dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "model = models.Model(inputs=inp, outputs=dense_1)\n",
    "opt = optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=opt, loss=losses.binary_crossentropy)\n",
    "model.summary()\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=2021)\n",
    "model.fit(x_train, y_train, batch_size=16, validation_data=(x_valid, y_valid), epochs=3, shuffle=True, verbose=2)\n",
    "\n",
    "model.save(os.path.join(model_path, 'cnn.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터는 RAM으로 fitting시키기에는 너무 크기 때문에 하나씩 처리해야 합니다. test_data_generator는 CNN에 입력하기 위한 테스트 wav 파일의 batch를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T13:12:26.331407Z",
     "iopub.status.busy": "2021-02-20T13:12:26.331407Z",
     "iopub.status.idle": "2021-02-20T13:12:26.361325Z",
     "shell.execute_reply": "2021-02-20T13:12:26.357336Z",
     "shell.execute_reply.started": "2021-02-20T13:12:26.331407Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_data_generator(batch=16):\n",
    "    fpaths = glob(os.path.join(test_data_path, '*wav'))\n",
    "    i = 0\n",
    "    for path in fpaths:\n",
    "        if i == 0:\n",
    "            imgs = []\n",
    "            fnames = []\n",
    "        i += 1\n",
    "        rate, samples = wavfile.read(path)\n",
    "        samples = pad_audio(samples)\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        imgs.append(specgram)\n",
    "        fnames.append(path.split('\\\\')[-1])\n",
    "        if i == batch:\n",
    "            i = 0\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(tuple(list(imgs.shape) + [1]))\n",
    "            yield fnames, imgs\n",
    "    if i < batch:\n",
    "        imgs = np.array(imgs)\n",
    "        imgs = imgs.reshape(tuple(list(imgs.shape) + [1]))\n",
    "        yield fnames, imgs\n",
    "    raise StopIteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련된 모델을 사용하여 테스트 데이터의 레이블을 예측합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T13:12:26.367309Z",
     "iopub.status.busy": "2021-02-20T13:12:26.366311Z",
     "iopub.status.idle": "2021-02-20T13:27:04.790937Z",
     "shell.execute_reply": "2021-02-20T13:27:04.789939Z",
     "shell.execute_reply.started": "2021-02-20T13:12:26.367309Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-8857457ef1bc>\u001b[0m in \u001b[0;36mtest_data_generator\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mfnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-99370109fa41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_data_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mpredicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpredicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "exit()\n",
    "del x_train, y_train\n",
    "gc.collect()\n",
    "\n",
    "index = []\n",
    "results = []\n",
    "for fnames, imgs in test_data_generator(batch=32):\n",
    "    predicts = model.predict(imgs)\n",
    "    predicts = np.argmax(predicts, axis=1)\n",
    "    predicts = [label_index[p] for p in predicts]\n",
    "    index.extend(fname)\n",
    "    results.extend(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['fname', 'label'])\n",
    "df['fname'] = index\n",
    "df['label'] = results\n",
    "df.to_csv(os.path.join(out_path, 'submission_2_sub.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
