{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "better-economics",
   "metadata": {},
   "source": [
    "# Light-Weight CNN LB 0.74\n",
    "https://www.kaggle.com/alphasis/light-weight-cnn-lb-0-74"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-mauritius",
   "metadata": {},
   "source": [
    "## 들어가며\n",
    "이 노트북은 경량 CNN을 구축하는 것을 목표로 합니다.\n",
    "\n",
    "리샘플링된 wav 파일의 (rate 8000) 스펙그램을 입력으로 사용합니다.\n",
    "\n",
    "kaggle 클라우드 하드웨어 제한으로 인해 이 스크립트는 원래 스크립트의 '파손'된 버전입니다.\n",
    "\n",
    "LB 0.74를 가져오려면 에폭을 5로 설정하고 chop_audio(num=1000)을 설정하고 모든 Conv 레이어 파라미터를 두 배로 설정해야 합니다.\n",
    "\n",
    "이 스크립트는 Alex Ozerin의 기준보다 약간 개선되었지만, 원본 wav파일(16000 sample rate)을 사용하면 더 높은 점수를 얻을 수 있다고 생각합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-release",
   "metadata": {},
   "source": [
    "## 이 스크립트를 개선하는 방법\n",
    "이것은 단지 경량 CNN이기 때문에, 성능이 제한적입니다. 성능을 향상시킬 수 있는 몇 가지 방법이 있습니다.\n",
    "\n",
    "1. 원래 wav 파일을 사용합니다.\n",
    "2. shop_audio를 사용하여 더 많은 '침묵' wav 파일을 만듭니다.\n",
    "3. 심층 CNN을 구축하거나 RNN을 사용합니다.\n",
    "4. 더 긴 에폭으로 훈련합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developing-calculator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T11:54:59.527590Z",
     "start_time": "2021-02-19T11:54:57.389514Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import gc\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-edgar",
   "metadata": {},
   "source": [
    "원래 샘플율은 16000이며, 데이터 크기를 줄이기 위해 8000으로 리샘플링 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "potential-convert",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T11:54:59.544259Z",
     "start_time": "2021-02-19T11:54:59.527590Z"
    }
   },
   "outputs": [],
   "source": [
    "L = 16000\n",
    "legal_labels = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "\n",
    "# src folders\n",
    "root_path = r'..'\n",
    "out_path = r'.'\n",
    "model_path = r'.'\n",
    "train_data_path = os.path.join(root_path, 'input', 'train', 'audio')\n",
    "test_data_path = os.path.join(root_path, 'input', 'test', 'audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pleasant-gravity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T11:54:59.559270Z",
     "start_time": "2021-02-19T11:54:59.545258Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_fft(y, fs):\n",
    "    T = 1.0 / fs\n",
    "    N = y.shape[0]\n",
    "    yf = fft(y)\n",
    "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "    # FFT is simmetrical, so we take just the first half\n",
    "    # FFT is also complex, to we take just the real part (abs)\n",
    "    vals = 2.0/N * np.abs(yf[0:N//2])\n",
    "    return xf, vals\n",
    "\n",
    "def log_specgram(audio, sample_rate, window_size=20, step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio, fs=sample_rate, window='hann',\n",
    "                                           nperseg=nperseg, noverlap=noverlap,\n",
    "                                           detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32)+eps)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-pilot",
   "metadata": {},
   "source": [
    "다음은 훈련 데이터 폴더 내의 모든 wav 파일을 잡을 수 있는 유틸리티 기능입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interior-biology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T11:54:59.575285Z",
     "start_time": "2021-02-19T11:54:59.560271Z"
    }
   },
   "outputs": [],
   "source": [
    "def list_wavs_fname(dirpath, ext='wav'):\n",
    "    print(dirpath)\n",
    "    fpaths = glob(os.path.join(dirpath, r'*/*' + ext))\n",
    "    pat = f'.+/(\\w+)/\\w+\\.' + ext + '$'\n",
    "    labels = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            labels.append(r.group(1))\n",
    "    pat = r'.+/(\\w+\\.' + ext + ')$'\n",
    "    fnames = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            fnames.append(r.group(1))\n",
    "    return labels, fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-assumption",
   "metadata": {},
   "source": [
    "__pad_audio__는 16000(1초) 미만의 오디오를 0으로 패딩하여 모두 동일한 길이를 갖도록 합니다.\n",
    "\n",
    "__chop_audio__는 16000(예: 배경 노이즈 폴더의 wav 파일)보다 큰 오디오를 16000으로 잘라냅니다. 또한 'num' 매개 변수가 주어지면 하나의 큰 wav 파일 중에서 여러 개의 청크를 생성합니다.\n",
    "\n",
    "__label_transform__은 레이블을 더미 값으로 변환합니다. 이것은 레이블을 예측하기 위해 softmax와 함께 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opponent-difficulty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T11:54:59.591299Z",
     "start_time": "2021-02-19T11:54:59.576286Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_audio(samples):\n",
    "    if len(samples) >= L: return samples\n",
    "    else: return np.pad(samples, pad_width=(L - len(samples), 0), mode='constant', constant_values=(0,0))\n",
    "    \n",
    "def chop_audio(samples, L=16000, num=20):\n",
    "    for i in range(num):\n",
    "        beg = np.random.randint(0, len(samples) - L)\n",
    "        yield samples[beg: beg + L]\n",
    "        \n",
    "def label_transform(labels):\n",
    "    nlabels = []\n",
    "    for label in labels:\n",
    "        if label == '__background_noise_':\n",
    "            nlabels.append('silence')\n",
    "        elif label not in legal_labels:\n",
    "            nlabels.append('unknown')\n",
    "        else:\n",
    "            nlabels.append(label)\n",
    "    return pd.get_dummies(pd.Series(nlabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-military",
   "metadata": {},
   "source": [
    "다음으로, 위에서 선언한 함수를 사용하여 x_train 및 y_train을 생성합니다. label_index는 판다스에서  더미 값을 생성하기 위해 사용하는 색인입니다. 나중에 사용하기 위해 저장해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "respected-racing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T11:54:59.671372Z",
     "start_time": "2021-02-19T11:54:59.591299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\input\\train\\audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tens_2g\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, fnames = list_wavs_fname(train_data_path)\n",
    "\n",
    "new_sample_rate = 8000\n",
    "y_train=[]\n",
    "x_train=[]\n",
    "\n",
    "for label, fname in zip(labels, fnames):\n",
    "    sample_rate, samples = wavfile.read(os.path.join(train_data_path, label, fname))\n",
    "    samples = pad_audio(samples)\n",
    "    if len(samples) > 16000:\n",
    "        n_samples = chop_audio(samples)\n",
    "    else: n_samples = [samples]\n",
    "    for samples in n_samples:\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / sample_rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        y_train.append(label)\n",
    "        x_train.append(specgram)\n",
    "x_train = np.array(x_train)\n",
    "x_train = x_train.reshape(tuple(list(x_train.shape) + [1]))\n",
    "y_train = label_transform(y_train)\n",
    "label_index = y_train.columns.values\n",
    "y_train = y_train.values\n",
    "y_train = np.array(y_train)\n",
    "del labels, fnames\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-protection",
   "metadata": {},
   "source": [
    "CNN은 아래와 같이 선언했습니다. 생성된 스펙그램은 모양이 (99, 81)이 되겠지만 Conv2D 레이어에 맞기 위해서는 형태를 바꿔야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "secret-western",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T11:55:00.655924Z",
     "start_time": "2021-02-19T11:54:59.671372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 99, 81, 1)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 99, 81, 1)         4         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 98, 80, 8)         40        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 97, 79, 8)         264       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 48, 39, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48, 39, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 46, 37, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 44, 35, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 22, 17, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 22, 17, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 20, 15, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 7, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10, 7, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2240)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               286848    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 314,368\n",
      "Trainable params: 313,854\n",
      "Non-trainable params: 514\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-64f90aa50700>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tens_2g\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2174\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2175\u001b[0m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[1;32m-> 2176\u001b[1;33m                                               default_test_size=0.25)\n\u001b[0m\u001b[0;32m   2177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tens_2g\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   1859\u001b[0m             \u001b[1;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m             'aforementioned parameters.'.format(n_samples, test_size,\n\u001b[1;32m-> 1861\u001b[1;33m                                                 train_size)\n\u001b[0m\u001b[0;32m   1862\u001b[0m         )\n\u001b[0;32m   1863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "input_shape = (99, 81, 1)\n",
    "nclass = 12\n",
    "inp = Input(shape=input_shape)\n",
    "norm_inp = BatchNormalization()(inp)\n",
    "img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(norm_inp)\n",
    "img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Convolution2D(32, kernel_size=3, activation=activations.relu)(img_1)\n",
    "img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "img_1 = Dropout(rate=0.2)(img_1)\n",
    "img_1 = Flatten()(img_1)\n",
    "\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(img_1))\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(dense_1))\n",
    "dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "model = models.Model(inputs=inp, outputs=dense_1)\n",
    "opt = optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=opt, loss=losses.binary_crossentropy)\n",
    "model.summary()\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=2017)\n",
    "model.fit(x_train, y_train, batch_size=16, validation_data=(x_valid, y_valid), epochs=3, shuffle=True, verbose=2)\n",
    "\n",
    "model.save(os.path.join(model_path, 'cnn.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-electron",
   "metadata": {},
   "source": [
    "테스트 데이터가 너무 커서 RAM에 맞지 않아 하나씩 처리해야 합니다. Generator test_data_generator가 CNN에 연결할 테스트 Wav 파일 배치를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-concern",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T11:55:00.655924Z",
     "start_time": "2021-02-19T11:54:57.396Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_data_generator(batch=16):\n",
    "    fpaths = glob(os.path.join(test_data_path, '*wav'))\n",
    "    i = 0\n",
    "    for path in fpaths:\n",
    "        if i == 0:\n",
    "            imgs = []\n",
    "            fnames = []\n",
    "        i += 1\n",
    "        rate, samples = wavfile.read(path)\n",
    "        samples = pad_audio(samples)\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        imgs.append(specgram)\n",
    "        fnames.append(path.split('\\\\')[-1])\n",
    "        if i == batch:\n",
    "            i = 0\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(tuple(list(imgs.shape) + [1]))\n",
    "            yield fnames, imgs\n",
    "    if i < batch:\n",
    "        imgs = np.array(imgs)\n",
    "        imgs = imgs.reshape(tuple(list(imgs.shape) + [1]))\n",
    "        yield fames, imgs\n",
    "    raise StopIteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-estimate",
   "metadata": {},
   "source": [
    "훈련된 모델을 사용하여 테스트 데이터의 레이블을 예측합니다. 그러나 Kaggle은 테스트 데이터를 제공하지 않기 때문에 다음 섹션은 여기서 실행되지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-rebecca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T11:55:00.656926Z",
     "start_time": "2021-02-19T11:54:57.397Z"
    }
   },
   "outputs": [],
   "source": [
    "exit() # \n",
    "del x_train, y_train\n",
    "gc.collect()\n",
    "\n",
    "index = []\n",
    "results = []\n",
    "for fnames, imgs in test_data_generator(batch=32):\n",
    "    predicts = model.predicts(imgs)\n",
    "    predicts = np.argmax(predicts, axis=1)\n",
    "    predicts = [label_index[p] for p in predicts]\n",
    "    index.extend(fnames)\n",
    "    results.extend(predicts)\n",
    "    \n",
    "df = pd.DataFrame(columns=['fname', 'label'])\n",
    "df['fname'] = index\n",
    "df['label'] = results\n",
    "df.to_csv(os.path.join(out_path, 'sub.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-river",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-stewart",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-medication",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-consistency",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-import",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-istanbul",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-huntington",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-friendship",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-helena",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-thompson",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
